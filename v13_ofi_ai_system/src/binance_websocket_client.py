#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""å¸å®‰WebSocketå®¢æˆ·ç«¯ - æ¥æ”¶çœŸå®è®¢å•ç°¿æ•°æ®

è¿™ä¸ªæ¨¡å—å®ç°äº†å¸å®‰æœŸè´§WebSocketå®¢æˆ·ç«¯ï¼Œç”¨äºæ¥æ”¶å®æ—¶çš„è®¢å•ç°¿æ•°æ®ã€‚
æ”¯æŒ5æ¡£è®¢å•ç°¿æ·±åº¦ï¼Œæ›´æ–°é¢‘ç‡100msã€‚

Author: V13 OFI+AI System
Created: 2025-01-17
"""

import websocket
import json
from datetime import datetime
from collections import deque
import logging
import pandas as pd
import numpy as np
import os
from pathlib import Path
import threading

# é…ç½®æ—¥å¿—ï¼ˆåŸºç¡€é…ç½®ï¼‰
logger = logging.getLogger(__name__)


class BinanceOrderBookStream:
    """å¸å®‰è®¢å•ç°¿WebSocketæµå®¢æˆ·ç«¯
    
    è¿™ä¸ªç±»è´Ÿè´£è¿æ¥å¸å®‰æœŸè´§WebSocketï¼Œæ¥æ”¶å®æ—¶çš„è®¢å•ç°¿æ•°æ®ã€‚
    è®¢å•ç°¿åŒ…å«5æ¡£ä¹°å•å’Œ5æ¡£å–å•ï¼Œæ›´æ–°é¢‘ç‡ä¸º100msã€‚
    
    Attributes:
        symbol (str): äº¤æ˜“å¯¹ç¬¦å·ï¼Œå¦‚'ethusdt'
        depth_levels (int): è®¢å•ç°¿æ·±åº¦æ¡£ä½ï¼Œé»˜è®¤5æ¡£
        ws_url (str): WebSocketè¿æ¥URL
        order_book_history (deque): å†å²è®¢å•ç°¿æ•°æ®ç¼“å­˜
        ws (WebSocketApp): WebSocketè¿æ¥å¯¹è±¡
    """
    
    def __init__(self, symbol='ethusdt', depth_levels=5):
        """åˆå§‹åŒ–å¸å®‰WebSocketå®¢æˆ·ç«¯
        
        Args:
            symbol (str): äº¤æ˜“å¯¹ç¬¦å·ï¼Œé»˜è®¤'ethusdt'ï¼ˆETHUSDTæ°¸ç»­åˆçº¦ï¼‰
            depth_levels (int): è®¢å•ç°¿æ·±åº¦æ¡£ä½ï¼Œé»˜è®¤5æ¡£
            
        Example:
            >>> client = BinanceOrderBookStream('ethusdt', 5)
            >>> # å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œå‡†å¤‡è¿æ¥
        """
        self.symbol = symbol.lower()
        self.depth_levels = depth_levels
        
        # æ„å»ºWebSocket URL
        # ä½¿ç”¨å¤‡ç”¨åŸŸå binancefuture.comï¼ˆæµ‹è¯•éªŒè¯å¯ç”¨ï¼‰
        # æ ¼å¼: wss://fstream.binancefuture.com/ws/{symbol}@depth{levels}@100ms
        self.ws_url = f"wss://fstream.binancefuture.com/ws/{self.symbol}@depth{depth_levels}@100ms"
        
        # è®¢å•ç°¿å†å²æ•°æ®ç¼“å­˜ï¼ˆæœ€å¤šä¿å­˜10000æ¡ï¼‰
        self.order_book_history = deque(maxlen=10000)
        
        # WebSocketè¿æ¥å¯¹è±¡ï¼ˆåˆå§‹åŒ–ä¸ºNoneï¼‰
        self.ws = None
        
        # æ•°æ®å­˜å‚¨ç›¸å…³
        self.save_interval = 60  # æ¯60ç§’ä¿å­˜ä¸€æ¬¡
        self.last_save_time = datetime.now()
        
        # ä¸‰å±‚å­˜å‚¨ç›®å½•
        self.data_dir = Path("v13_ofi_ai_system/data/order_book")
        self.ndjson_dir = self.data_dir / "ndjson"  # Layer 1: åŸå§‹æµ
        self.parquet_dir = self.data_dir / "parquet"  # Layer 2: åˆ†æå­˜å‚¨
        self.csv_dir = self.data_dir / "csv"  # Legacy: CSVå¤‡ä»½
        
        self.ndjson_dir.mkdir(parents=True, exist_ok=True)
        self.parquet_dir.mkdir(parents=True, exist_ok=True)
        self.csv_dir.mkdir(parents=True, exist_ok=True)
        
        # åºåˆ—å·å’Œæ—¶å»¶è·Ÿè¸ª
        self.message_seq = 0  # æ¶ˆæ¯åºåˆ—å·
        self.last_order_book = None  # ä¸Šä¸€ä¸ªè®¢å•ç°¿çŠ¶æ€ï¼ˆç”¨äºå¢é‡æ£€æµ‹ï¼‰
        self.last_update_id = None  # ä¸Šä¸€ä¸ªæ›´æ–°IDï¼ˆpuå­—æ®µï¼‰
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            'total_messages': 0,
            'start_time': datetime.now(),
            'latency_list': [],
            'last_print_time': datetime.now(),
            'last_metrics_time': datetime.now(),
            # åºåˆ—ä¸€è‡´æ€§ç»Ÿè®¡ï¼ˆæœŸè´§WSä¸¥æ ¼å¯¹é½ï¼‰
            'gaps': 0,  # è¿ç»­åŒºé—´å†…çš„ç¼ºå£è®¡æ•°ï¼ˆåŒºé—´å†…u-U-1çš„ç´¯è®¡ï¼‰
            'max_gap': 0,  # å•æ¬¡æœ€å¤§ç¼ºå£
            'resync': 0,  # resyncæ¬¡æ•°ï¼ˆpu != last_uï¼‰
            'reconnects': 0,  # é‡è¿æ¬¡æ•°
            'last_u': None,  # ä¸Šä¸€æ¬¡çš„uå€¼ï¼ˆç”¨äºpuå¯¹é½æ£€æŸ¥ï¼‰
        }
        
        # é…ç½®å¢å¼ºæ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()
        
        logger.info(f"="*60)
        logger.info(f"BinanceOrderBookStream initialized for {symbol.upper()}")
        logger.info(f"WebSocket URL: {self.ws_url}")
        logger.info(f"Data directory: {self.data_dir}")
        logger.info(f"="*60)
    
    def __repr__(self):
        """å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"BinanceOrderBookStream(symbol='{self.symbol}', depth_levels={self.depth_levels})"
    
    def __str__(self):
        """å¯¹è±¡çš„å¯è¯»å­—ç¬¦ä¸²è¡¨ç¤º"""
        status = "connected" if self.ws else "not connected"
        history_size = len(self.order_book_history)
        return f"BinanceOrderBookStream({self.symbol.upper()}, {status}, {history_size} records)"
    
    def _setup_logging(self):
        """é…ç½®å¢å¼ºçš„æ—¥å¿—ç³»ç»Ÿï¼ˆæ§åˆ¶å°+æ–‡ä»¶ï¼‰"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path("v13_ofi_ai_system/logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # æ—¥å¿—æ–‡ä»¶åï¼ˆæŒ‰æ—¥æœŸï¼‰
        log_file = log_dir / f"{self.symbol}_{datetime.now().strftime('%Y%m%d')}.log"
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°logger
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        logger.setLevel(logging.INFO)
        
        logger.info(f"æ—¥å¿—ç³»ç»Ÿå·²é…ç½®: {log_file}")
    
    def print_order_book(self, order_book):
        """å®æ—¶æ‰“å°è®¢å•ç°¿æ•°æ®ï¼ˆæ ¼å¼åŒ–æ˜¾ç¤ºï¼‰"""
        print()
        print("=" * 80)
        print(f"ğŸ“Š å®æ—¶è®¢å•ç°¿ - {order_book['symbol']} - Seq: {order_book['seq']}")
        print("=" * 80)
        print(f"â° æ—¶é—´: {order_book['timestamp'].strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        print(f"ğŸ“¡ æ—¶å»¶: {order_book['latency_ms']:.2f}ms")
        print()
        
        # æ‰“å°ä¹°å•
        print("ğŸ’š ä¹°å•ï¼ˆBidsï¼‰:")
        print(f"  {'æ¡£ä½':<8} {'ä»·æ ¼':>12} {'æ•°é‡':>15} {'æ€»é¢':>15}")
        print("-" * 55)
        for i, (price, qty) in enumerate(order_book['bids'], 1):
            total = price * qty
            print(f"  æ¡£ä½{i:<5} {price:>12.2f} {qty:>15.4f} {total:>15.2f}")
        
        print()
        
        # æ‰“å°å–å•
        print("â¤ï¸  å–å•ï¼ˆAsksï¼‰:")
        print(f"  {'æ¡£ä½':<8} {'ä»·æ ¼':>12} {'æ•°é‡':>15} {'æ€»é¢':>15}")
        print("-" * 55)
        for i, (price, qty) in enumerate(order_book['asks'], 1):
            total = price * qty
            print(f"  æ¡£ä½{i:<5} {price:>12.2f} {qty:>15.4f} {total:>15.2f}")
        
        # æ‰“å°ä»·å·®
        spread = order_book['asks'][0][0] - order_book['bids'][0][0]
        mid_price = (order_book['asks'][0][0] + order_book['bids'][0][0]) / 2
        spread_bps = (spread / mid_price) * 10000
        
        print()
        print(f"ğŸ“ˆ ä»·å·®: {spread:.2f} USDT ({spread_bps:.2f} bps)")
        print(f"ğŸ“Š ä¸­é—´ä»·: {mid_price:.2f} USDT")
        print("=" * 80)
        print()
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«åˆ†ä½æ•°å’Œåºåˆ—ä¸€è‡´æ€§ï¼‰"""
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        if elapsed == 0:
            return
        
        rate = self.stats['total_messages'] / elapsed
        avg_latency = sum(self.stats['latency_list']) / len(self.stats['latency_list']) if self.stats['latency_list'] else 0
        
        print()
        print("=" * 80)
        print("ğŸ“Š è¿è¡Œç»Ÿè®¡")
        print("=" * 80)
        print(f"â±ï¸  è¿è¡Œæ—¶é—´: {elapsed:.1f}ç§’")
        print(f"ğŸ“¨ æ¥æ”¶æ¶ˆæ¯: {self.stats['total_messages']} æ¡")
        print(f"âš¡ æ¥æ”¶é€Ÿç‡: {rate:.2f} æ¡/ç§’")
        print(f"ğŸ“¡ å¹³å‡æ—¶å»¶: {avg_latency:.2f}ms")
        
        # æ—¶å»¶åˆ†ä½æ•°ï¼ˆç¡¬æ ‡å‡†2ï¼‰
        if self.stats['latency_list']:
            percentiles = self.calculate_percentiles()
            print(f"ğŸ“Š æ—¶å»¶åˆ†ä½:")
            print(f"   - P50 (ä¸­ä½æ•°): {percentiles['p50']:.2f}ms")
            print(f"   - P95: {percentiles['p95']:.2f}ms")
            print(f"   - P99: {percentiles['p99']:.2f}ms")
            print(f"ğŸ“‰ æœ€å°æ—¶å»¶: {min(self.stats['latency_list']):.2f}ms")
            print(f"ğŸ“ˆ æœ€å¤§æ—¶å»¶: {max(self.stats['latency_list']):.2f}ms")
        
        # åºåˆ—ä¸€è‡´æ€§ç»Ÿè®¡ï¼ˆç¡¬æ ‡å‡†3 - æœŸè´§WSä¸¥æ ¼å¯¹é½ï¼‰
        print(f"ğŸ”— åºåˆ—ä¸€è‡´æ€§ (æœŸè´§WSä¸¥æ ¼å¯¹é½):")
        print(f"   - Gaps (åŒºé—´ç¼ºå£): {self.stats['gaps']} ä¸ªupdateId")
        print(f"   - Max Gap (æœ€å¤§åŒºé—´): {self.stats['max_gap']} ä¸ªupdateId")
        print(f"   - Resync (å¯¹é½ä¸­æ–­): {self.stats['resync']} æ¬¡")
        print(f"   - Reconnects (é‡è¿): {self.stats['reconnects']} æ¬¡")
        
        print(f"ğŸ’¾ ç¼“å­˜æ•°æ®: {len(self.order_book_history)} æ¡")
        print("=" * 80)
        print()
    
    def on_open(self, ws):
        """WebSocketè¿æ¥æˆåŠŸæ—¶çš„å›è°ƒ
        
        Args:
            ws: WebSocketè¿æ¥å¯¹è±¡
        """
        logger.info(f"âœ… WebSocketè¿æ¥æˆåŠŸ: {self.symbol.upper()}")
        logger.info(f"è®¢é˜…è®¢å•ç°¿æµ: {self.depth_levels}æ¡£æ·±åº¦, 100msæ›´æ–°")
    
    def on_message(self, ws, message):
        """æ¥æ”¶åˆ°WebSocketæ¶ˆæ¯æ—¶çš„å›è°ƒ
        
        è¿™ä¸ªæ–¹æ³•åœ¨æ¯æ¬¡æ”¶åˆ°è®¢å•ç°¿æ›´æ–°æ—¶è¢«è°ƒç”¨ï¼ˆçº¦100msä¸€æ¬¡ï¼‰ã€‚
        æ¶ˆæ¯æ ¼å¼ä¸ºJSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«æ—¶é—´æˆ³å’Œè®¢å•ç°¿æ•°æ®ã€‚
        
        Args:
            ws: WebSocketè¿æ¥å¯¹è±¡
            message (str): æ¥æ”¶åˆ°çš„JSONæ¶ˆæ¯
            
        å¸å®‰è®¢å•ç°¿æ¶ˆæ¯æ ¼å¼:
        {
            "e": "depthUpdate",        // äº‹ä»¶ç±»å‹
            "E": 1234567890123,        // äº‹ä»¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            "s": "ETHUSDT",           // äº¤æ˜“å¯¹
            "U": 123456789,           // ç¬¬ä¸€ä¸ªæ›´æ–°ID
            "u": 123456799,           // æœ€åä¸€ä¸ªæ›´æ–°ID
            "b": [                    // ä¹°å•ï¼ˆbidsï¼‰
                ["3245.50", "10.5"],  // [ä»·æ ¼, æ•°é‡]
                ["3245.40", "8.3"],
                ...
            ],
            "a": [                    // å–å•ï¼ˆasksï¼‰
                ["3245.60", "11.2"],
                ["3245.70", "9.5"],
                ...
            ]
        }
        """
        try:
            # 1. éªŒè¯æ¶ˆæ¯æ ¼å¼
            if not message or not isinstance(message, str):
                logger.warning(f"æ”¶åˆ°æ— æ•ˆæ¶ˆæ¯æ ¼å¼: {type(message)}")
                return
            
            # 2. è§£æJSONæ•°æ®
            try:
                data = json.loads(message)
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                return
            
            # 3. éªŒè¯å¿…éœ€å­—æ®µ
            if 'E' not in data or 'b' not in data or 'a' not in data:
                logger.warning(f"æ¶ˆæ¯ç¼ºå°‘å¿…éœ€å­—æ®µ: {data.keys()}")
                return
            
            # 4. è®¡ç®—æ¥æ”¶æ—¶å»¶ï¼ˆå¢å¼ºç‰ˆï¼‰
            receive_time = datetime.now()
            ts_recv = receive_time.timestamp() * 1000  # æ¥æ”¶æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
            timestamp_ms = data['E']  # äº‹ä»¶æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
            timestamp = datetime.fromtimestamp(timestamp_ms / 1000.0)
            
            # è®¡ç®—ä¸¤ç§æ—¶å»¶
            latency_event_ms = (receive_time - timestamp).total_seconds() * 1000  # äº‹ä»¶æ—¶å»¶
            pipeline_start = datetime.now()
            
            # 5. é€’å¢åºåˆ—å·
            self.message_seq += 1
            
            # 6. æå–æ›´æ–°IDå­—æ®µï¼ˆU, uï¼‰
            U = data.get('U', 0)  # ç¬¬ä¸€ä¸ªæ›´æ–°ID
            u = data.get('u', 0)   # æœ€åä¸€ä¸ªæ›´æ–°ID
            pu = data.get('pu', None)  # æ¶ˆæ¯è‡ªå¸¦çš„puï¼ˆå®é™…ä¸å­˜åœ¨ï¼Œè¿™é‡Œç”¨last_uæ¨¡æ‹Ÿï¼‰
            
            # 7. æœŸè´§WSä¸¥æ ¼å¯¹é½æ£€æµ‹ï¼ˆpu == last_u è¿ç»­æ€§ï¼‰
            if self.stats['last_u'] is not None:
                # æ£€æŸ¥è¿ç»­æ€§ï¼špuåº”è¯¥ç­‰äºlast_u
                if pu is None:
                    # å¸å®‰å®é™…ä¸å‘é€puï¼Œéœ€è¦è‡ªå·±æ£€æµ‹ U == last_u + 1
                    pu_expected = self.stats['last_u']
                    if U != pu_expected + 1:
                        # è§¦å‘resync
                        self.stats['resync'] += 1
                        logger.warning(f"âš ï¸ Resyncè§¦å‘! last_u={self.stats['last_u']}, U={U}, gap={U - self.stats['last_u'] - 1}")
            
            # 8. è®¡ç®—è¿ç»­åŒºé—´å†…çš„ç¼ºå£ï¼ˆu - U + 1 = å®é™…æ›´æ–°æ•°ï¼Œç†æƒ³åº”è¯¥è¿ç»­ï¼‰
            # å¦‚æœåŒºé—´å†…æœ‰ç¼ºå£ï¼Œè¯´æ˜æŸäº›updateIdè¢«è·³è¿‡
            interval_updates = u - U + 1  # åŒºé—´åŒ…å«çš„æ›´æ–°IDæ•°é‡
            # å®é™…ä¸Šæ¯ä¸ªæ¶ˆæ¯éƒ½æ˜¯èšåˆçš„ï¼Œä¸ä¸€å®šè¿ç»­ï¼Œä½†æˆ‘ä»¬ç»Ÿè®¡åŒºé—´å†…ç†è®ºç¼ºå£
            if interval_updates > 1:
                # åŒºé—´å†…çš„ç¼ºå£ = (u - U) - èšåˆæ•° + 1ï¼Œè¿™é‡Œç®€åŒ–ä¸º u - Uï¼ˆå› ä¸ºç†æƒ³è¿ç»­æ—¶u-U=0ï¼‰
                interval_gap = u - U  # åŒºé—´è·¨åº¦ï¼ˆ0è¡¨ç¤ºå•ä¸ªæ›´æ–°ï¼Œ>0è¡¨ç¤ºæœ‰ç¼ºå£ï¼‰
                if interval_gap > 0:
                    self.stats['gaps'] += interval_gap
                    self.stats['max_gap'] = max(self.stats['max_gap'], interval_gap)
            
            # æ›´æ–°last_uå’Œpu
            self.stats['last_u'] = u
            prev_update_id = self.last_update_id
            self.last_update_id = u
            
            # 5. æå–ä¹°å•ï¼ˆbidsï¼‰- 5æ¡£
            bids = []
            for bid in data['b'][:self.depth_levels]:
                price = float(bid[0])
                quantity = float(bid[1])
                bids.append([price, quantity])
            
            # 6. æå–å–å•ï¼ˆasksï¼‰- 5æ¡£
            asks = []
            for ask in data['a'][:self.depth_levels]:
                price = float(ask[0])
                quantity = float(ask[1])
                asks.append([price, quantity])
            
            # 7. éªŒè¯æ•°æ®å®Œæ•´æ€§
            if len(bids) < self.depth_levels or len(asks) < self.depth_levels:
                logger.warning(f"è®¢å•ç°¿æ·±åº¦ä¸è¶³: bids={len(bids)}, asks={len(asks)}")
                return
            
            # 8. è®¡ç®—ç®¡é“æ—¶å»¶
            latency_pipeline_ms = (datetime.now() - pipeline_start).total_seconds() * 1000
            
            # 9. æ„å»ºè®¢å•ç°¿æ•°æ®ç»“æ„ï¼ˆå®Œæ•´ç‰ˆ - æ»¡è¶³NDJSONå­—æ®µè¦æ±‚ï¼‰
            order_book = {
                'seq': self.message_seq,  # åºåˆ—å·
                'timestamp': timestamp,
                'symbol': self.symbol.upper(),
                'bids': bids,
                'asks': asks,
                # æ–°å¢å®Œæ•´å­—æ®µ
                'ts_recv': ts_recv,  # æ¥æ”¶æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                'E': timestamp_ms,  # äº‹ä»¶æ—¶é—´ï¼ˆä¿ç•™åŸå­—æ®µåï¼‰
                'U': U,  # ç¬¬ä¸€ä¸ªæ›´æ–°ID
                'u': u,  # æœ€åä¸€ä¸ªæ›´æ–°ID
                'pu': prev_update_id,  # ä¸Šä¸€ä¸ªæ›´æ–°IDï¼ˆå®é™…æ˜¯ä¸Šä¸€æ¡æ¶ˆæ¯çš„uï¼‰
                'latency_event_ms': round(latency_event_ms, 2),  # äº‹ä»¶æ—¶å»¶
                'latency_pipeline_ms': round(latency_pipeline_ms, 2),  # ç®¡é“æ—¶å»¶
                # ä¿ç•™å…¼å®¹å­—æ®µ
                'event_time': timestamp_ms,
                'latency_ms': round(latency_event_ms, 2),
                'receive_time': receive_time
            }
            
            # 10. æ›´æ–°ç»Ÿè®¡æ•°æ®
            self.stats['total_messages'] += 1
            self.stats['latency_list'].append(latency_event_ms)
            # åªä¿ç•™æœ€è¿‘1000ä¸ªæ—¶å»¶æ•°æ®ï¼ˆæ»šåŠ¨çª—å£ï¼‰
            if len(self.stats['latency_list']) > 1000:
                self.stats['latency_list'] = self.stats['latency_list'][-1000:]
            
            # 10. å­˜å‚¨åˆ°å†å²è®°å½•
            self.order_book_history.append(order_book)
            
            # 11. å®æ—¶å†™å…¥NDJSON
            self._write_to_ndjson(order_book)
            
            # 12. å®šæœŸæ‰“å°è®¢å•ç°¿å’Œä¿å­˜æŒ‡æ ‡ï¼ˆæ¯10ç§’ä¸€æ¬¡ï¼‰
            time_since_print = (datetime.now() - self.stats['last_print_time']).total_seconds()
            if time_since_print >= 10:
                self.print_order_book(order_book)
                self.print_statistics()
                self.save_metrics_json()  # æ–°å¢ï¼šä¿å­˜metrics.json
                self.stats['last_print_time'] = datetime.now()
            
            # 13. æ—¥å¿—è®°å½•ï¼ˆæ¯100æ¡ä¸€æ¬¡ï¼‰
            if self.stats['total_messages'] % 100 == 0:
                logger.info(f"å·²æ¥æ”¶ {self.stats['total_messages']} æ¡è®¢å•ç°¿æ•°æ®, "
                           f"é€Ÿç‡: {self.stats['total_messages'] / (datetime.now() - self.stats['start_time']).total_seconds():.2f} æ¡/ç§’")
                
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}", exc_info=True)
    
    def on_error(self, ws, error):
        """WebSocketè¿æ¥å‡ºé”™æ—¶çš„å›è°ƒ
        
        Args:
            ws: WebSocketè¿æ¥å¯¹è±¡
            error: é”™è¯¯å¯¹è±¡æˆ–é”™è¯¯æ¶ˆæ¯
        """
        logger.error(f"âŒ WebSocketé”™è¯¯: {error}")
        
        # æ ¹æ®é”™è¯¯ç±»å‹è¿›è¡Œåˆ†ç±»å¤„ç†
        error_str = str(error)
        if "Connection refused" in error_str:
            logger.error("è¿æ¥è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–URL")
        elif "timeout" in error_str.lower():
            logger.error("è¿æ¥è¶…æ—¶ï¼Œå¯èƒ½ç½‘ç»œä¸ç¨³å®š")
        elif "SSL" in error_str or "Certificate" in error_str:
            logger.error("SSLè¯ä¹¦é”™è¯¯")
        else:
            logger.error(f"æœªçŸ¥é”™è¯¯ç±»å‹: {error_str}")
    
    def on_close(self, ws, close_status_code, close_msg):
        """WebSocketè¿æ¥å…³é—­æ—¶çš„å›è°ƒ
        
        Args:
            ws: WebSocketè¿æ¥å¯¹è±¡
            close_status_code: å…³é—­çŠ¶æ€ç 
            close_msg: å…³é—­æ¶ˆæ¯
        """
        logger.warning(f"âš ï¸ WebSocketè¿æ¥å·²å…³é—­")
        logger.warning(f"çŠ¶æ€ç : {close_status_code}")
        logger.warning(f"å…³é—­æ¶ˆæ¯: {close_msg}")
        
        # è®°å½•é‡è¿æ¬¡æ•°ï¼ˆç¡¬æ ‡å‡†3ï¼‰
        self.stats['reconnects'] += 1
        logger.info(f"é‡è¿æ¬¡æ•°: {self.stats['reconnects']}")
        
        # è®°å½•è¿æ¥ç»Ÿè®¡
        total_records = len(self.order_book_history)
        logger.info(f"æœ¬æ¬¡ä¼šè¯å…±æ¥æ”¶ {total_records} æ¡è®¢å•ç°¿æ•°æ®")
    
    def get_latest_order_book(self):
        """è·å–æœ€æ–°çš„è®¢å•ç°¿æ•°æ®
        
        Returns:
            dict: æœ€æ–°çš„è®¢å•ç°¿æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰æ•°æ®åˆ™è¿”å›None
            
        Example:
            >>> client = BinanceOrderBookStream()
            >>> # ... è¿è¡Œä¸€æ®µæ—¶é—´å ...
            >>> latest = client.get_latest_order_book()
            >>> if latest:
            ...     print(f"Bid1: {latest['bids'][0]}")
            ...     print(f"Ask1: {latest['asks'][0]}")
        """
        if len(self.order_book_history) == 0:
            return None
        return self.order_book_history[-1]
    
    def get_order_book_count(self):
        """è·å–å·²æ¥æ”¶çš„è®¢å•ç°¿æ•°æ®æ€»æ•°
        
        Returns:
            int: è®¢å•ç°¿æ•°æ®æ•°é‡
        """
        return len(self.order_book_history)
    
    def calculate_percentiles(self):
        """è®¡ç®—æ—¶å»¶åˆ†ä½æ•°ï¼ˆç¡¬æ ‡å‡†2ï¼šp50/p95/p99ï¼‰
        
        Returns:
            dict: åŒ…å«p50, p95, p99çš„å­—å…¸
        """
        if not self.stats['latency_list']:
            return {'p50': 0, 'p95': 0, 'p99': 0}
        
        latencies = np.array(self.stats['latency_list'])
        
        return {
            'p50': np.percentile(latencies, 50),
            'p95': np.percentile(latencies, 95),
            'p99': np.percentile(latencies, 99)
        }
    
    def save_metrics_json(self):
        """ä¿å­˜æŒ‡æ ‡åˆ°metrics.jsonæ–‡ä»¶ï¼ˆç¡¬æ ‡å‡†4ï¼šå‘¨æœŸäº§ç‰©ï¼‰
        
        æ¯10ç§’åˆ·æ–°ä¸€æ¬¡ï¼Œä¿å­˜å½“å‰è¿è¡Œç»Ÿè®¡å’Œåˆ†ä½æ•°
        """
        try:
            elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
            rate = self.stats['total_messages'] / elapsed if elapsed > 0 else 0
            
            # è®¡ç®—åˆ†ä½æ•°
            percentiles = self.calculate_percentiles()
            
            # æ„å»ºæŒ‡æ ‡æ•°æ®
            metrics = {
                'timestamp': datetime.now().isoformat(),
                'runtime_seconds': round(elapsed, 2),
                'total_messages': self.stats['total_messages'],
                'message_rate': round(rate, 2),
                'latency': {
                    'avg_ms': round(sum(self.stats['latency_list']) / len(self.stats['latency_list']), 2) if self.stats['latency_list'] else 0,
                    'min_ms': round(min(self.stats['latency_list']), 2) if self.stats['latency_list'] else 0,
                    'max_ms': round(max(self.stats['latency_list']), 2) if self.stats['latency_list'] else 0,
                    'p50_ms': round(percentiles['p50'], 2),
                    'p95_ms': round(percentiles['p95'], 2),
                    'p99_ms': round(percentiles['p99'], 2)
                },
                'sequence_consistency': {
                    'gaps': self.stats['gaps'],
                    'max_gap': self.stats['max_gap'],
                    'resync': self.stats['resync'],
                    'reconnects': self.stats['reconnects']
                },
                'cache_size': len(self.order_book_history),
                'symbol': self.symbol.upper()
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            metrics_file = self.data_dir / 'metrics.json'
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            
            logger.debug(f"æŒ‡æ ‡å·²ä¿å­˜åˆ° {metrics_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜metrics.jsonå¤±è´¥: {e}", exc_info=True)
    
    def _write_to_ndjson(self, order_book):
        """å®æ—¶å†™å…¥NDJSONæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        
        Args:
            order_book (dict): è®¢å•ç°¿æ•°æ®
            
        Note:
            NDJSONæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¾¿äºæµå¼å¤„ç†å’Œå›æ”¾
            å®Œæ•´å­—æ®µï¼šts_recv, E, U, u, pu, latency_event_ms, latency_pipeline_ms
        """
        try:
            # ç”Ÿæˆä»Šå¤©çš„æ–‡ä»¶å
            date_str = datetime.now().strftime('%Y%m%d')
            ndjson_file = self.ndjson_dir / f"{self.symbol}_{date_str}.ndjson"
            
            # å‡†å¤‡å†™å…¥çš„æ•°æ®ï¼ˆå®Œæ•´ç‰ˆï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼‰
            record = {
                'seq': order_book['seq'],
                'timestamp': order_book['timestamp'].isoformat(),
                'symbol': order_book['symbol'],
                'bids': order_book['bids'],
                'asks': order_book['asks'],
                # å¿…éœ€å­—æ®µï¼ˆç¡¬æ ‡å‡†1ï¼‰
                'ts_recv': order_book['ts_recv'],
                'E': order_book['E'],
                'U': order_book['U'],
                'u': order_book['u'],
                'pu': order_book['pu'],
                'latency_event_ms': order_book['latency_event_ms'],
                'latency_pipeline_ms': order_book['latency_pipeline_ms']
            }
            
            # è¿½åŠ å†™å…¥ï¼ˆæ¯è¡Œä¸€ä¸ªJSONï¼‰
            with open(ndjson_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
                
        except Exception as e:
            logger.error(f"å†™å…¥NDJSONå¤±è´¥: {e}", exc_info=True)
    
    def convert_ndjson_to_parquet(self, ndjson_file=None):
        """å°†NDJSONæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼
        
        Args:
            ndjson_file (Path): NDJSONæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤è½¬æ¢ä»Šå¤©çš„æ–‡ä»¶
            
        Returns:
            str: ç”Ÿæˆçš„Parquetæ–‡ä»¶è·¯å¾„
            
        Note:
            Parquetåˆ—å¼å­˜å‚¨ï¼Œå‹ç¼©ç‡é«˜ï¼ŒæŸ¥è¯¢å¿«ï¼Œé€‚åˆOFIè®¡ç®—
        """
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶ï¼Œä½¿ç”¨ä»Šå¤©çš„
            if ndjson_file is None:
                date_str = datetime.now().strftime('%Y%m%d')
                ndjson_file = self.ndjson_dir / f"{self.symbol}_{date_str}.ndjson"
            
            if not ndjson_file.exists():
                logger.warning(f"NDJSONæ–‡ä»¶ä¸å­˜åœ¨: {ndjson_file}")
                return None
            
            # è¯»å–NDJSONæ–‡ä»¶
            records = []
            with open(ndjson_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        record = json.loads(line)
                        # å±•å¹³æ•°æ®ç»“æ„
                        flat_record = {
                            'seq': record['seq'],
                            'timestamp': record['timestamp'],
                            'event_time': record['event_time'],
                            'latency_ms': record['latency_ms'],
                            'symbol': record['symbol']
                        }
                        
                        # æ·»åŠ 5æ¡£ä¹°å•
                        for i, (price, qty) in enumerate(record['bids'], 1):
                            flat_record[f'bid_price_{i}'] = price
                            flat_record[f'bid_qty_{i}'] = qty
                        
                        # æ·»åŠ 5æ¡£å–å•
                        for i, (price, qty) in enumerate(record['asks'], 1):
                            flat_record[f'ask_price_{i}'] = price
                            flat_record[f'ask_qty_{i}'] = qty
                        
                        records.append(flat_record)
            
            if not records:
                logger.warning("NDJSONæ–‡ä»¶ä¸ºç©º")
                return None
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(records)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # ç”ŸæˆParquetæ–‡ä»¶å
            date_str = datetime.now().strftime('%Y%m%d')
            parquet_file = self.parquet_dir / f"{self.symbol}_{date_str}.parquet"
            
            # ä¿å­˜ä¸ºParquetï¼ˆä½¿ç”¨snappyå‹ç¼©ï¼‰
            df.to_parquet(parquet_file, engine='pyarrow', compression='snappy', index=False)
            
            logger.info(f"âœ… NDJSONâ†’Parquetè½¬æ¢æˆåŠŸ: {parquet_file} ({len(df)} æ¡è®°å½•)")
            return str(parquet_file)
            
        except Exception as e:
            logger.error(f"NDJSONâ†’Parquetè½¬æ¢å¤±è´¥: {e}", exc_info=True)
            return None
    
    def save_to_csv(self, force=False):
        """ä¿å­˜è®¢å•ç°¿æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            force (bool): æ˜¯å¦å¼ºåˆ¶ä¿å­˜ï¼ˆå¿½ç•¥æ—¶é—´é—´éš”ï¼‰
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰ä¿å­˜åˆ™è¿”å›None
            
        Note:
            é»˜è®¤æ¯60ç§’è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹IOæ“ä½œ
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
        if not force:
            time_since_last_save = (datetime.now() - self.last_save_time).total_seconds()
            if time_since_last_save < self.save_interval:
                return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if len(self.order_book_history) == 0:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
        
        try:
            # å°†æ•°æ®è½¬æ¢ä¸ºDataFrameæ ¼å¼
            data_list = []
            for ob in self.order_book_history:
                # å±•å¹³è®¢å•ç°¿æ•°æ®
                row = {
                    'timestamp': ob['timestamp'],
                    'symbol': ob['symbol'],
                    'event_time': ob['event_time']
                }
                
                # æ·»åŠ ä¹°å•æ•°æ®ï¼ˆ5æ¡£ï¼‰
                for i, (price, qty) in enumerate(ob['bids'], 1):
                    row[f'bid_price_{i}'] = price
                    row[f'bid_qty_{i}'] = qty
                
                # æ·»åŠ å–å•æ•°æ®ï¼ˆ5æ¡£ï¼‰
                for i, (price, qty) in enumerate(ob['asks'], 1):
                    row[f'ask_price_{i}'] = price
                    row[f'ask_qty_{i}'] = qty
                
                data_list.append(row)
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data_list)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆæŒ‰æ—¥æœŸå’Œæ—¶é—´ï¼‰
            filename = f"{self.symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            filepath = self.data_dir / filename
            
            # ä¿å­˜åˆ°CSV
            df.to_csv(filepath, index=False)
            
            # æ›´æ–°æœ€åä¿å­˜æ—¶é—´
            self.last_save_time = datetime.now()
            
            logger.info(f"âœ… æ•°æ®å·²ä¿å­˜: {filepath} ({len(df)} æ¡è®°å½•)")
            return str(filepath)
            
        except Exception as e:
            logger.error(f"ä¿å­˜CSVå¤±è´¥: {e}", exc_info=True)
            return None
    
    def auto_save_loop(self):
        """è‡ªåŠ¨ä¿å­˜å¾ªç¯ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        while self.ws and self.ws.keep_running:
            try:
                # æ¯60ç§’å°è¯•ä¿å­˜ä¸€æ¬¡
                import time
                time.sleep(self.save_interval)
                self.save_to_csv(force=False)
            except Exception as e:
                logger.error(f"è‡ªåŠ¨ä¿å­˜å¼‚å¸¸: {e}")
                break
    
    def run(self, reconnect=True):
        """å¯åŠ¨WebSocketè¿æ¥
        
        è¿™ä¸ªæ–¹æ³•ä¼šå»ºç«‹åˆ°å¸å®‰WebSocketçš„è¿æ¥ï¼Œå¹¶å¼€å§‹æ¥æ”¶è®¢å•ç°¿æ•°æ®ã€‚
        è¿æ¥æ˜¯é˜»å¡çš„ï¼Œä¼šä¸€ç›´è¿è¡Œç›´åˆ°æ‰‹åŠ¨åœæ­¢æˆ–å‘ç”Ÿé”™è¯¯ã€‚
        
        Args:
            reconnect (bool): æ˜¯å¦åœ¨æ–­çº¿åè‡ªåŠ¨é‡è¿ï¼Œé»˜è®¤True
            
        Example:
            >>> client = BinanceOrderBookStream('ethusdt')
            >>> client.run()  # å¼€å§‹æ¥æ”¶æ•°æ®ï¼Œé˜»å¡è¿è¡Œ
        """
        logger.info("=" * 60)
        logger.info(f"å¯åŠ¨å¸å®‰WebSocketå®¢æˆ·ç«¯")
        logger.info(f"äº¤æ˜“å¯¹: {self.symbol.upper()}")
        logger.info(f"è®¢å•ç°¿æ·±åº¦: {self.depth_levels}æ¡£")
        logger.info(f"WebSocket URL: {self.ws_url}")
        logger.info(f"è‡ªåŠ¨é‡è¿: {'å¼€å¯' if reconnect else 'å…³é—­'}")
        logger.info("=" * 60)
        
        # åˆ›å»ºWebSocketè¿æ¥
        self.ws = websocket.WebSocketApp(
            self.ws_url,
            on_open=self.on_open,
            on_message=self.on_message,
            on_error=self.on_error,
            on_close=self.on_close
        )
        
        # å¯åŠ¨è¿æ¥ï¼ˆé˜»å¡è¿è¡Œï¼‰
        try:
            self.ws.run_forever(
                reconnect=5 if reconnect else 0  # é‡è¿é—´éš”5ç§’
            )
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨å…³é—­è¿æ¥...")
            self.ws.close()
        except Exception as e:
            logger.error(f"è¿è¡Œæ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            raise

