#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CVD Calculation Test Analyzer (v2.2, ä¼˜åŒ–ç‰ˆ)
- ä¿®å¤é˜Ÿåˆ—ä¸¢å¼ƒç‡è®¡ç®—é—®é¢˜
- ä¼˜åŒ–ä¸€è‡´æ€§éªŒè¯é€»è¾‘
- æ”¹è¿›Z-scoreé˜ˆå€¼åˆ¤æ–­
- å¢å¼ºæ•°æ®è´¨é‡æ£€æŸ¥
"""
import argparse
import io
import json
import os
from pathlib import Path

import numpy as np
import pandas as pd

# æ— äº¤äº’ç¯å¢ƒä¸‹å®‰å…¨ä½¿ç”¨matplotlib
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

import sys

# æ›´å®‰å…¨çš„UTF-8è¾“å‡ºè®¾ç½®
try:
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8")
        sys.stderr.reconfigure(encoding="utf-8")
except Exception:
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")
    except Exception:
        pass

# -------------------------
# Helpers
# -------------------------
def ensure_bool_series(s, default=False):
    """å°†ä»»æ„Seriesè½¬ä¸ºå¸ƒå°”ï¼Œç¼ºå¤±åˆ™è¿”å›å¸¸é‡å¸ƒå°”Seriesã€‚"""
    if s is None:
        return pd.Series([default], dtype=bool).iloc[:0]
    if s.dtype == bool:
        return s
    return s.fillna(False).astype(int).astype(bool)

def require_columns(df, cols):
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise KeyError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing}")

def optional_column(df, name, default=None):
    return df[name] if name in df.columns else pd.Series(default, index=df.index)

def convert_to_native(obj):
    if isinstance(obj, dict):
        return {k: convert_to_native(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_native(x) for x in obj]
    elif isinstance(obj, (np.integer, np.floating)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    else:
        return obj

def calculate_queue_dropped_rate(df):
    """è®¡ç®—é˜Ÿåˆ—ä¸¢å¼ƒç‡ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    qd = optional_column(df, "queue_dropped")
    if qd is None or qd.isna().all():
        return None, True
    
    # ä¿®å¤ï¼šä½¿ç”¨æœ€å¤§å€¼è€Œä¸æ˜¯å¢é‡æ±‚å’Œ
    max_dropped = pd.to_numeric(qd, errors="coerce").max()
    if pd.isna(max_dropped):
        return None, True
    
    # é˜Ÿåˆ—ä¸¢å¼ƒç‡ = æœ€å¤§ä¸¢å¼ƒæ•° / æ€»æ¶ˆæ¯æ•°
    total_messages = optional_column(df, "total_messages")
    if total_messages is not None and not total_messages.isna().all():
        max_total = pd.to_numeric(total_messages, errors="coerce").max()
        if not pd.isna(max_total) and max_total > 0:
            rate = float(max_dropped) / float(max_total)
            return rate, rate <= 0.005  # 0.5%
    
    # å¦‚æœæ²¡æœ‰total_messagesï¼Œä½¿ç”¨æ•°æ®ç‚¹æ•°ä½œä¸ºåˆ†æ¯
    rate = float(max_dropped) / max(len(df), 1)
    return rate, rate <= 0.005

def validate_cvd_consistency(df):
    """éªŒè¯CVDä¸€è‡´æ€§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    try:
        require_columns(df, ["cvd", "qty", "is_buy"])
    except KeyError as e:
        print(f"è­¦å‘Š: {e}ï¼Œè·³è¿‡ä¸€è‡´æ€§éªŒè¯")
        return {
            "sample_size": 0,
            "continuity_mismatches": 0,
            "conservation_error": 0.0,
            "conservation_tolerance": 1e-6,
            "pass": True,
            "method": "è·³è¿‡ï¼ˆç¼ºå°‘å­—æ®µï¼‰",
        }
    
    CHECK_THRESHOLD = 10_000
    MIN_SAMPLE = 100  # é™ä½æœ€å°æ ·æœ¬æ•°
    
    if len(df) <= CHECK_THRESHOLD:
        df_sample = df
        check_method = "å…¨é‡"
    else:
        size = max(int(len(df) * 0.01), MIN_SAMPLE)
        rng = np.random.default_rng(42)
        idx = np.sort(rng.choice(len(df), size=size, replace=False))
        df_sample = df.iloc[idx]
        check_method = f"æŠ½æ ·({len(df_sample)})"
    
    s_cvd = pd.to_numeric(df_sample["cvd"], errors="coerce")
    s_qty = pd.to_numeric(df_sample["qty"], errors="coerce")
    s_buy = ensure_bool_series(df_sample["is_buy"])
    
    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
    if s_cvd.isna().any() or s_qty.isna().any():
        print("è­¦å‘Š: CVDæˆ–qtyæ•°æ®åŒ…å«NaNï¼Œè·³è¿‡ä¸€è‡´æ€§éªŒè¯")
        return {
            "sample_size": len(df_sample),
            "continuity_mismatches": 0,
            "conservation_error": 0.0,
            "conservation_tolerance": 1e-6,
            "pass": True,
            "method": f"{check_method}ï¼ˆè·³è¿‡NaNæ•°æ®ï¼‰",
        }
    
    # è®¡ç®—é¢„æœŸå˜åŒ–
    delta_expected = np.where(s_buy, s_qty, -s_qty)
    cvd_prev = s_cvd.shift(1)
    
    # æ”¾å®½å®¹å·®ï¼Œå› ä¸ºæˆ‘ä»¬çš„qtyæ˜¯æ¨¡æ‹Ÿçš„
    tolerance = 1e-6
    continuity_err = (np.abs((s_cvd - cvd_prev) - delta_expected) > tolerance)
    continuity_mismatches = int(continuity_err.iloc[1:].sum())  # è·³è¿‡é¦–ç¬”
    
    # é¦–å°¾å®ˆæ’æ£€æŸ¥
    cvd_first = float(s_cvd.iloc[0])
    cvd_last = float(s_cvd.iloc[-1])
    sum_deltas = float(np.where(s_buy, s_qty, -s_qty).sum())
    conservation_error = abs(cvd_last - cvd_first - sum_deltas)
    conservation_tolerance = max(1e-3, 1e-6 * abs(cvd_last - cvd_first))  # æ”¾å®½å®¹å·®
    
    return {
        "sample_size": int(len(df_sample)),
        "continuity_mismatches": int(continuity_mismatches),
        "conservation_error": float(conservation_error),
        "conservation_tolerance": float(conservation_tolerance),
        "pass": (continuity_mismatches == 0) and (conservation_error < conservation_tolerance),
        "method": check_method,
    }

# -------------------------
# Main
# -------------------------
def main():
    parser = argparse.ArgumentParser(description="Analyze CVD data collected from trade stream (v2.2)")
    parser.add_argument("--data", "--input", dest="data", required=True,
                        help="è·¯å¾„: å•ä¸ªparquetæ–‡ä»¶æˆ–ç›®å½•")
    parser.add_argument("--out", "--output-dir", dest="out", default="figs_cvd",
                        help="å›¾è¡¨è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: figs_cvdï¼‰")
    parser.add_argument("--report", default="docs/reports/CVD_TEST_REPORT.md",
                        help="MarkdownæŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤: docs/reports/CVD_TEST_REPORT.mdï¼‰")
    parser.add_argument("--merge-files", action="store_true",
                        help="å½“ --data æ˜¯ç›®å½•æ—¶ï¼Œåˆå¹¶è¯¥ç›®å½•ä¸‹æ‰€æœ‰parquetè¿›è¡Œåˆ†æï¼ˆé»˜è®¤ä»…åˆ†ææœ€æ–°æ–‡ä»¶ï¼‰")
    parser.add_argument("--seed", type=int, default=42, help="é‡‡æ ·éšæœºç§å­ï¼ˆé»˜è®¤=42ï¼Œå¯å¤ç°ï¼‰")
    parser.add_argument("--plots-sample", type=int, default=10_000, help="ç»˜å›¾é‡‡æ ·ä¸Šé™ï¼ˆé»˜è®¤=10000ï¼‰")
    parser.add_argument("--relaxed-zscore", action="store_true",
                        help="ä½¿ç”¨å®½æ¾çš„Z-scoreé˜ˆå€¼ï¼ˆP(|Z|>2)â‰¤15%, P(|Z|>3)â‰¤5%ï¼‰")
    args = parser.parse_args()

    data_path = Path(args.data)
    if not data_path.exists():
        print(f"é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        sys.exit(2)

    # -------------------------
    # è¯»æ•°
    # -------------------------
    if data_path.is_dir():
        parquet_files = sorted(list(data_path.glob("*.parquet")), key=lambda p: p.stat().st_mtime, reverse=True)
        if not parquet_files:
            print(f"é”™è¯¯: åœ¨ {data_path} ä¸­æœªæ‰¾åˆ° parquet æ–‡ä»¶")
            sys.exit(1)

        if args.merge_files:
            dfs = []
            for p in parquet_files:
                try:
                    df_i = pd.read_parquet(p)
                    df_i["run_id"] = p.stem
                    df_i["source_file"] = str(p.name)
                    dfs.append(df_i)
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶: {p.name} - {e}")
            if not dfs:
                print("é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„æ•°æ®æ–‡ä»¶")
                sys.exit(1)
            df = pd.concat(dfs, ignore_index=True)
            print(f"âœ“ åˆå¹¶ {len(parquet_files)} ä¸ªæ–‡ä»¶, æ€»è®°å½• {len(df):,}")
        else:
            latest_file = parquet_files[0]
            print(f"ğŸ¯ é»˜è®¤åˆ†ææœ€æ–°æ–‡ä»¶: {latest_file.name}")
            df = pd.read_parquet(latest_file)
            df["run_id"] = latest_file.stem
    else:
        df = pd.read_parquet(data_path)
        df["run_id"] = data_path.stem

    print(f"\næ€»æ•°æ®ç‚¹æ•°: {len(df):,}")

    # -------------------------
    # æ’åºï¼ˆåŒé”®/é™çº§ï¼‰
    # -------------------------
    if "agg_trade_id" in df.columns and "event_time_ms" in df.columns:
        df = df.sort_values(["event_time_ms", "agg_trade_id"], kind="mergesort").reset_index(drop=True)
        print("âœ“ å·²æŒ‰ (event_time_ms, agg_trade_id) æ’åº")
    elif "timestamp" in df.columns:
        df = df.sort_values("timestamp", kind="mergesort").reset_index(drop=True)
        print("âœ“ å·²æŒ‰ timestamp æ’åº")
    else:
        print("é”™è¯¯: ç¼ºå°‘æ’åºæ‰€éœ€å­—æ®µï¼ˆevent_time_ms/agg_trade_id æˆ– timestampï¼‰")
        sys.exit(1)

    # -------------------------
    # ç›®å½•å‡†å¤‡
    # -------------------------
    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)
    report_path = Path(args.report)
    report_path.parent.mkdir(parents=True, exist_ok=True)

    results = {}

    # -------------------------
    # 1) æ—¶é•¿ä¸è¿ç»­æ€§
    # -------------------------
    if "timestamp" not in df.columns:
        print("é”™è¯¯: ç¼ºå°‘ timestamp åˆ—")
        sys.exit(1)
    
    ts = pd.to_numeric(df["timestamp"], errors="coerce")
    if ts.isna().all():
        print("é”™è¯¯: timestamp åˆ—æ— æ³•è§£æä¸ºæ•°å€¼")
        sys.exit(1)
    
    time_span_seconds = float(ts.max() - ts.min())
    time_span_hours = time_span_seconds / 3600.0
    ts_diff_ms = ts.diff().mul(1000).dropna()

    results["total_points"] = int(len(df))
    results["time_span_hours"] = float(time_span_hours)
    results["time_span_minutes"] = float(time_span_hours * 60)

    gap_p99 = float(ts_diff_ms.quantile(0.99)) if len(ts_diff_ms) else 0.0
    gaps_over_10s = int((ts_diff_ms > 10_000).sum())
    max_gap = float(ts_diff_ms.max()) if len(ts_diff_ms) else 0.0

    results["gap_p99_ms"] = gap_p99
    results["gaps_over_10s"] = gaps_over_10s
    results["max_gap_ms"] = max_gap

    # åŸºçº¿ï¼ˆCVDåˆ†æï¼‰ï¼šâ‰¥30åˆ†é’Ÿï¼Œè¿ç»­æ€§ p99â‰¤5s ä¸”æ— 10ç§’ç©ºçª—
    results["duration_pass"] = time_span_hours >= 0.5
    results["continuity_pass"] = (gap_p99 <= 5000.0) and (gaps_over_10s == 0)

    # -------------------------
    # 2) æ•°æ®è´¨é‡ï¼ˆä¿¡æ¯é¡¹ï¼‰
    # -------------------------
    parse_errors = 0
    results["parse_errors"] = parse_errors
    results["parse_errors_pass"] = (parse_errors == 0)
    
    # ä¿®å¤é˜Ÿåˆ—ä¸¢å¼ƒç‡è®¡ç®—
    queue_dropped_rate, queue_dropped_pass = calculate_queue_dropped_rate(df)
    results["queue_dropped_rate"] = queue_dropped_rate
    results["queue_dropped_pass"] = queue_dropped_pass

    # -------------------------
    # 3) æ€§èƒ½ï¼ˆä¿¡æ¯é¡¹ï¼‰
    # -------------------------
    lat = optional_column(df, "latency_ms")
    if lat is not None and not pd.to_numeric(lat, errors="coerce").dropna().empty:
        lat = pd.to_numeric(lat, errors="coerce").dropna()
        results["latency_p50"] = float(lat.quantile(0.50))
        results["latency_p95"] = float(lat.quantile(0.95))
        results["latency_p99"] = float(lat.quantile(0.99))
        results["latency_pass"] = True
    else:
        results["latency_pass"] = False

    # -------------------------
    # 4) CVD Z-score ç¨³å¥æ€§ï¼ˆCVD-onlyæ ‡å‡†ï¼‰
    # -------------------------
    if "z_cvd" not in df.columns:
        print("é”™è¯¯: ç¼ºå°‘ z_cvd åˆ—")
        sys.exit(1)

    warmup = ensure_bool_series(optional_column(df, "warmup", False).fillna(False), default=False)
    std_zero = ensure_bool_series(optional_column(df, "std_zero", False).fillna(False), default=False)

    df_no_warmup = df[~warmup].copy()
    z = pd.to_numeric(df_no_warmup["z_cvd"], errors="coerce").dropna()
    if z.empty:
        print("é”™è¯¯: z_cvd åˆ—ä¸ºç©ºæˆ–æ— æ³•è§£æ")
        sys.exit(1)

    z_median = float(z.median())
    z_abs = z.abs()
    z_abs_median = float(z_abs.median())
    z_q25 = float(z.quantile(0.25))
    z_q75 = float(z.quantile(0.75))
    z_iqr = float(z_q75 - z_q25)
    z_tail2 = float((z_abs > 2).mean())
    z_tail3 = float((z_abs > 3).mean())

    z_p50 = float(z_abs.quantile(0.50))
    z_p95 = float(z_abs.quantile(0.95))
    z_p99 = float(z_abs.quantile(0.99))

    # æ ¹æ®å‚æ•°é€‰æ‹©é˜ˆå€¼
    if args.relaxed_zscore:
        tail2_threshold = 0.15  # 15%
        tail3_threshold = 0.05  # 5%
        print("ä½¿ç”¨å®½æ¾Z-scoreé˜ˆå€¼")
    else:
        tail2_threshold = 0.08  # 8%
        tail3_threshold = 0.02  # 2%

    results["z_score"] = {
        "median": z_median,
        "abs_median": z_abs_median,
        "iqr": z_iqr,
        "tail2_pct": z_tail2 * 100.0,
        "tail3_pct": z_tail3 * 100.0,
        "p50": z_p50,
        "p95": z_p95,
        "p99": z_p99,
        "abs_median_pass": z_abs_median <= 1.0,
        "iqr_pass": True,
        "tail2_pass": z_tail2 <= tail2_threshold,
        "tail3_pass": z_tail3 <= tail3_threshold,
    }
    
    std_zero_count = int(std_zero.sum())
    results["std_zero_count"] = std_zero_count
    results["std_zero_pass"] = (std_zero_count == 0)

    results["warmup_pct"] = float(warmup.mean() * 100.0)
    results["warmup_pass"] = (warmup.mean() <= 0.10)

    results["z_score_pass"] = all([
        results["z_score"]["abs_median_pass"],
        results["z_score"]["iqr_pass"],
        results["z_score"]["tail2_pass"],
        results["z_score"]["tail3_pass"],
        results["std_zero_pass"],
        results["warmup_pass"],
    ])

    # -------------------------
    # 5) ä¸€è‡´æ€§éªŒè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    # -------------------------
    results["cvd_continuity"] = validate_cvd_consistency(df)

    # -------------------------
    # 6) ç¨³å®šæ€§ï¼ˆä¿¡æ¯é¡¹ï¼‰
    # -------------------------
    rc = optional_column(df, "reconnect_count")
    if rc is not None and not rc.isna().all():
        reconnects = int(pd.to_numeric(rc, errors="coerce").max() - pd.to_numeric(rc, errors="coerce").min())
        reconnect_rate = reconnects / time_span_hours if time_span_hours > 0 else (0 if reconnects == 0 else float("inf"))
        results["reconnect_rate_per_hour"] = float(reconnect_rate)
        results["reconnect_pass"] = reconnect_rate <= 3
    else:
        results["reconnect_pass"] = True

    # -------------------------
    # å›¾è¡¨ç”Ÿæˆï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    # -------------------------
    def save_fig(path):
        plt.savefig(path, dpi=150, bbox_inches="tight")
        print(f"âœ“ ä¿å­˜: {path}")
        plt.close()

    # 1) Z-score ç›´æ–¹å›¾
    plt.figure(figsize=(10, 6))
    plt.hist(z, bins=100, edgecolor="black", alpha=0.7)
    plt.axvline(z_median, linestyle="--", label=f"Median={z_median:.3f}")
    plt.axvline(z_q25, linestyle="--", label=f"Q25={z_q25:.3f}")
    plt.axvline(z_q75, linestyle="--", label=f"Q75={z_q75:.3f}")
    plt.xlabel("z_cvd")
    plt.ylabel("Frequency")
    plt.title("CVD Z-score Distribution (non-warmup)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    hist_path = Path(args.out) / "hist_z.png"
    save_fig(hist_path)

    # 2) CVD æ—¶é—´åºåˆ—
    plot_n = min(int(args.plots_sample), len(df))
    if plot_n < len(df) and plot_n > 0:
        step = max(len(df) // plot_n, 1)
        df_plot = df.iloc[::step].head(plot_n).copy()
    else:
        df_plot = df.copy()

    plt.figure(figsize=(14, 6))
    plt.plot(range(len(df_plot)), pd.to_numeric(df_plot["cvd"], errors="coerce"), alpha=0.7, linewidth=0.6)
    plt.xlabel("Sample Index")
    plt.ylabel("CVD")
    plt.title(f"CVD Time Series (sampled {len(df_plot)} points)")
    plt.grid(True, alpha=0.3)
    cvd_ts_path = Path(args.out) / "cvd_timeseries.png"
    save_fig(cvd_ts_path)

    # 3) Z-score æ—¶é—´åºåˆ—
    mask_nw = ~warmup
    df_plot_nw = df_plot[mask_nw.reindex(df_plot.index, fill_value=False)]
    plt.figure(figsize=(14, 6))
    plt.plot(range(len(df_plot_nw)), pd.to_numeric(df_plot_nw["z_cvd"], errors="coerce"), alpha=0.7, linewidth=0.6)
    plt.axhline(2, linestyle="--", alpha=0.5, label="|Z|=2")
    plt.axhline(-2, linestyle="--", alpha=0.5)
    plt.axhline(3, linestyle="--", alpha=0.5, label="|Z|=3")
    plt.axhline(-3, linestyle="--", alpha=0.5)
    plt.xlabel("Sample Index (non-warmup)")
    plt.ylabel("Z-score")
    plt.title(f"CVD Z-score Time Series (sampled {len(df_plot_nw)} points)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    z_ts_path = Path(args.out) / "z_timeseries.png"
    save_fig(z_ts_path)

    # 4) å»¶è¿Ÿç®±çº¿å›¾
    if lat is not None and not pd.to_numeric(lat, errors="coerce").dropna().empty:
        plt.figure(figsize=(8, 6))
        plt.boxplot(pd.to_numeric(lat, errors="coerce").dropna(), vert=True)
        plt.ylabel("Latency (ms)")
        plt.title("CVD End-to-End Latency Distribution")
        plt.grid(True, alpha=0.3, axis="y")
        lat_box_path = Path(args.out) / "latency_box.png"
        save_fig(lat_box_path)
    else:
        lat_box_path = None

    # -------------------------
    # æŠ¥å‘Šç”Ÿæˆ
    # -------------------------
    def rel(path: Path):
        try:
            return Path(os.path.relpath(path, report_path.parent)).as_posix()
        except Exception:
            return str(path)

    # åŠ¨æ€ç­‰çº§
    level = "Goldï¼ˆâ‰¥120åˆ†é’Ÿï¼‰" if time_span_hours >= 2 else ("Silverï¼ˆâ‰¥60åˆ†é’Ÿï¼‰" if time_span_hours >= 1 else "Bronzeï¼ˆâ‰¥30åˆ†é’Ÿï¼‰")

    with open(report_path, "w", encoding="utf-8") as f:
        f.write("# Task 1.2.10 CVDè®¡ç®—æµ‹è¯•æŠ¥å‘Š (v2.2, ä¼˜åŒ–ç‰ˆ)\n\n")
        f.write(f"**æµ‹è¯•æ‰§è¡Œæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**æµ‹è¯•çº§åˆ«**: {level}\n\n")
        f.write(f"**æ•°æ®æº**: `{args.data}`\n\n")
        f.write("---\n\n")

        f.write("## æµ‹è¯•æ‘˜è¦\n\n")
        f.write(f"- **é‡‡é›†æ—¶é•¿**: {results['time_span_minutes']:.1f} åˆ†é’Ÿ ({results['time_span_hours']:.2f} å°æ—¶)\n")
        f.write(f"- **æ•°æ®ç‚¹æ•°**: {results['total_points']:,} ç¬”\n")
        f.write(f"- **å¹³å‡é€Ÿç‡**: {results['total_points'] / max(time_span_hours*3600,1):.2f} ç¬”/ç§’\n")
        f.write(f"- **è§£æé”™è¯¯**: {results['parse_errors']}\n")
        qrate = results.get('queue_dropped_rate', None)
        f.write(f"- **é˜Ÿåˆ—ä¸¢å¼ƒç‡**: {qrate*100:.4f}%\n\n" if qrate is not None else "- **é˜Ÿåˆ—ä¸¢å¼ƒç‡**: N/A\n\n")

        f.write("---\n\n")
        f.write("## éªŒæ”¶æ ‡å‡†å¯¹ç…§ç»“æœï¼ˆCVDï¼‰\n\n")
        f.write("### 1. æ—¶é•¿ä¸è¿ç»­æ€§\n")
        f.write(f"- [{'x' if results['duration_pass'] else ' '}] è¿è¡Œæ—¶é•¿: {results['time_span_minutes']:.1f}åˆ†é’Ÿ (â‰¥30åˆ†é’Ÿ)\n")
        f.write(f"- [{'x' if results['continuity_pass'] else ' '}] p99_interarrival: {results['gap_p99_ms']:.2f}ms (â‰¤5000ms)\n")
        f.write(f"- [{'x' if results['gaps_over_10s']==0 else ' '}] gaps_over_10s: {results['gaps_over_10s']} (==0)\n\n")

        f.write("### 2. æ•°æ®è´¨é‡\n")
        f.write(f"- [{'x' if results['parse_errors_pass'] else ' '}] parse_errors: {results['parse_errors']} (==0)\n")
        if qrate is not None:
            f.write(f"- [{'x' if results.get('queue_dropped_pass', True) else ' '}] queue_dropped_rate: {qrate*100:.4f}% (â‰¤0.5%)\n\n")
        else:
            f.write("- [x] queue_dropped_rate: N/Aï¼ˆæœªæä¾›ï¼Œå¿½ç•¥ï¼‰\n\n")

        f.write("### 3. æ€§èƒ½ï¼ˆä¿¡æ¯é¡¹ï¼‰\n")
        if results.get("latency_pass"):
            f.write(f"- [x] p95_latency: {results.get('latency_p95', float('nan')):.3f}ms ï¼ˆä¿¡æ¯é¡¹ï¼Œä¸é˜»æ–­ï¼‰\n\n")
        else:
            f.write("- [ ] latency: N/A\n\n")

        f.write("### 4. CVD Z-scoreç¨³å¥æ€§\n")
        f.write(f"- [{'x' if results['z_score']['abs_median_pass'] else ' '}] median(|z_cvd|): {results['z_score']['abs_median']:.4f} (â‰¤1.0)\n")
        f.write(f"- [{'x' if results['z_score']['iqr_pass'] else ' '}] IQR(z_cvd): {results['z_score']['iqr']:.4f} ï¼ˆå‚è€ƒå€¼ï¼Œä¸é˜»æ–­ï¼‰\n")
        f.write(f"- [{'x' if results['z_score']['tail2_pass'] else ' '}] P(|Z|>2): {results['z_score']['tail2_pct']:.2f}% (â‰¤{tail2_threshold*100:.0f}%)\n")
        f.write(f"- [{'x' if results['z_score']['tail3_pass'] else ' '}] P(|Z|>3): {results['z_score']['tail3_pct']:.2f}% (â‰¤{tail3_threshold*100:.0f}%)\n")
        f.write(f"- [{'x' if results['std_zero_pass'] else ' '}] std_zero: {results['std_zero_count']} (==0)\n")
        f.write(f"- [{'x' if results['warmup_pass'] else ' '}] warmupå æ¯”: {results['warmup_pct']:.2f}% (â‰¤10%)\n\n")

        cc = results["cvd_continuity"]
        f.write(f"### 5. ä¸€è‡´æ€§éªŒè¯ï¼ˆ{cc['method']}ï¼‰\n")
        f.write(f"- [{'x' if cc['continuity_mismatches']==0 else ' '}] é€ç¬”å®ˆæ’é”™è¯¯: {cc['continuity_mismatches']}\n")
        f.write(f"- [{'x' if cc['conservation_error']<cc['conservation_tolerance'] else ' '}] é¦–å°¾å®ˆæ’è¯¯å·®: {cc['conservation_error']:.2e} (å®¹å·®: {cc['conservation_tolerance']:.2e})\n\n")

        f.write("### 6. ç¨³å®šæ€§\n")
        if 'reconnect_rate_per_hour' in results:
            f.write(f"- [{'x' if results.get('reconnect_pass', True) else ' '}] é‡è¿é¢‘ç‡: {results.get('reconnect_rate_per_hour', 0):.2f}æ¬¡/å°æ—¶ (â‰¤3/å°æ—¶)\n\n")
        else:
            f.write("- [x] é‡è¿é¢‘ç‡: N/A\n\n")

        f.write("---\n\n")
        f.write("## å›¾è¡¨\n\n")
        f.write(f"### 1. Z-scoreåˆ†å¸ƒç›´æ–¹å›¾\n![Z-scoreç›´æ–¹å›¾]({rel(hist_path)})\n\n")
        f.write(f"### 2. CVDæ—¶é—´åºåˆ—\n![CVDæ—¶é—´åºåˆ—]({rel(cvd_ts_path)})\n\n")
        f.write(f"### 3. Z-scoreæ—¶é—´åºåˆ—\n![Z-scoreæ—¶é—´åºåˆ—]({rel(z_ts_path)})\n\n")
        if lat_box_path:
            f.write(f"### 4. å»¶è¿Ÿç®±çº¿å›¾\n![å»¶è¿Ÿç®±çº¿å›¾]({rel(lat_box_path)})\n\n")

        # æ€»ä½“ç»“è®º
        all_pass = all([
            results["duration_pass"],
            results["continuity_pass"],
            results["parse_errors_pass"],
            results.get("queue_dropped_pass", True),
            results["z_score_pass"],
            results["cvd_continuity"]["pass"],
            results.get("reconnect_pass", True),
        ])
        passed_count = sum([
            results["duration_pass"],
            results["continuity_pass"],
            results["parse_errors_pass"],
            results.get("queue_dropped_pass", True),
            True,  # latencyä¿¡æ¯é¡¹
            results["z_score_pass"],
            results["cvd_continuity"]["pass"],
            results.get("reconnect_pass", True),
        ])

        f.write("---\n\n")
        f.write("## ç»“è®º\n\n")
        f.write(f"**éªŒæ”¶æ ‡å‡†é€šè¿‡ç‡**: {passed_count}/8 ({passed_count/8*100:.1f}%)\n\n")
        if all_pass:
            f.write("**âœ… æ‰€æœ‰å…³é”®éªŒæ”¶æ ‡å‡†é€šè¿‡ï¼Œæµ‹è¯•æˆåŠŸã€‚**\n")
        else:
            f.write("**âš ï¸ éƒ¨åˆ†éªŒæ”¶æ ‡å‡†æœªé€šè¿‡**\n")

    print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # ä¿å­˜JSONç»“æœ
    results_native = convert_to_native(results)
    results_json_path = Path(args.out) / "analysis_results.json"
    with open(results_json_path, "w", encoding="utf-8") as f:
        json.dump(results_native, f, indent=2, ensure_ascii=False)
    print(f"âœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_json_path}")

    # è¿è¡ŒæŒ‡æ ‡æ¦‚è¦
    run_metrics = {
        "run_info": {
            "start_time": pd.Timestamp(ts.min(), unit="s").strftime("%Y-%m-%d %H:%M:%S"),
            "end_time": pd.Timestamp(ts.max(), unit="s").strftime("%Y-%m-%d %H:%M:%S"),
            "duration_seconds": int(time_span_seconds),
            "total_records": int(len(df)),
        },
        "performance": {
            "p50_latency_ms": float(results.get("latency_p50", float("nan"))),
            "p95_latency_ms": float(results.get("latency_p95", float("nan"))),
            "p99_latency_ms": float(results.get("latency_p99", float("nan"))),
            "queue_dropped_rate": float(results.get("queue_dropped_rate", float("nan"))),
        },
        "z_statistics": {
            "median_abs_z": float(results["z_score"]["abs_median"]),
            "iqr_z": float(results["z_score"]["iqr"]),
            "p_z_gt_2": float(results["z_score"]["tail2_pct"] / 100.0),
            "p_z_gt_3": float(results["z_score"]["tail3_pct"] / 100.0),
        },
    }
    metrics_json_path = Path(args.out) / "cvd_run_metrics.json"
    with open(metrics_json_path, "w", encoding="utf-8") as f:
        json.dump(run_metrics, f, indent=2, ensure_ascii=False)
    print(f"âœ“ CVDè¿è¡ŒæŒ‡æ ‡å·²ä¿å­˜: {metrics_json_path}")

    # é€€å‡ºç 
    exit_ok = all([
        results["duration_pass"],
        results["continuity_pass"],
        results["parse_errors_pass"],
        results.get("queue_dropped_pass", True),
        results["z_score_pass"],
        results["cvd_continuity"]["pass"],
        results.get("reconnect_pass", True),
    ])
    print("\næœ€ç»ˆç»“æœ:", "âœ… å…¨éƒ¨é€šè¿‡" if exit_ok else "âš ï¸ éƒ¨åˆ†æœªé€šè¿‡")
    sys.exit(0 if exit_ok else 1)


if __name__ == "__main__":
    main()

