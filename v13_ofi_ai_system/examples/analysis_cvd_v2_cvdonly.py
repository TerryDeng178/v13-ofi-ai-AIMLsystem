#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CVD Calculation Test Analyzer (v2.1, CVD-only branding)
- æ˜ç¡®é¢å‘ CVDï¼ˆç´¯è®¡æˆäº¤é‡å·®ï¼‰ä¿¡å·çš„è®¡ç®—ä¸ç¨³å®šæ€§æµ‹è¯•
- ç§»é™¤ä¸ OFI ç›¸å…³çš„ç›®å½•/å‘½åï¼Œé»˜è®¤è¾“å‡ºåˆ° cvd_system/*
- å…¶ä½™é€»è¾‘ä¸ v2 ä¸€è‡´ï¼ˆé˜ˆå€¼ã€æŠ½æ ·ã€å‘é‡åŒ–ä¸€è‡´æ€§æ ¡éªŒç­‰ï¼‰
"""
import argparse
import io
import json
import os
from pathlib import Path

import numpy as np
import pandas as pd

# æ— äº¤äº’ç¯å¢ƒä¸‹å®‰å…¨ä½¿ç”¨matplotlib
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

import sys

# æ›´å®‰å…¨çš„UTF-8è¾“å‡ºè®¾ç½®ï¼ˆJupyter / é‡å®šå‘åœºæ™¯ä¸æŠ¥é”™ï¼‰
try:
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8")
        sys.stderr.reconfigure(encoding="utf-8")
except Exception:
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")
    except Exception:
        pass  # ä¿åº•ï¼šä¸ä¿®æ”¹

# -------------------------
# Helpers
# -------------------------
def ensure_bool_series(s, default=False):
    """å°†ä»»æ„Seriesè½¬ä¸ºå¸ƒå°”ï¼Œç¼ºå¤±åˆ™è¿”å›å¸¸é‡å¸ƒå°”Seriesã€‚"""
    if s is None:
        return pd.Series([default], dtype=bool).iloc[:0]
    if s.dtype == bool:
        return s
    return s.fillna(False).astype(int).astype(bool)

def require_columns(df, cols):
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise KeyError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing}")

def optional_column(df, name, default=None):
    return df[name] if name in df.columns else pd.Series(default, index=df.index)

def convert_to_native(obj):
    if isinstance(obj, dict):
        return {k: convert_to_native(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_native(x) for x in obj]
    elif isinstance(obj, (np.integer, np.floating)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    else:
        return obj

# -------------------------
# Main
# -------------------------
def main():
    parser = argparse.ArgumentParser(description="Analyze CVD data collected from trade stream (v2.1)")
    parser.add_argument("--data", "--input", dest="data", required=True,
                        help="è·¯å¾„: å•ä¸ªparquetæ–‡ä»¶æˆ–ç›®å½•")
    parser.add_argument("--out", "--output-dir", dest="out", default="cvd_system/figs",
                        help="å›¾è¡¨è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: cvd_system/figsï¼‰")
    parser.add_argument("--report", default="cvd_system/docs/reports/CVD_TEST_REPORT.md",
                        help="MarkdownæŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤: cvd_system/docs/reports/CVD_TEST_REPORT.mdï¼‰")
    parser.add_argument("--merge-files", action="store_true",
                        help="å½“ --data æ˜¯ç›®å½•æ—¶ï¼Œåˆå¹¶è¯¥ç›®å½•ä¸‹æ‰€æœ‰parquetè¿›è¡Œåˆ†æï¼ˆé»˜è®¤ä»…åˆ†ææœ€æ–°æ–‡ä»¶ï¼‰")
    parser.add_argument("--seed", type=int, default=42, help="é‡‡æ ·éšæœºç§å­ï¼ˆé»˜è®¤=42ï¼Œå¯å¤ç°ï¼‰")
    parser.add_argument("--plots-sample", type=int, default=10_000, help="ç»˜å›¾é‡‡æ ·ä¸Šé™ï¼ˆé»˜è®¤=10000ï¼‰")
    args = parser.parse_args()

    rng = np.random.default_rng(args.seed)

    data_path = Path(args.data)
    if not data_path.exists():
        print(f"é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        sys.exit(2)

    # -------------------------
    # è¯»æ•°
    # -------------------------
    if data_path.is_dir():
        parquet_files = sorted(list(data_path.glob("*.parquet")), key=lambda p: p.stat().st_mtime, reverse=True)
        if not parquet_files:
            print(f"é”™è¯¯: åœ¨ {data_path} ä¸­æœªæ‰¾åˆ° parquet æ–‡ä»¶")
            sys.exit(1)

        if args.merge_files:
            dfs = []
            for p in parquet_files:
                try:
                    df_i = pd.read_parquet(p)
                    df_i["run_id"] = p.stem
                    df_i["source_file"] = str(p.name)
                    dfs.append(df_i)
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶: {p.name} - {e}")
            if not dfs:
                print("é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„æ•°æ®æ–‡ä»¶")
                sys.exit(1)
            df = pd.concat(dfs, ignore_index=True)
            print(f"âœ“ åˆå¹¶ {len(parquet_files)} ä¸ªæ–‡ä»¶, æ€»è®°å½• {len(df):,}")
        else:
            latest_file = parquet_files[0]
            print(f"ğŸ¯ é»˜è®¤åˆ†ææœ€æ–°æ–‡ä»¶: {latest_file.name}")
            df = pd.read_parquet(latest_file)
            df["run_id"] = latest_file.stem
    else:
        df = pd.read_parquet(data_path)
        df["run_id"] = data_path.stem

    print(f"\næ€»æ•°æ®ç‚¹æ•°: {len(df):,}")

    # -------------------------
    # æ’åºï¼ˆåŒé”®/é™çº§ï¼‰
    # -------------------------
    if "agg_trade_id" in df.columns and "event_time_ms" in df.columns:
        df = df.sort_values(["event_time_ms", "agg_trade_id"], kind="mergesort").reset_index(drop=True)
        print("âœ“ å·²æŒ‰ (event_time_ms, agg_trade_id) æ’åº")
    elif "agg_trade_id" in df.columns and "ts_ms" in df.columns:
        df = df.sort_values(["ts_ms", "agg_trade_id"], kind="mergesort").reset_index(drop=True)
        print("âœ“ å·²æŒ‰ (ts_ms, agg_trade_id) æ’åº")
    elif "timestamp" in df.columns:
        df = df.sort_values("timestamp", kind="mergesort").reset_index(drop=True)
        print("âš ï¸ ç¼ºå°‘ agg_trade_id æˆ– event_time_msï¼Œä½¿ç”¨ timestamp æ’åº")
    elif "ts_ms" in df.columns:
        df = df.sort_values("ts_ms", kind="mergesort").reset_index(drop=True)
        print("âš ï¸ ç¼ºå°‘æ’åºå­—æ®µï¼Œä½¿ç”¨ ts_ms æ’åº")
    else:
        print("é”™è¯¯: ç¼ºå°‘æ’åºæ‰€éœ€å­—æ®µï¼ˆevent_time_ms/agg_trade_id æˆ– timestamp æˆ– ts_msï¼‰")
        sys.exit(1)

    # -------------------------
    # ç›®å½•å‡†å¤‡
    # -------------------------
    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)
    report_path = Path(args.report)
    report_path.parent.mkdir(parents=True, exist_ok=True)

    results = {}

    # -------------------------
    # 1) æ—¶é•¿ä¸è¿ç»­æ€§
    # -------------------------
    from pandas.api.types import is_numeric_dtype

    # å¦‚æœæ²¡æœ‰timestampåˆ—ï¼Œå°è¯•ä»å„ç§æ—¶é—´æˆ³åˆ—ç”Ÿæˆ
    if "timestamp" not in df.columns:
        if "event_time_ms" in df.columns:
            df["timestamp"] = pd.to_numeric(df["event_time_ms"], errors="coerce") / 1000.0
            print("âš ï¸ ç¼ºå°‘ timestamp åˆ—ï¼Œä» event_time_ms ç”Ÿæˆ")
        elif "ts_ms" in df.columns:
            df["timestamp"] = pd.to_numeric(df["ts_ms"], errors="coerce") / 1000.0
            print("âš ï¸ ç¼ºå°‘ timestamp åˆ—ï¼Œä» ts_ms ç”Ÿæˆ")
        elif "ts" in df.columns:
            df["timestamp"] = pd.to_numeric(df["ts"], errors="coerce")
            print("âš ï¸ ç¼ºå°‘ timestamp åˆ—ï¼Œä» ts ç”Ÿæˆ")
        else:
            print("é”™è¯¯: ç¼ºå°‘ timestampã€event_time_msã€ts_ms æˆ– ts åˆ—")
            sys.exit(1)
    
    ts = pd.to_numeric(df["timestamp"], errors="coerce")
    if ts.isna().all():
        print("é”™è¯¯: timestamp åˆ—æ— æ³•è§£æä¸ºæ•°å€¼")
        sys.exit(1)
    time_span_seconds = float(ts.max() - ts.min())
    time_span_hours = time_span_seconds / 3600.0
    ts_diff_ms = ts.diff().mul(1000).dropna()

    results["total_points"] = int(len(df))
    results["time_span_hours"] = float(time_span_hours)
    results["time_span_minutes"] = float(time_span_hours * 60)

    gap_p99 = float(ts_diff_ms.quantile(0.99)) if len(ts_diff_ms) else 0.0
    gaps_over_10s = int((ts_diff_ms > 10_000).sum())
    max_gap = float(ts_diff_ms.max()) if len(ts_diff_ms) else 0.0

    results["gap_p99_ms"] = gap_p99
    results["gaps_over_10s"] = gaps_over_10s
    results["max_gap_ms"] = max_gap

    # åŸºçº¿ï¼ˆCVDåˆ†æï¼‰ï¼šâ‰¥30åˆ†é’Ÿï¼Œè¿ç»­æ€§ p99â‰¤5s ä¸”æ— 10ç§’ç©ºçª—
    results["duration_pass"] = time_span_hours >= 0.5
    results["continuity_pass"] = (gap_p99 <= 5000.0) and (gaps_over_10s == 0)

    # -------------------------
    # 2) æ•°æ®è´¨é‡ï¼ˆä¿¡æ¯é¡¹ï¼‰
    # -------------------------
    parse_errors = 0
    results["parse_errors"] = parse_errors
    results["parse_errors_pass"] = (parse_errors == 0)
    qd = optional_column(df, "queue_dropped")
    if qd is not None and not qd.isna().all():
        queue_dropped_incremental = qd.diff().clip(lower=0).fillna(0).sum()
        queue_dropped_rate = float(queue_dropped_incremental) / max(len(df), 1)
        results["queue_dropped_rate"] = float(queue_dropped_rate)
        results["queue_dropped_pass"] = queue_dropped_rate <= 0.005  # 0.5%
    else:
        results["queue_dropped_pass"] = True

    # -------------------------
    # 3) æ€§èƒ½ï¼ˆä¿¡æ¯é¡¹ï¼‰
    # -------------------------
    lat = optional_column(df, "latency_ms")
    if lat is not None and not pd.to_numeric(lat, errors="coerce").dropna().empty:
        lat = pd.to_numeric(lat, errors="coerce").dropna()
        results["latency_p50"] = float(lat.quantile(0.50))
        results["latency_p95"] = float(lat.quantile(0.95))
        results["latency_p99"] = float(lat.quantile(0.99))
        results["latency_pass"] = True  # ä¿¡æ¯é¡¹ï¼Œä¸é˜»æ–­
    else:
        results["latency_pass"] = False

    # -------------------------
    # 4) CVD Z-score ç¨³å¥æ€§ï¼ˆCVD-onlyæ ‡å‡†ï¼‰
    # -------------------------
    if "z_cvd" not in df.columns:
        print("é”™è¯¯: ç¼ºå°‘ z_cvd åˆ—")
        sys.exit(1)

    warmup = ensure_bool_series(optional_column(df, "warmup", False).fillna(False), default=False)
    std_zero = ensure_bool_series(optional_column(df, "std_zero", False).fillna(False), default=False)

    df_no_warmup = df[~warmup].copy()
    z = pd.to_numeric(df_no_warmup["z_cvd"], errors="coerce").dropna()
    if z.empty:
        print("é”™è¯¯: z_cvd åˆ—ä¸ºç©ºæˆ–æ— æ³•è§£æ")
        sys.exit(1)

    z_median = float(z.median())
    z_abs = z.abs()
    z_abs_median = float(z_abs.median())
    z_q25 = float(z.quantile(0.25))
    z_q75 = float(z.quantile(0.75))
    z_iqr = float(z_q75 - z_q25)
    z_tail2 = float((z_abs > 2).mean())
    z_tail3 = float((z_abs > 3).mean())

    z_p50 = float(z_abs.quantile(0.50))
    z_p95 = float(z_abs.quantile(0.95))
    z_p99 = float(z_abs.quantile(0.99))

    # CVDä¸“ç”¨æ ‡å‡†ï¼ˆä¿¡æ¯å­¦å¯¹ç§°ï¼Œä¸ OFI æ— å…³ï¼‰ï¼š
    # - median(|Z|) â‰¤ 1.0
    # - P(|Z|>2) â‰¤ 8%
    # - P(|Z|>3) â‰¤ 2%
    # - IQR ä»…ä½œä¸ºå‚è€ƒï¼Œä¸åšé˜»æ–­
    results["z_score"] = {
        "median": z_median,
        "abs_median": z_abs_median,
        "iqr": z_iqr,
        "tail2_pct": z_tail2 * 100.0,
        "tail3_pct": z_tail3 * 100.0,
        "p50": z_p50,
        "p95": z_p95,
        "p99": z_p99,
        "abs_median_pass": z_abs_median <= 1.0,
        "iqr_pass": True,
        "tail2_pass": z_tail2 <= 0.08,
        "tail3_pass": z_tail3 <= 0.02,
    }
    std_zero_count = int(std_zero.sum())
    results["std_zero_count"] = std_zero_count
    results["std_zero_pass"] = (std_zero_count == 0)

    results["warmup_pct"] = float(warmup.mean() * 100.0)
    results["warmup_pass"] = (warmup.mean() <= 0.10)

    results["z_score_pass"] = all([
        results["z_score"]["abs_median_pass"],
        results["z_score"]["iqr_pass"],
        results["z_score"]["tail2_pass"],
        results["z_score"]["tail3_pass"],
        results["std_zero_pass"],
        results["warmup_pass"],
    ])

    # -------------------------
    # 5) ä¸€è‡´æ€§éªŒè¯ï¼ˆå¢é‡å®ˆæ’ï¼‰
    # -------------------------
    # CVDæ•°æ®å¯èƒ½æ²¡æœ‰qtyå’Œis_buyï¼Œæ‰€ä»¥æ”¹ä¸ºå¯é€‰
    has_qty_is_buy = "qty" in df.columns and "is_buy" in df.columns
    if has_qty_is_buy:
        require_columns(df, ["cvd", "qty", "is_buy"])
    else:
        print("âš ï¸ ç¼ºå°‘ qty æˆ– is_buy åˆ—ï¼Œè·³è¿‡ä¸€è‡´æ€§éªŒè¯")
    CHECK_THRESHOLD = 10_000
    MIN_SAMPLE = 1_000
    if len(df) <= CHECK_THRESHOLD:
        df_sample = df
        check_method = "å…¨é‡"
    else:
        size = max(int(len(df) * 0.01), MIN_SAMPLE)
        idx = np.sort(rng.choice(len(df), size=size, replace=False))
        df_sample = df.iloc[idx]
        check_method = f"æŠ½æ ·({len(df_sample)})"

    s_cvd = pd.to_numeric(df_sample["cvd"], errors="coerce")
    
    if has_qty_is_buy:
        s_qty = pd.to_numeric(df_sample["qty"], errors="coerce")
        s_buy = ensure_bool_series(df_sample["is_buy"])

        delta_expected = np.where(s_buy, s_qty, -s_qty)
        cvd_prev = s_cvd.shift(1)
        continuity_err = (np.abs((s_cvd - cvd_prev) - delta_expected) > 1e-9)
        continuity_mismatches = int(continuity_err.iloc[1:].sum())  # è·³è¿‡é¦–ç¬”

        cvd_first = float(s_cvd.iloc[0])
        cvd_last = float(s_cvd.iloc[-1])
        sum_deltas = float(np.where(s_buy, s_qty, -s_qty).sum())
        conservation_error = abs(cvd_last - cvd_first - sum_deltas)
        conservation_tolerance = max(1e-6, 1e-8 * abs(cvd_last - cvd_first))

        results["cvd_continuity"] = {
            "sample_size": int(len(df_sample)),
            "continuity_mismatches": int(continuity_mismatches),
            "conservation_error": float(conservation_error),
            "conservation_tolerance": float(conservation_tolerance),
            "pass": (continuity_mismatches == 0) and (conservation_error < conservation_tolerance),
            "method": check_method,
        }
    else:
        # æ²¡æœ‰qty/is_buyæ—¶ï¼Œè·³è¿‡ä¸€è‡´æ€§éªŒè¯
        results["cvd_continuity"] = {
            "sample_size": 0,
            "continuity_mismatches": 0,
            "conservation_error": 0.0,
            "conservation_tolerance": 0.0,
            "pass": True,  # è·³è¿‡æ—¶æ ‡è®°ä¸ºé€šè¿‡
            "method": "è·³è¿‡ä¸€æ¬¡æ€§éªŒè¯ï¼ˆæ— qty/is_buyï¼‰",
        }

    # -------------------------
    # 6) ç¨³å®šæ€§ï¼ˆä¿¡æ¯é¡¹ï¼‰
    # -------------------------
    rc = optional_column(df, "reconnect_count")
    if rc is not None and not rc.isna().all():
        reconnects = int(pd.to_numeric(rc, errors="coerce").max() - pd.to_numeric(rc, errors="coerce").min())
        reconnect_rate = reconnects / time_span_hours if time_span_hours > 0 else (0 if reconnects == 0 else float("inf"))
        results["reconnect_rate_per_hour"] = float(reconnect_rate)
        results["reconnect_pass"] = reconnect_rate <= 3
    else:
        results["reconnect_pass"] = True

    # -------------------------
    # å›¾è¡¨ï¼ˆCVD-onlyå‘½åï¼‰
    # -------------------------
    def save_fig(path):
        plt.savefig(path, dpi=150, bbox_inches="tight")
        print(f"âœ“ ä¿å­˜: {path}")
        plt.close()

    # 1) Z-score ç›´æ–¹å›¾ï¼ˆéwarmupï¼‰
    plt.figure(figsize=(10, 6))
    plt.hist(z, bins=100, edgecolor="black", alpha=0.7)
    plt.axvline(z_median, linestyle="--", label=f"Median={z_median:.3f}")
    plt.axvline(z_q25, linestyle="--", label=f"Q25={z_q25:.3f}")
    plt.axvline(z_q75, linestyle="--", label=f"Q75={z_q75:.3f}")
    plt.xlabel("z_cvd")
    plt.ylabel("Frequency")
    plt.title("CVD Z-score Distribution (non-warmup)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    hist_path = Path(args.out) / "cvd_hist_z.png"
    save_fig(hist_path)

    # 2) CVD æ—¶é—´åºåˆ—ï¼ˆé‡‡æ ·ï¼‰
    plot_n = min(int(args.plots_sample), len(df))
    if plot_n < len(df) and plot_n > 0:
        step = max(len(df) // plot_n, 1)
        df_plot = df.iloc[::step].head(plot_n).copy()
    else:
        df_plot = df.copy()

    plt.figure(figsize=(14, 6))
    plt.plot(range(len(df_plot)), pd.to_numeric(df_plot["cvd"], errors="coerce"), alpha=0.7, linewidth=0.6)
    plt.xlabel("Sample Index")
    plt.ylabel("CVD")
    plt.title(f"CVD Time Series (sampled {len(df_plot)} points)")
    plt.grid(True, alpha=0.3)
    cvd_ts_path = Path(args.out) / "cvd_timeseries.png"
    save_fig(cvd_ts_path)

    # 3) Z-score æ—¶é—´åºåˆ—ï¼ˆéwarmupåŒé‡‡æ ·ï¼‰
    mask_nw = ~warmup
    df_plot_nw = df_plot[mask_nw.reindex(df_plot.index, fill_value=False)]
    plt.figure(figsize=(14, 6))
    plt.plot(range(len(df_plot_nw)), pd.to_numeric(df_plot_nw["z_cvd"], errors="coerce"), alpha=0.7, linewidth=0.6)
    plt.axhline(2, linestyle="--", alpha=0.5, label="|Z|=2")
    plt.axhline(-2, linestyle="--", alpha=0.5)
    plt.axhline(3, linestyle="--", alpha=0.5, label="|Z|=3")
    plt.axhline(-3, linestyle="--", alpha=0.5)
    plt.xlabel("Sample Index (non-warmup)")
    plt.ylabel("Z-score")
    plt.title(f"CVD Z-score Time Series (sampled {len(df_plot_nw)} points)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    z_ts_path = Path(args.out) / "cvd_z_timeseries.png"
    save_fig(z_ts_path)

    # 4) å»¶è¿Ÿç®±çº¿å›¾ï¼ˆå¦‚æœ‰ï¼‰
    if lat is not None and not pd.to_numeric(lat, errors="coerce").dropna().empty:
        plt.figure(figsize=(8, 6))
        plt.boxplot(pd.to_numeric(lat, errors="coerce").dropna(), vert=True)
        plt.ylabel("Latency (ms)")
        plt.title("CVD End-to-End Latency Distribution")
        plt.grid(True, alpha=0.3, axis="y")
        lat_box_path = Path(args.out) / "cvd_latency_box.png"
        save_fig(lat_box_path)
    else:
        lat_box_path = None

    # 5) Interarrival åˆ†å¸ƒ
    interarrival_ms = ts_diff_ms.copy()
    interarrival_p95 = float(interarrival_ms.quantile(0.95)) if len(interarrival_ms) else float("nan")
    interarrival_p99 = float(interarrival_ms.quantile(0.99)) if len(interarrival_ms) else float("nan")
    interarrival_filtered = interarrival_ms[interarrival_ms < (interarrival_p99 * 1.5)] if np.isfinite(interarrival_p99) else interarrival_ms

    plt.figure(figsize=(12, 6))
    plt.hist(interarrival_filtered, bins=100, edgecolor="black", alpha=0.7)
    if np.isfinite(interarrival_p95):
        plt.axvline(interarrival_p95, linestyle="--", linewidth=2, label=f"P95={interarrival_p95:.1f}ms")
    if np.isfinite(interarrival_p99):
        plt.axvline(interarrival_p99, linestyle="--", linewidth=2, label=f"P99={interarrival_p99:.1f}ms")
    plt.xlabel("Interarrival Time (ms)")
    plt.ylabel("Frequency")
    ttl_cut = f"P99Ã—1.5={interarrival_p99*1.5:.1f}ms" if np.isfinite(interarrival_p99) else "N/A"
    plt.title(f"Message Interarrival Distribution (filtered at {ttl_cut})")
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    interarrival_path = Path(args.out) / "cvd_interarrival_hist.png"
    save_fig(interarrival_path)

    # 6) Event ID å·®å€¼åˆ†å¸ƒï¼ˆä¼˜å…ˆ agg_trade_idï¼‰
    event_id_path = None
    if "agg_trade_id" in df.columns:
        diffs = pd.to_numeric(df["agg_trade_id"], errors="coerce").diff().dropna()
        dup_count = int((diffs == 0).sum())
        backward_count = int((diffs < 0).sum())
        large_gap_count = int((diffs > 10_000).sum())
        dup_rate = dup_count / max(len(df), 1)
        backward_rate = backward_count / max(len(df), 1)

        # ç»˜å›¾ï¼ˆè¿‡æ»¤è´Ÿå·®ä¸æå¤§å€¼ï¼‰
        id_diff_p99 = float(diffs.quantile(0.99)) if len(diffs) else float("nan")
        diffs_f = diffs[(diffs >= 0) & (diffs < (id_diff_p99 * 1.5))] if np.isfinite(id_diff_p99) else diffs

        plt.figure(figsize=(12, 6))
        plt.hist(diffs_f, bins=100, edgecolor="black", alpha=0.7)
        plt.axvline(0, linestyle="--", linewidth=2, label=f"Duplicate ({dup_count})")
        plt.xlabel("aggTradeId Difference")
        plt.ylabel("Frequency")
        plt.title(
            "aggTradeId Difference Distribution\n"
            f"Duplicates: {dup_count} ({dup_rate*100:.3f}%), "
            f"Backward: {backward_count} ({backward_rate*100:.3f}%), "
            f"Large gaps >10k: {large_gap_count}"
        )
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        event_id_path = Path(args.out) / "cvd_event_id_diff.png"
        save_fig(event_id_path)

    elif "event_time_ms" in df.columns:
        diffs = pd.to_numeric(df["event_time_ms"], errors="coerce").diff().dropna()
        zero = int((diffs == 0).sum())
        back = int((diffs < 0).sum())
        big = int((diffs > 10_000).sum())

        id_diff_p99 = float(diffs.quantile(0.99)) if len(diffs) else float("nan")
        diffs_f = diffs[(diffs >= 0) & (diffs < (id_diff_p99 * 1.5))] if np.isfinite(id_diff_p99) else diffs

        plt.figure(figsize=(12, 6))
        plt.hist(diffs_f, bins=100, edgecolor="black", alpha=0.7)
        plt.axvline(0, linestyle="--", linewidth=2, label=f"Zero/Duplicate ({zero})")
        plt.xlabel("Event ID Difference (ms)")
        plt.ylabel("Frequency")
        plt.title(f"Event ID Difference Distribution (event_time_ms)\n(Zero: {zero}, Backward: {back}, Large gaps >10s: {big})")
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        event_id_path = Path(args.out) / "cvd_event_id_diff.png"
        save_fig(event_id_path)

    # -------------------------
    # æŠ¥å‘Šï¼ˆCVD-onlyï¼‰
    # -------------------------
    def rel(path: Path):
        try:
            return Path(os.path.relpath(path, report_path.parent)).as_posix()
        except Exception:
            return str(path)

    # åŠ¨æ€ç­‰çº§
    level = "Goldï¼ˆâ‰¥120åˆ†é’Ÿï¼‰" if time_span_hours >= 2 else ("Silverï¼ˆâ‰¥60åˆ†é’Ÿï¼‰" if time_span_hours >= 1 else "Bronzeï¼ˆâ‰¥30åˆ†é’Ÿï¼‰")

    with open(report_path, "w", encoding="utf-8") as f:
        f.write("# Task 1.2.10 CVDè®¡ç®—æµ‹è¯•æŠ¥å‘Š (v2.1, CVD-only)\n\n")
        f.write(f"**æµ‹è¯•æ‰§è¡Œæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**æµ‹è¯•çº§åˆ«**: {level}\n\n")
        f.write(f"**æ•°æ®æº**: `{args.data}`\n\n")
        f.write("---\n\n")

        f.write("## æµ‹è¯•æ‘˜è¦\n\n")
        f.write(f"- **é‡‡é›†æ—¶é•¿**: {results['time_span_minutes']:.1f} åˆ†é’Ÿ ({results['time_span_hours']:.2f} å°æ—¶)\n")
        f.write(f"- **æ•°æ®ç‚¹æ•°**: {results['total_points']:,} ç¬”\n")
        f.write(f"- **å¹³å‡é€Ÿç‡**: {results['total_points'] / max(time_span_hours*3600,1):.2f} ç¬”/ç§’\n")
        f.write(f"- **è§£æé”™è¯¯**: {results['parse_errors']}\n")
        qrate = results.get('queue_dropped_rate', None)
        f.write(f"- **é˜Ÿåˆ—ä¸¢å¼ƒç‡**: {qrate*100:.4f}%\n\n" if qrate is not None else "- **é˜Ÿåˆ—ä¸¢å¼ƒç‡**: N/A\n\n")

        f.write("---\n\n")
        f.write("## éªŒæ”¶æ ‡å‡†å¯¹ç…§ç»“æœï¼ˆCVDï¼‰\n\n")
        f.write("### 1. æ—¶é•¿ä¸è¿ç»­æ€§\n")
        f.write(f"- [{'x' if results['duration_pass'] else ' '}] è¿è¡Œæ—¶é•¿: {results['time_span_minutes']:.1f}åˆ†é’Ÿ (â‰¥30åˆ†é’Ÿ)\n")
        f.write(f"- [{'x' if results['continuity_pass'] else ' '}] p99_interarrival: {results['gap_p99_ms']:.2f}ms (â‰¤5000ms)\n")
        f.write(f"- [{'x' if results['gaps_over_10s']==0 else ' '}] gaps_over_10s: {results['gaps_over_10s']} (==0)\n\n")

        f.write("### 2. æ•°æ®è´¨é‡\n")
        f.write(f"- [{'x' if results['parse_errors_pass'] else ' '}] parse_errors: {results['parse_errors']} (==0)\n")
        if qrate is not None:
            f.write(f"- [{'x' if results.get('queue_dropped_pass', True) else ' '}] queue_dropped_rate: {qrate*100:.4f}% (â‰¤0.5%)\n\n")
        else:
            f.write("- [x] queue_dropped_rate: N/Aï¼ˆæœªæä¾›ï¼Œå¿½ç•¥ï¼‰\n\n")

        f.write("### 3. æ€§èƒ½ï¼ˆä¿¡æ¯é¡¹ï¼‰\n")
        if results.get("latency_pass"):
            f.write(f"- [x] p95_latency: {results.get('latency_p95', float('nan')):.3f}ms ï¼ˆä¿¡æ¯é¡¹ï¼Œä¸é˜»æ–­ï¼‰\n\n")
        else:
            f.write("- [ ] latency: N/A\n\n")

        f.write("### 4. CVD Z-scoreç¨³å¥æ€§\n")
        f.write(f"- [{'x' if results['z_score']['abs_median_pass'] else ' '}] median(|z_cvd|): {results['z_score']['abs_median']:.4f} (â‰¤1.0)\n")
        f.write(f"- [{'x' if results['z_score']['iqr_pass'] else ' '}] IQR(z_cvd): {results['z_score']['iqr']:.4f} ï¼ˆå‚è€ƒå€¼ï¼Œä¸é˜»æ–­ï¼‰\n")
        f.write(f"- [{'x' if results['z_score']['tail2_pass'] else ' '}] P(|Z|>2): {results['z_score']['tail2_pct']:.2f}% (â‰¤8%)\n")
        f.write(f"- [{'x' if results['z_score']['tail3_pass'] else ' '}] P(|Z|>3): {results['z_score']['tail3_pct']:.2f}% (â‰¤2%)\n")
        f.write(f"- [{'x' if results['std_zero_pass'] else ' '}] std_zero: {results['std_zero_count']} (==0)\n")
        f.write(f"- [{'x' if results['warmup_pass'] else ' '}] warmupå æ¯”: {results['warmup_pct']:.2f}% (â‰¤10%)\n\n")

        cc = results["cvd_continuity"]
        f.write(f"### 5. ä¸€è‡´æ€§éªŒè¯ï¼ˆ{cc['method']}ï¼‰\n")
        f.write(f"- [{'x' if cc['continuity_mismatches']==0 else ' '}] é€ç¬”å®ˆæ’é”™è¯¯: {cc['continuity_mismatches']}\n")
        f.write(f"- [{'x' if cc['conservation_error']<cc['conservation_tolerance'] else ' '}] é¦–å°¾å®ˆæ’è¯¯å·®: {cc['conservation_error']:.2e} (å®¹å·®: {cc['conservation_tolerance']:.2e})\n\n")

        f.write("### 6. ç¨³å®šæ€§\n")
        if 'reconnect_rate_per_hour' in results:
            f.write(f"- [{'x' if results.get('reconnect_pass', True) else ' '}] é‡è¿é¢‘ç‡: {results.get('reconnect_rate_per_hour', 0):.2f}æ¬¡/å°æ—¶ (â‰¤3/å°æ—¶)\n\n")
        else:
            f.write("- [x] é‡è¿é¢‘ç‡: N/A\n\n")

        f.write("---\n\n")
        f.write("## å›¾è¡¨\n\n")
        f.write(f"### 1. Z-scoreåˆ†å¸ƒç›´æ–¹å›¾\n![Z-scoreç›´æ–¹å›¾]({rel(hist_path)})\n\n")
        f.write(f"### 2. CVDæ—¶é—´åºåˆ—\n![CVDæ—¶é—´åºåˆ—]({rel(cvd_ts_path)})\n\n")
        f.write(f"### 3. Z-scoreæ—¶é—´åºåˆ—\n![Z-scoreæ—¶é—´åºåˆ—]({rel(z_ts_path)})\n\n")
        if lat_box_path:
            f.write(f"### 4. å»¶è¿Ÿç®±çº¿å›¾\n![å»¶è¿Ÿç®±çº¿å›¾]({rel(lat_box_path)})\n\n")
        f.write(f"### 5. æ¶ˆæ¯åˆ°è¾¾é—´éš”åˆ†å¸ƒ\n![Interarrivalåˆ†å¸ƒ]({rel(interarrival_path)})\n\n")
        if event_id_path:
            f.write(f"### 6. Event IDå·®å€¼åˆ†å¸ƒ\n![Event IDå·®å€¼]({rel(event_id_path)})\n\n")

        # æ€»ä½“ç»“è®º
        all_pass = all([
            results["duration_pass"],
            results["continuity_pass"],
            results["parse_errors_pass"],
            results.get("queue_dropped_pass", True),
            results["z_score_pass"],
            results["cvd_continuity"]["pass"],
            results.get("reconnect_pass", True),
        ])
        passed_count = sum([
            results["duration_pass"],
            results["continuity_pass"],
            results["parse_errors_pass"],
            results.get("queue_dropped_pass", True),
            True,  # latencyä¿¡æ¯é¡¹
            results["z_score_pass"],
            results["cvd_continuity"]["pass"],
            results.get("reconnect_pass", True),
        ])

        f.write("---\n\n")
        f.write("## ç»“è®º\n\n")
        f.write(f"**éªŒæ”¶æ ‡å‡†é€šè¿‡ç‡**: {passed_count}/8 ({passed_count/8*100:.1f}%)\n\n")
        if all_pass:
            f.write("**âœ… æ‰€æœ‰å…³é”®éªŒæ”¶æ ‡å‡†é€šè¿‡ï¼Œæµ‹è¯•æˆåŠŸã€‚**\n")
        else:
            f.write("**âš ï¸ éƒ¨åˆ†éªŒæ”¶æ ‡å‡†æœªé€šè¿‡**\n")

    print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # ä¿å­˜JSONç»“æœ
    results_native = convert_to_native(results)
    results_json_path = Path(args.out) / "cvd_analysis_results.json"
    with open(results_json_path, "w", encoding="utf-8") as f:
        json.dump(results_native, f, indent=2, ensure_ascii=False)
    print(f"âœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_json_path}")

    # è¿è¡ŒæŒ‡æ ‡æ¦‚è¦
    run_metrics = {
        "run_info": {
            "start_time": pd.Timestamp(ts.min(), unit="s").strftime("%Y-%m-%d %H:%M:%S"),
            "end_time": pd.Timestamp(ts.max(), unit="s").strftime("%Y-%m-%d %H:%M:%S"),
            "duration_seconds": int(time_span_seconds),
            "total_records": int(len(df)),
        },
        "performance": {
            "p50_latency_ms": float(results.get("latency_p50", float("nan"))),
            "p95_latency_ms": float(results.get("latency_p95", float("nan"))),
            "p99_latency_ms": float(results.get("latency_p99", float("nan"))),
            "queue_dropped_rate": float(results.get("queue_dropped_rate", float("nan"))),
        },
        "z_statistics": {
            "median_abs_z": float(results["z_score"]["abs_median"]),
            "iqr_z": float(results["z_score"]["iqr"]),
            "p_z_gt_2": float(results["z_score"]["tail2_pct"] / 100.0),
            "p_z_gt_3": float(results["z_score"]["tail3_pct"] / 100.0),
        },
    }
    metrics_json_path = Path(args.out) / "cvd_run_metrics.json"
    with open(metrics_json_path, "w", encoding="utf-8") as f:
        json.dump(run_metrics, f, indent=2, ensure_ascii=False)
    print(f"âœ“ CVDè¿è¡ŒæŒ‡æ ‡å·²ä¿å­˜: {metrics_json_path}")

    # é€€å‡ºç ï¼ˆä¸ä»¥ latency é˜»æ–­ï¼‰
    exit_ok = all([
        results["duration_pass"],
        results["continuity_pass"],
        results["parse_errors_pass"],
        results.get("queue_dropped_pass", True),
        results["z_score_pass"],
        results["cvd_continuity"]["pass"],
        results.get("reconnect_pass", True),
    ])
    print("\næœ€ç»ˆç»“æœ:", "âœ… å…¨éƒ¨é€šè¿‡" if exit_ok else "âš ï¸ éƒ¨åˆ†æœªé€šè¿‡")
    sys.exit(0 if exit_ok else 1)


if __name__ == "__main__":
    main()
