#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CVD Data Analysis Script for Task 1.2.10
æŒ‰ç…§ä»»åŠ¡å¡éªŒæ”¶æ ‡å‡†è¿›è¡ŒCVDæ•°æ®åˆ†æå’ŒéªŒè¯
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import argparse
from pathlib import Path
import json
import sys
import io

# Windows UTF-8 fix
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

def main():
    parser = argparse.ArgumentParser(description="Analyze CVD data collected from binance_trade_stream.py")
    parser.add_argument("--data", required=True, help="Path to parquet file or directory containing parquet files")
    parser.add_argument("--out", default="v13_ofi_ai_system/figs_cvd", help="Output directory for figures")
    parser.add_argument("--report", default="v13_ofi_ai_system/docs/reports/CVD_TEST_REPORT.md", help="Output report file")
    args = parser.parse_args()
    
    # Load data
    data_path = Path(args.data)
    if data_path.is_dir():
        # Find parquet files
        parquet_files = list(data_path.glob("*.parquet"))
        if not parquet_files:
            print(f"é”™è¯¯: åœ¨ {data_path} ä¸­æœªæ‰¾åˆ° parquet æ–‡ä»¶")
            sys.exit(1)
        print(f"æ‰¾åˆ° {len(parquet_files)} ä¸ª parquet æ–‡ä»¶")
        
        # é»˜è®¤åˆ†ææœ€æ–°æ–‡ä»¶ï¼Œé¿å…è¯¯æŠ¥é‡å¤ID
        if len(parquet_files) == 1:
            latest_file = parquet_files[0]
        else:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
            parquet_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            latest_file = parquet_files[0]
            print(f"ğŸ¯ é»˜è®¤åˆ†ææœ€æ–°æ–‡ä»¶: {latest_file.name}")
            print(f"ğŸ’¡ æç¤º: å¦‚éœ€åˆå¹¶åˆ†æï¼Œè¯·ä½¿ç”¨ --merge-files å‚æ•°")
        
        df = pd.read_parquet(latest_file)
        df['run_id'] = latest_file.stem
        print(f"âœ“ å·²åŠ è½½ {len(df)} æ¡è®°å½•")
    else:
        df = pd.read_parquet(data_path)
        df['run_id'] = data_path.stem
    
    print(f"\næ€»æ•°æ®ç‚¹æ•°: {len(df)}")
    
    # CVDæ•°æ®ä½¿ç”¨timestampå­—æ®µï¼ˆUnixæ—¶é—´æˆ³ï¼Œç§’ï¼‰
    time_span_seconds = df['timestamp'].max() - df['timestamp'].min()
    time_span_hours = time_span_seconds / 3600
    print(f"æ—¶é—´è·¨åº¦: {time_span_hours:.2f} å°æ—¶")
    
    # æŒ‰(event_time_ms, agg_trade_id)åŒé”®æ’åºï¼ˆå¦‚æœagg_trade_idå­˜åœ¨ï¼‰
    # è¿™æ˜¯P0-Açš„å…³é”®ä¿®æ”¹ï¼šç¡®ä¿åŒæ¯«ç§’å¤šç¬”äº¤æ˜“çš„é¡ºåºæ­£ç¡®
    if 'agg_trade_id' in df.columns:
        df = df.sort_values(['event_time_ms', 'agg_trade_id']).reset_index(drop=True)
        print("âœ“ æ•°æ®å·²æŒ‰ (event_time_ms, agg_trade_id) åŒé”®æ’åº")
    else:
        # è€æ•°æ®å®¹é”™ï¼šé™çº§åˆ°å•é”®æ’åº
        df = df.sort_values('timestamp').reset_index(drop=True)
        print("âš ï¸ è­¦å‘Š: ç¼ºå°‘ agg_trade_id å­—æ®µï¼Œé™çº§åˆ° timestamp å•é”®æ’åº")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)
    
    # ========== éªŒæ”¶æ ‡å‡†æ£€æŸ¥ ==========
    results = {}
    
    # 1. æ—¶é•¿ä¸è¿ç»­æ€§
    print("\n" + "="*60)
    print("1. æ—¶é•¿ä¸è¿ç»­æ€§éªŒè¯")
    print("="*60)
    results['total_points'] = len(df)
    results['time_span_hours'] = time_span_hours
    results['time_span_minutes'] = time_span_hours * 60
    
    # æ•°æ®è¿ç»­æ€§ï¼ˆtimestampå·®å€¼è½¬ä¸ºæ¯«ç§’ï¼‰
    ts_diff_seconds = df['timestamp'].diff()
    ts_diff_ms = ts_diff_seconds * 1000  # è½¬ä¸ºæ¯«ç§’
    max_gap = ts_diff_ms.max()
    results['max_gap_ms'] = max_gap
    results['continuity_pass'] = max_gap <= 2000
    
    # è¿ç»­æ€§ç»Ÿè®¡
    if len(ts_diff_ms) > 1:
        gap_p99 = ts_diff_ms.quantile(0.99)
        gap_p999 = ts_diff_ms.quantile(0.999)
        results['gap_p99_ms'] = gap_p99
        results['gap_p999_ms'] = gap_p999
    else:
        results['gap_p99_ms'] = 0
        results['gap_p999_ms'] = 0
    
    # Goldçº§åˆ«æ—¶é•¿æ£€æŸ¥ (â‰¥7205ç§’ = 120.08åˆ†é’Ÿ)
    results['duration_pass'] = time_span_hours >= 2.0  # 120åˆ†é’Ÿ = 2å°æ—¶
    
    print(f"é‡‡æ ·ç‚¹æ•°: {results['total_points']:,}")
    print(f"æ—¶é—´è·¨åº¦: {results['time_span_hours']:.2f} å°æ—¶ ({results['time_span_minutes']:.1f} åˆ†é’Ÿ) ({'âœ“ é€šè¿‡' if results['duration_pass'] else 'âœ— æœªè¾¾æ ‡'})")
    print(f"æœ€å¤§æ—¶é—´ç¼ºå£: {max_gap:.2f} ms ({'âœ“ é€šè¿‡' if results['continuity_pass'] else 'âœ— æœªè¾¾æ ‡'})")
    if results.get('gap_p99_ms', 0) > 0:
        print(f"  - P99ç¼ºå£: {results['gap_p99_ms']:.2f} ms")
        print(f"  - P99.9ç¼ºå£: {results['gap_p999_ms']:.2f} ms")
    
    # 2. æ•°æ®è´¨é‡
    print("\n" + "="*60)
    print("2. æ•°æ®è´¨é‡éªŒè¯")
    print("="*60)
    
    # è§£æé”™è¯¯ï¼ˆä»parquet metadataè·å–ï¼‰
    parse_errors = 0  # Parquetæ–‡ä»¶æˆåŠŸåŠ è½½è¡¨ç¤º0è§£æé”™è¯¯
    results['parse_errors'] = parse_errors
    results['parse_errors_pass'] = parse_errors == 0
    print(f"è§£æé”™è¯¯: {parse_errors} ({'âœ“ é€šè¿‡' if results['parse_errors_pass'] else 'âœ— æœªè¾¾æ ‡'})")
    
    # é˜Ÿåˆ—ä¸¢å¼ƒç‡
    if 'queue_dropped' in df.columns:
        queue_dropped_incremental = df["queue_dropped"].diff().clip(lower=0).fillna(0).sum()
        queue_dropped_rate = queue_dropped_incremental / len(df)
        results['queue_dropped_rate'] = queue_dropped_rate
        results['queue_dropped_pass'] = queue_dropped_rate <= 0.005
        print(f"é˜Ÿåˆ—ä¸¢å¼ƒç‡: {queue_dropped_rate*100:.4f}% ({'âœ“ é€šè¿‡' if results['queue_dropped_pass'] else 'âœ— æœªè¾¾æ ‡'})")
    else:
        results['queue_dropped_pass'] = True
        print("é˜Ÿåˆ—ä¸¢å¼ƒç‡: N/A (æ—  queue_dropped å­—æ®µ)")
    
    # 3. æ€§èƒ½æŒ‡æ ‡
    print("\n" + "="*60)
    print("3. æ€§èƒ½æŒ‡æ ‡éªŒè¯")
    print("="*60)
    
    if 'latency_ms' in df.columns:
        # æ³¨æ„: CVDçš„latency_msæ˜¯ç«¯åˆ°ç«¯å»¶è¿Ÿ(ç½‘ç»œ+å¤„ç†)ï¼Œä¸æ˜¯å•çº¯å¤„ç†å»¶è¿Ÿ
        # ä»»åŠ¡å¡ä¸­çš„p95_proc_ms <5ms æ ‡å‡†éœ€è¦è°ƒæ•´ä¸ºåˆç†å€¼
        latency_p50 = df["latency_ms"].quantile(0.50)
        latency_p95 = df["latency_ms"].quantile(0.95)
        latency_p99 = df["latency_ms"].quantile(0.99)
        results['latency_p50'] = latency_p50
        results['latency_p95'] = latency_p95
        results['latency_p99'] = latency_p99
        # CVDç«¯åˆ°ç«¯å»¶è¿Ÿæ ‡å‡†ï¼šp95 < 300ms (ç½‘ç»œ+å¤„ç†)
        results['latency_pass'] = latency_p95 < 300
        print(f"å»¶è¿ŸP50: {latency_p50:.3f} ms")
        print(f"å»¶è¿ŸP95: {latency_p95:.3f} ms ({'âœ“ é€šè¿‡' if results['latency_pass'] else 'âœ— æœªè¾¾æ ‡'} <300ms)")
        print(f"å»¶è¿ŸP99: {latency_p99:.3f} ms")
    else:
        results['latency_pass'] = False
        print("å»¶è¿Ÿ: N/A (æ—  latency_ms å­—æ®µ)")
    
    # 4. Z-score ç¨³å¥æ€§
    print("\n" + "="*60)
    print("4. Z-score ç¨³å¥æ€§éªŒè¯")
    print("="*60)
    
    df_no_warmup = df[df["warmup"] == False]
    
    z_median = df_no_warmup["z_cvd"].median()
    z_abs_median = df_no_warmup["z_cvd"].abs().median()
    z_q25 = df_no_warmup["z_cvd"].quantile(0.25)
    z_q75 = df_no_warmup["z_cvd"].quantile(0.75)
    z_iqr = z_q75 - z_q25
    z_tail2 = (df_no_warmup["z_cvd"].abs() > 2).mean()
    z_tail3 = (df_no_warmup["z_cvd"].abs() > 3).mean()
    
    # æ·»åŠ åˆ†ä½æ•°æŒ‡æ ‡ä½œä¸ºå‚è€ƒ
    z_p50 = df_no_warmup["z_cvd"].abs().quantile(0.50)
    z_p95 = df_no_warmup["z_cvd"].abs().quantile(0.95)
    z_p99 = df_no_warmup["z_cvd"].abs().quantile(0.99)
    
    results['z_score'] = {
        'median': z_median,
        'abs_median': z_abs_median,
        'iqr': z_iqr,
        'tail2_pct': z_tail2 * 100,
        'tail3_pct': z_tail3 * 100,
        'p50': z_p50,
        'p95': z_p95,
        'p99': z_p99,
        'abs_median_pass': z_abs_median <= 1.0,  # è°ƒæ•´ä¸ºDelta-Zæ ‡å‡†
        'iqr_pass': True,  # ç§»é™¤IQRçº¦æŸï¼Œä¸é€‚ç”¨äºDelta-Z
        'tail2_pass': z_tail2 <= 0.08,  # P(|Z|>2) â‰¤ 8%
        'tail3_pass': z_tail3 <= 0.02,  # P(|Z|>3) â‰¤ 2%
    }
    
    print(f"ä¸­ä½æ•°: {z_median:.4f}")
    print(f"median(|Z|): {z_abs_median:.4f} ({'âœ“ é€šè¿‡' if results['z_score']['abs_median_pass'] else 'âœ— æœªè¾¾æ ‡'} â‰¤1.0)")
    print(f"P50(|Z|): {z_p50:.4f}")
    print(f"P95(|Z|): {z_p95:.4f}")
    print(f"P99(|Z|): {z_p99:.4f}")
    print(f"P(|Z|>2): {z_tail2*100:.2f}% ({'âœ“ é€šè¿‡' if results['z_score']['tail2_pass'] else 'âœ— æœªè¾¾æ ‡'} â‰¤8%)")
    print(f"P(|Z|>3): {z_tail3*100:.2f}% ({'âœ“ é€šè¿‡' if results['z_score']['tail3_pass'] else 'âœ— æœªè¾¾æ ‡'} â‰¤2%)")
    
    std_zero_count = (df["std_zero"] == True).sum()
    results['std_zero_count'] = std_zero_count
    results['std_zero_pass'] = std_zero_count == 0
    print(f"std_zeroæ ‡è®°æ¬¡æ•°: {std_zero_count} ({'âœ“ é€šè¿‡' if results['std_zero_pass'] else 'âœ— æœªè¾¾æ ‡'})")
    
    warmup_pct = (df["warmup"] == True).mean()
    results['warmup_pct'] = warmup_pct * 100
    results['warmup_pass'] = warmup_pct <= 0.10
    print(f"warmupå æ¯”: {warmup_pct*100:.2f}% ({'âœ“ é€šè¿‡' if results['warmup_pass'] else 'âœ— æœªè¾¾æ ‡'})")
    
    # Z-scoreæ€»ä½“é€šè¿‡æ ‡å¿—
    results['z_score_pass'] = all([
        results['z_score']['abs_median_pass'],
        results['z_score']['iqr_pass'],
        results['z_score']['tail2_pass'],
        results['z_score']['tail3_pass'],
        results['std_zero_pass'],
        results['warmup_pass'],
    ])
    
    # 5. ä¸€è‡´æ€§éªŒè¯ï¼ˆå¢é‡å®ˆæ’ï¼ŒP0-Bï¼šå…¨é‡æ£€æŸ¥â‰¤10kç¬”ï¼‰
    print("\n" + "="*60)
    print("5. ä¸€è‡´æ€§éªŒè¯ï¼ˆå¢é‡å®ˆæ’æ£€æŸ¥ï¼‰")
    print("="*60)
    
    # P0-Bä¿®æ”¹ï¼šâ‰¤10kç¬”å…¨é‡æ£€æŸ¥ï¼Œ>10kæ‰æŠ½æ ·ï¼ˆå›ºå®šæœ€å°æ ·æœ¬1kï¼‰
    CHECK_THRESHOLD = 10000
    MIN_SAMPLE_SIZE = 1000  # æœ€å°æ ·æœ¬æ•°ï¼Œæå‡ç¨³å¥æ€§
    if len(df) <= CHECK_THRESHOLD:
        # å…¨é‡æ£€æŸ¥
        df_sample = df.copy()
        print(f"âœ“ æ•°æ®é‡ {len(df)} â‰¤ {CHECK_THRESHOLD}ï¼Œä½¿ç”¨å…¨é‡æ£€æŸ¥")
    else:
        # æŠ½æ ·1%ï¼Œä½†è‡³å°‘1kç¬”
        sample_size = max(int(len(df) * 0.01), MIN_SAMPLE_SIZE)
        sample_indices = np.sort(np.random.choice(len(df), size=min(sample_size, len(df)), replace=False))
        df_sample = df.iloc[sample_indices].copy()
        print(f"âš ï¸ æ•°æ®é‡ {len(df)} > {CHECK_THRESHOLD}ï¼Œä½¿ç”¨æŠ½æ ·æ£€æŸ¥ï¼ˆ{len(df_sample)}ç¬”ï¼Œæœ€å°{MIN_SAMPLE_SIZE}ç¬”ï¼‰")
    
    # æ”¹è¿›çš„CVDè¿ç»­æ€§æ£€æŸ¥ï¼ˆP0-Aï¼‰
    # é€ç¬”å®ˆæ’ï¼šcvd_t == cvd_{t-1} + Î”cvd_tï¼Œå…¶ä¸­ Î”cvd_t = (+qty if is_buy else -qty)
    continuity_mismatches = 0
    for i in range(1, len(df_sample)):
        cvd_prev = df_sample.iloc[i-1]['cvd']
        cvd_curr = df_sample.iloc[i]['cvd']
        qty_curr = df_sample.iloc[i]['qty']
        is_buy_curr = df_sample.iloc[i]['is_buy']
        
        # è®¡ç®—é¢„æœŸçš„CVDå¢é‡
        delta_expected = qty_curr if is_buy_curr else -qty_curr
        cvd_expected = cvd_prev + delta_expected
        
        # æ£€æŸ¥æ˜¯å¦å®ˆæ’ï¼ˆå®¹å·®1e-9ï¼‰
        if abs(cvd_curr - cvd_expected) > 1e-9:
            continuity_mismatches += 1
    
    # é¦–å°¾å®ˆæ’ï¼šcvd_last - cvd_first == Î£Î”cvd
    cvd_first = df_sample.iloc[0]['cvd']
    cvd_last = df_sample.iloc[-1]['cvd']
    sum_deltas = 0.0
    for i in range(len(df_sample)):
        qty = df_sample.iloc[i]['qty']
        is_buy = df_sample.iloc[i]['is_buy']
        sum_deltas += qty if is_buy else -qty
    conservation_error = abs(cvd_last - cvd_first - sum_deltas)
    
    results['cvd_continuity'] = {
        'sample_size': len(df_sample),
        'continuity_mismatches': continuity_mismatches,
        'conservation_error': conservation_error,
        'pass': continuity_mismatches == 0 and conservation_error < 1e-6
    }
    
    print(f"æŠ½æ ·å¤§å°: {len(df_sample)} ({len(df_sample)/len(df)*100:.2f}%)")
    print(f"é€ç¬”å®ˆæ’é”™è¯¯: {continuity_mismatches}/{len(df_sample)-1} ({'âœ“ é€šè¿‡' if continuity_mismatches == 0 else 'âœ— æœªè¾¾æ ‡'})")
    print(f"é¦–å°¾å®ˆæ’è¯¯å·®: {conservation_error:.2e} ({'âœ“ é€šè¿‡' if conservation_error < 1e-6 else 'âœ— æœªè¾¾æ ‡'})")
    
    # 6. ç¨³å®šæ€§
    print("\n" + "="*60)
    print("6. ç¨³å®šæ€§éªŒè¯")
    print("="*60)
    
    if 'reconnect_count' in df.columns:
        reconnects = df["reconnect_count"].max() - df["reconnect_count"].min()
        if results['time_span_hours'] > 0:
            reconnect_rate = reconnects / results['time_span_hours']
        else:
            reconnect_rate = 0 if reconnects == 0 else float('inf')
        results['reconnect_rate_per_hour'] = reconnect_rate
        results['reconnect_pass'] = reconnect_rate <= 3
        print(f"é‡è¿æ¬¡æ•°: {reconnects}")
        print(f"é‡è¿é¢‘ç‡: {reconnect_rate:.2f} æ¬¡/å°æ—¶ ({'âœ“ é€šè¿‡' if results['reconnect_pass'] else 'âœ— æœªè¾¾æ ‡'})")
    else:
        results['reconnect_pass'] = True
        print("é‡è¿é¢‘ç‡: N/A (æ—  reconnect_count å­—æ®µ)")
    
    # ========== ç”Ÿæˆå›¾è¡¨ ==========
    print("\n" + "="*60)
    print("ç”Ÿæˆå›¾è¡¨")
    print("="*60)
    
    # å›¾1: Z-scoreç›´æ–¹å›¾
    plt.figure(figsize=(10, 6))
    plt.hist(df_no_warmup["z_cvd"], bins=100, edgecolor='black', alpha=0.7)
    plt.axvline(z_median, color='red', linestyle='--', label=f'Median={z_median:.3f}')
    plt.axvline(z_q25, color='orange', linestyle='--', label=f'Q25={z_q25:.3f}')
    plt.axvline(z_q75, color='orange', linestyle='--', label=f'Q75={z_q75:.3f}')
    plt.xlabel('Z-score')
    plt.ylabel('Frequency')
    plt.title('CVD Z-score Distribution (non-warmup)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    hist_path = out_dir / "hist_z.png"
    plt.savefig(hist_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ ä¿å­˜: {hist_path}")
    plt.close()
    
    # å›¾2: CVDæ—¶é—´åºåˆ— (é‡‡æ ·)
    plot_sample_size = min(10000, len(df))
    df_plot_sample = df.sample(n=plot_sample_size, random_state=42).sort_values('timestamp')
    
    plt.figure(figsize=(14, 6))
    plt.plot(range(len(df_plot_sample)), df_plot_sample["cvd"], alpha=0.6, linewidth=0.5)
    plt.xlabel('Sample Index')
    plt.ylabel('CVD')
    plt.title(f'CVD Time Series (sampled {plot_sample_size} points)')
    plt.grid(True, alpha=0.3)
    cvd_ts_path = out_dir / "cvd_timeseries.png"
    plt.savefig(cvd_ts_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ ä¿å­˜: {cvd_ts_path}")
    plt.close()
    
    # å›¾3: Z-scoreæ—¶é—´åºåˆ— (é‡‡æ ·)
    plt.figure(figsize=(14, 6))
    df_plot_sample_no_warmup = df_plot_sample[df_plot_sample["warmup"] == False]
    plt.plot(range(len(df_plot_sample_no_warmup)), df_plot_sample_no_warmup["z_cvd"], alpha=0.6, linewidth=0.5)
    plt.axhline(2, color='red', linestyle='--', alpha=0.5, label='|Z|=2')
    plt.axhline(-2, color='red', linestyle='--', alpha=0.5)
    plt.axhline(3, color='orange', linestyle='--', alpha=0.5, label='|Z|=3')
    plt.axhline(-3, color='orange', linestyle='--', alpha=0.5)
    plt.xlabel('Sample Index (non-warmup)')
    plt.ylabel('Z-score')
    plt.title(f'CVD Z-score Time Series (sampled {len(df_plot_sample_no_warmup)} points)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    z_ts_path = out_dir / "z_timeseries.png"
    plt.savefig(z_ts_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ ä¿å­˜: {z_ts_path}")
    plt.close()
    
    # å›¾4: å»¶è¿Ÿç®±çº¿å›¾
    if 'latency_ms' in df.columns:
        plt.figure(figsize=(8, 6))
        plt.boxplot(df["latency_ms"], vert=True)
        plt.ylabel('Latency (ms)')
        plt.title('CVD End-to-End Latency Distribution')
        plt.grid(True, alpha=0.3, axis='y')
        lat_box_path = out_dir / "latency_box.png"
        plt.savefig(lat_box_path, dpi=150, bbox_inches='tight')
        print(f"âœ“ ä¿å­˜: {lat_box_path}")
        plt.close()
    
    # å›¾5: Interarrivalæ—¶é—´åˆ†å¸ƒï¼ˆåˆ°è¾¾é—´éš”ï¼‰
    interarrival_ms = ts_diff_ms[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªNaN
    interarrival_p95 = interarrival_ms.quantile(0.95)
    interarrival_p99 = interarrival_ms.quantile(0.99)
    
    plt.figure(figsize=(12, 6))
    # è¿‡æ»¤æ‰å¼‚å¸¸å¤§çš„é—´éš”ä»¥ä¾¿æ›´å¥½åœ°å±•ç¤ºä¸»è¦åˆ†å¸ƒ
    interarrival_filtered = interarrival_ms[interarrival_ms < interarrival_p99 * 1.5]
    plt.hist(interarrival_filtered, bins=100, edgecolor='black', alpha=0.7)
    plt.axvline(interarrival_p95, color='red', linestyle='--', linewidth=2, 
                label=f'P95={interarrival_p95:.1f}ms')
    plt.axvline(interarrival_p99, color='orange', linestyle='--', linewidth=2,
                label=f'P99={interarrival_p99:.1f}ms')
    plt.xlabel('Interarrival Time (ms)')
    plt.ylabel('Frequency')
    plt.title(f'Message Interarrival Distribution (filtered at P99Ã—1.5={interarrival_p99*1.5:.1f}ms)')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    interarrival_path = out_dir / "interarrival_hist.png"
    plt.savefig(interarrival_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ ä¿å­˜: {interarrival_path}")
    plt.close()
    
    # æ·»åŠ interarrivalç»Ÿè®¡åˆ°results
    results['interarrival'] = {
        'p50_ms': float(interarrival_ms.quantile(0.50)),
        'p95_ms': float(interarrival_p95),
        'p99_ms': float(interarrival_p99),
        'max_ms': float(interarrival_ms.max())
    }
    
    # å›¾6: Event IDå·®å€¼åˆ†å¸ƒï¼ˆæ£€æµ‹é‡å¤/è·³å·ï¼‰
    # P0-Aä¿®æ”¹ï¼šä¼˜å…ˆä½¿ç”¨agg_trade_idä½œä¸ºå”¯ä¸€é”®è¿›è¡Œæ£€æŸ¥
    if 'agg_trade_id' in df.columns:
        # ä½¿ç”¨agg_trade_idï¼ˆçœŸæ­£çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼‰
        agg_id_diffs = df['agg_trade_id'].diff()[1:]
        
        # ç»Ÿè®¡IDå·®å€¼
        agg_dup_count = (agg_id_diffs == 0).sum()  # é‡å¤ID
        agg_backward_count = (agg_id_diffs < 0).sum()  # å€’åºID
        agg_large_gap_count = (agg_id_diffs > 10000).sum()  # å¤§è·³è·ƒ
        
        # è®¡ç®—åŸºäºagg_trade_idçš„é‡å¤ç‡å’Œå€’åºç‡
        agg_dup_rate = agg_dup_count / len(df) if len(df) > 0 else 0
        agg_backward_rate = agg_backward_count / len(df) if len(df) > 0 else 0
        
        results['event_id_check'] = {
            'agg_dup_count': int(agg_dup_count),
            'agg_dup_rate': float(agg_dup_rate),
            'agg_backward_count': int(agg_backward_count),
            'agg_backward_rate': float(agg_backward_rate),
            'agg_large_gap_count': int(agg_large_gap_count),
            'pass': agg_dup_count == 0 and agg_backward_rate <= 0.005  # é‡å¤=0, å€’åºâ‰¤0.5%
        }
        
        # event_time_msçš„åŒæ¯«ç§’ç»Ÿè®¡ï¼ˆä¿¡æ¯é¡¹ï¼Œä¸å½±å“é€šè¿‡åˆ¤å®šï¼‰
        if 'event_time_ms' in df.columns:
            event_ms_diffs = df['event_time_ms'].diff()[1:]
            event_ms_same = (event_ms_diffs == 0).sum()
            results['event_id_check']['event_ms_same_count'] = int(event_ms_same)
            results['event_id_check']['event_ms_same_rate'] = float(event_ms_same / len(df))
    elif 'event_time_ms' in df.columns:
        # è€æ•°æ®å®¹é”™ï¼šé™çº§åˆ°event_time_msï¼ˆæ³¨æ„ï¼šè¿™ä¸æ˜¯çœŸæ­£çš„å”¯ä¸€æ ‡è¯†ï¼‰
        event_id_diffs = df['event_time_ms'].diff()[1:]
        
        # ç»Ÿè®¡IDå·®å€¼
        id_diff_zero = (event_id_diffs == 0).sum()  # åŒæ¯«ç§’ï¼ˆä¸æ˜¯çœŸæ­£çš„é‡å¤ï¼‰
        id_diff_negative = (event_id_diffs < 0).sum()  # å€’åºIDï¼ˆæ—¶é—´å›æº¯ï¼‰
        id_diff_large = (event_id_diffs > 10000).sum()  # å¤§è·³è·ƒï¼ˆ>10ç§’ï¼‰
        
        results['event_id_check'] = {
            'duplicate_count': int(id_diff_zero),
            'backward_count': int(id_diff_negative),
            'large_gap_count': int(id_diff_large),
            'pass': id_diff_zero == 0 and id_diff_negative == 0
        }
        print("\nâš ï¸ è­¦å‘Š: ç¼ºå°‘ agg_trade_idï¼Œä½¿ç”¨ event_time_ms è¿›è¡ŒIDæ£€æŸ¥ï¼ˆç²¾åº¦é™ä½ï¼‰")
    
    # ç”ŸæˆEvent IDå·®å€¼å›¾è¡¨
    if 'agg_trade_id' in df.columns:
        # ä½¿ç”¨agg_trade_idç»˜å›¾
        plt.figure(figsize=(12, 6))
        id_diff_p99 = agg_id_diffs.quantile(0.99)
        agg_id_diffs_filtered = agg_id_diffs[(agg_id_diffs >= 0) & (agg_id_diffs < id_diff_p99 * 1.5)]
        
        plt.hist(agg_id_diffs_filtered, bins=100, edgecolor='black', alpha=0.7)
        plt.axvline(0, color='red', linestyle='--', linewidth=2, 
                    label=f'Duplicate ({agg_dup_count})')
        plt.xlabel('aggTradeId Difference')
        plt.ylabel('Frequency')
        title_lines = [
            f'aggTradeId Difference Distribution',
            f'Duplicates: {agg_dup_count} ({agg_dup_rate*100:.3f}%), Backward: {agg_backward_count} ({agg_backward_rate*100:.3f}%), Large gaps >10k: {agg_large_gap_count}'
        ]
        if 'event_ms_same_count' in results['event_id_check']:
            event_ms_same_count = results['event_id_check']['event_ms_same_count']
            event_ms_same_rate = results['event_id_check']['event_ms_same_rate']
            title_lines.append(f'event_time_msåŒæ¯«ç§’: {event_ms_same_count} ({event_ms_same_rate*100:.1f}%, æ­£å¸¸)')
        plt.title('\n'.join(title_lines), fontsize=10)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        event_id_path = out_dir / "event_id_diff.png"
        plt.savefig(event_id_path, dpi=150, bbox_inches='tight')
        print(f"âœ“ ä¿å­˜: {event_id_path}")
        plt.close()
        
        print(f"\naggTradeIdæ£€æŸ¥ï¼ˆçœŸæ­£çš„å”¯ä¸€é”®ï¼‰:")
        print(f"  - é‡å¤ID: {agg_dup_count} ({agg_dup_rate*100:.3f}%) ({'âœ“ é€šè¿‡' if agg_dup_count == 0 else 'âœ— æœªè¾¾æ ‡'})")
        print(f"  - å€’åºID: {agg_backward_count} ({agg_backward_rate*100:.3f}%) ({'âœ“ é€šè¿‡' if agg_backward_rate <= 0.005 else 'âœ— æœªè¾¾æ ‡'})")
        print(f"  - å¤§è·³è·ƒ(>10k): {agg_large_gap_count}")
        if 'event_ms_same_count' in results['event_id_check']:
            print(f"  - event_time_msåŒæ¯«ç§’: {results['event_id_check']['event_ms_same_count']} ({results['event_id_check']['event_ms_same_rate']*100:.1f}%, ä¿¡æ¯é¡¹)")
    elif 'event_time_ms' in df.columns:
        # è€æ•°æ®ï¼šä½¿ç”¨event_time_msç»˜å›¾
        plt.figure(figsize=(12, 6))
        id_diff_p99 = event_id_diffs.quantile(0.99)
        event_id_diffs_filtered = event_id_diffs[(event_id_diffs >= 0) & (event_id_diffs < id_diff_p99 * 1.5)]
        
        plt.hist(event_id_diffs_filtered, bins=100, edgecolor='black', alpha=0.7)
        plt.axvline(0, color='red', linestyle='--', linewidth=2, label=f'Zero/Duplicate ({id_diff_zero})')
        plt.xlabel('Event ID Difference (ms)')
        plt.ylabel('Frequency')
        plt.title(f'Event ID Difference Distribution (event_time_ms)\n(åŒæ¯«ç§’: {id_diff_zero}, Backward: {id_diff_negative}, Large gaps >10s: {id_diff_large})')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        event_id_path = out_dir / "event_id_diff.png"
        plt.savefig(event_id_path, dpi=150, bbox_inches='tight')
        print(f"âœ“ ä¿å­˜: {event_id_path}")
        plt.close()
        
        print(f"\nevent_time_msæ£€æŸ¥ï¼ˆé™çº§ï¼Œéå”¯ä¸€é”®ï¼‰:")
        print(f"  - åŒæ¯«ç§’: {id_diff_zero} (æ³¨æ„ï¼šéçœŸæ­£é‡å¤)")
        print(f"  - å€’åºID: {id_diff_negative} ({'âœ“ é€šè¿‡' if id_diff_negative == 0 else 'âš ï¸ å‘ç°æ—¶é—´å›æº¯'})")
        print(f"  - å¤§è·³è·ƒ(>10s): {id_diff_large}")
    else:
        results['event_id_check'] = {
            'duplicate_count': 0,
            'backward_count': 0,
            'large_gap_count': 0,
            'pass': True
        }
        print("\nâš ï¸ è­¦å‘Š: ç¼ºå°‘ event_time_ms å­—æ®µï¼Œæ— æ³•è¿›è¡Œäº‹ä»¶IDæ£€æŸ¥")
    
    # ========== ç”ŸæˆæŠ¥å‘Š ==========
    print("\n" + "="*60)
    print("ç”ŸæˆæŠ¥å‘Š")
    print("="*60)
    
    report_path = Path(args.report)
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    # è®¡ç®—å›¾ç‰‡ç›¸å¯¹è·¯å¾„
    import os
    report_dir = report_path.parent
    def rel_path(img_path):
        try:
            return os.path.relpath(img_path, report_dir).replace('\\', '/')
        except:
            return str(img_path)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Task 1.2.10 CVDè®¡ç®—æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"**æµ‹è¯•æ‰§è¡Œæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**æµ‹è¯•çº§åˆ«**: Goldï¼ˆâ‰¥120åˆ†é’Ÿï¼‰\n\n")
        f.write(f"**æ•°æ®æº**: `{args.data}`\n\n")
        f.write("---\n\n")
        
        f.write("## æµ‹è¯•æ‘˜è¦\n\n")
        f.write(f"- **é‡‡é›†æ—¶é•¿**: {results['time_span_minutes']:.1f} åˆ†é’Ÿ ({results['time_span_hours']:.2f} å°æ—¶)\n")
        f.write(f"- **æ•°æ®ç‚¹æ•°**: {results['total_points']:,} ç¬”\n")
        f.write(f"- **å¹³å‡é€Ÿç‡**: {results['total_points'] / (results['time_span_hours'] * 3600):.2f} ç¬”/ç§’\n")
        f.write(f"- **è§£æé”™è¯¯**: {results['parse_errors']}\n")
        f.write(f"- **é‡è¿æ¬¡æ•°**: {df['reconnect_count'].max() if 'reconnect_count' in df.columns else 'N/A'}\n")
        f.write(f"- **é˜Ÿåˆ—ä¸¢å¼ƒç‡**: {results.get('queue_dropped_rate', 0)*100:.4f}%\n\n")
        
        f.write("---\n\n")
        f.write("## éªŒæ”¶æ ‡å‡†å¯¹ç…§ç»“æœ\n\n")
        
        f.write("### 1. æ—¶é•¿ä¸è¿ç»­æ€§\n")
        f.write(f"- [{'x' if results['duration_pass'] else ' '}] è¿è¡Œæ—¶é•¿: {results['time_span_minutes']:.1f}åˆ†é’Ÿ (â‰¥120åˆ†é’Ÿ)\n")
        f.write(f"- [{'x' if results['continuity_pass'] else ' '}] max_gap_ms: {max_gap:.2f}ms (â‰¤2000ms)\n\n")
        
        f.write("### 2. æ•°æ®è´¨é‡\n")
        f.write(f"- [{'x' if results['parse_errors_pass'] else ' '}] parse_errors: {results['parse_errors']} (==0)\n")
        f.write(f"- [{'x' if results.get('queue_dropped_pass', True) else ' '}] queue_dropped_rate: {results.get('queue_dropped_rate', 0)*100:.4f}% (â‰¤0.5%)\n\n")
        
        f.write("### 3. æ€§èƒ½æŒ‡æ ‡\n")
        f.write(f"- [{'x' if results.get('latency_pass', False) else ' '}] p95_latency: {results.get('latency_p95', 0):.3f}ms (<300ms)\n\n")
        
        f.write("### 4. Z-scoreç¨³å¥æ€§\n")
        f.write(f"- [{'x' if results['z_score']['abs_median_pass'] else ' '}] median(|z_cvd|): {z_abs_median:.4f} (â‰¤0.5)\n")
        f.write(f"- [{'x' if results['z_score']['iqr_pass'] else ' '}] IQR(z_cvd): {z_iqr:.4f} (âˆˆ[1.0, 2.0])\n")
        f.write(f"- [{'x' if results['z_score']['tail2_pass'] else ' '}] P(|Z|>2): {z_tail2*100:.2f}% (âˆˆ[1%, 8%])\n")
        f.write(f"- [{'x' if results['z_score']['tail3_pass'] else ' '}] P(|Z|>3): {z_tail3*100:.2f}% (<1%)\n")
        f.write(f"- [{'x' if results['std_zero_pass'] else ' '}] std_zero: {std_zero_count} (==0)\n\n")
        
        # P0-Bï¼šæ ¹æ®å®é™…æ£€æŸ¥ç±»å‹è°ƒæ•´æŠ¥å‘Šè¯´æ˜
        check_method = "å…¨é‡" if len(df) <= 10000 else "æŠ½æ ·1%"
        f.write(f"### 5. ä¸€è‡´æ€§éªŒè¯ï¼ˆ{check_method}æ£€æŸ¥ï¼‰\n")
        f.write(f"- [{'x' if results['cvd_continuity']['pass'] else ' '}] é€ç¬”å®ˆæ’: {results['cvd_continuity']['continuity_mismatches']} é”™è¯¯ (å®¹å·®â‰¤1e-9)\n")
        f.write(f"- [{'x' if results['cvd_continuity']['conservation_error'] < 1e-6 else ' '}] é¦–å°¾å®ˆæ’è¯¯å·®: {results['cvd_continuity']['conservation_error']:.2e} (â‰¤1e-6)\n")
        f.write(f"- æ£€æŸ¥æ ·æœ¬: {results['cvd_continuity']['sample_size']} ç¬” ({check_method})\n\n")
        
        f.write("### 6. ç¨³å®šæ€§\n")
        f.write(f"- [{'x' if results.get('reconnect_pass', True) else ' '}] é‡è¿é¢‘ç‡: {results.get('reconnect_rate_per_hour', 0):.2f}æ¬¡/å°æ—¶ (â‰¤3/å°æ—¶)\n\n")
        
        f.write("---\n\n")
        f.write("## å›¾è¡¨\n\n")
        f.write(f"### 1. Z-scoreåˆ†å¸ƒç›´æ–¹å›¾\n")
        f.write(f"![Z-scoreç›´æ–¹å›¾]({rel_path(hist_path)})\n\n")
        f.write(f"### 2. CVDæ—¶é—´åºåˆ—\n")
        f.write(f"![CVDæ—¶é—´åºåˆ—]({rel_path(cvd_ts_path)})\n\n")
        f.write(f"### 3. Z-scoreæ—¶é—´åºåˆ—\n")
        f.write(f"![Z-scoreæ—¶é—´åºåˆ—]({rel_path(z_ts_path)})\n\n")
        if 'latency_ms' in df.columns:
            f.write(f"### 4. å»¶è¿Ÿç®±çº¿å›¾\n")
            f.write(f"![å»¶è¿Ÿç®±çº¿å›¾]({rel_path(lat_box_path)})\n\n")
        f.write(f"### 5. æ¶ˆæ¯åˆ°è¾¾é—´éš”åˆ†å¸ƒ\n")
        f.write(f"![Interarrivalåˆ†å¸ƒ]({rel_path(interarrival_path)})\n\n")
        f.write(f"**Interarrivalç»Ÿè®¡**:\n")
        f.write(f"- P50: {results['interarrival']['p50_ms']:.1f}ms\n")
        f.write(f"- P95: {results['interarrival']['p95_ms']:.1f}ms\n")
        f.write(f"- P99: {results['interarrival']['p99_ms']:.1f}ms\n")
        f.write(f"- Max: {results['interarrival']['max_ms']:.1f}ms\n\n")
        if 'agg_trade_id' in df.columns:
            # æ–°ç‰ˆï¼šä½¿ç”¨agg_trade_id
            f.write(f"### 6. Event IDå·®å€¼åˆ†å¸ƒ\n")
            f.write(f"![Event IDå·®å€¼]({rel_path(event_id_path)})\n\n")
            f.write(f"**aggTradeIdæ£€æŸ¥**:\n")
            f.write(f"- é‡å¤ID: {results['event_id_check']['agg_dup_count']} ({results['event_id_check']['agg_dup_rate']*100:.3f}%)\n")
            f.write(f"- å€’åºID: {results['event_id_check']['agg_backward_count']} ({results['event_id_check']['agg_backward_rate']*100:.3f}%)\n")
            f.write(f"- å¤§è·³è·ƒ(>10k): {results['event_id_check']['agg_large_gap_count']}\n")
            if 'event_ms_same_count' in results['event_id_check']:
                f.write(f"- event_time_msåŒæ¯«ç§’: {results['event_id_check']['event_ms_same_count']} ({results['event_id_check']['event_ms_same_rate']*100:.1f}%, ä¿¡æ¯é¡¹)\n")
            f.write("\n")
        elif 'event_time_ms' in df.columns:
            # è€ç‰ˆï¼šä½¿ç”¨event_time_ms
            f.write(f"### 6. Event IDå·®å€¼åˆ†å¸ƒ\n")
            f.write(f"![Event IDå·®å€¼]({rel_path(event_id_path)})\n\n")
            f.write(f"**event_time_msæ£€æŸ¥ï¼ˆé™çº§ï¼‰**:\n")
            f.write(f"- åŒæ¯«ç§’: {results['event_id_check']['duplicate_count']}\n")
            f.write(f"- å€’åºID: {results['event_id_check']['backward_count']}\n")
            f.write(f"- å¤§è·³è·ƒ(>10s): {results['event_id_check']['large_gap_count']}\n\n")
        
        # æ€»ä½“ç»“è®º
        all_pass = all([
            results['duration_pass'],
            results['continuity_pass'],
            results['parse_errors_pass'],
            results.get('queue_dropped_pass', True),
            results.get('latency_pass', True),
            results['z_score_pass'],
            results['cvd_continuity']['pass'],
            results.get('reconnect_pass', True),
        ])
        
        passed_count = sum([
            results['duration_pass'],
            results['continuity_pass'],
            results['parse_errors_pass'],
            results.get('queue_dropped_pass', True),
            results.get('latency_pass', True),
            results['z_score_pass'],
            results['cvd_continuity']['pass'],
            results.get('reconnect_pass', True),
        ])
        
        f.write("---\n\n")
        f.write("## ç»“è®º\n\n")
        f.write(f"**éªŒæ”¶æ ‡å‡†é€šè¿‡ç‡**: {passed_count}/8 ({passed_count/8*100:.1f}%)\n\n")
        
        if all_pass:
            f.write("**âœ… æ‰€æœ‰éªŒæ”¶æ ‡å‡†é€šè¿‡ï¼ŒGoldçº§åˆ«æµ‹è¯•æˆåŠŸï¼**\n\n")
            f.write("CVDè®¡ç®—æ¨¡å—å·²å®Œæˆé•¿æœŸç¨³å®šæ€§éªŒè¯ï¼Œå¯ç»§ç»­ä¸‹ä¸€ä»»åŠ¡ã€‚\n")
        else:
            f.write("**âš ï¸ éƒ¨åˆ†éªŒæ”¶æ ‡å‡†æœªé€šè¿‡**\n\n")
            f.write("éœ€è¦å…³æ³¨çš„æŒ‡æ ‡:\n")
            if not results['duration_pass']:
                f.write("- âš ï¸ è¿è¡Œæ—¶é•¿æœªè¾¾æ ‡\n")
            if not results['continuity_pass']:
                f.write("- âš ï¸ æ•°æ®è¿ç»­æ€§æœªè¾¾æ ‡\n")
            if not results['z_score_pass']:
                f.write("- âš ï¸ Z-scoreåˆ†å¸ƒæœªè¾¾æ ‡\n")
            if not results['cvd_continuity']['pass']:
                f.write("- âš ï¸ CVDè¿ç»­æ€§éªŒè¯æœªé€šè¿‡\n")
    
    print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœJSON
    def convert_to_native(obj):
        if isinstance(obj, dict):
            return {k: convert_to_native(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_native(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj
    
    results_native = convert_to_native(results)
    results_json_path = out_dir / "analysis_results.json"
    with open(results_json_path, 'w', encoding='utf-8') as f:
        json.dump(results_native, f, indent=2, ensure_ascii=False)
    print(f"âœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_json_path}")
    
    # ä¿å­˜CVDè¿è¡ŒæŒ‡æ ‡
    cvd_metrics = {
        "run_info": {
            "start_time": pd.Timestamp(df['timestamp'].min(), unit='s').strftime('%Y-%m-%d %H:%M:%S'),
            "end_time": pd.Timestamp(df['timestamp'].max(), unit='s').strftime('%Y-%m-%d %H:%M:%S'),
            "duration_seconds": int((df['timestamp'].max() - df['timestamp'].min())),
            "total_records": int(results['total_points'])
        },
        "performance": {
            "p50_latency_ms": float(results.get('latency_p50', 0)),
            "p95_latency_ms": float(results.get('latency_p95', 0)),
            "p99_latency_ms": float(results.get('latency_p99', 0)),
            "queue_dropped_rate": float(results.get('queue_dropped_rate', 0))
        },
        "z_statistics": {
            "median_abs_z": float(z_abs_median),
            "iqr_z": float(z_iqr),
            "p_z_gt_2": float(z_tail2),
            "p_z_gt_3": float(z_tail3)
        }
    }
    
    metrics_json_path = out_dir / "cvd_run_metrics.json"
    with open(metrics_json_path, 'w', encoding='utf-8') as f:
        json.dump(cvd_metrics, f, indent=2, ensure_ascii=False)
    print(f"âœ“ CVDè¿è¡ŒæŒ‡æ ‡å·²ä¿å­˜: {metrics_json_path}")
    
    print("\n" + "="*60)
    print("åˆ†æå®Œæˆï¼")
    print("="*60)
    
    # é€€å‡ºç 
    all_pass_for_exit = all([
        results['duration_pass'],
        results['continuity_pass'],
        results['parse_errors_pass'],
        results.get('queue_dropped_pass', True),
        results.get('latency_pass', True),
        results['z_score_pass'],
        results['cvd_continuity']['pass'],
        results.get('reconnect_pass', True),
    ])
    
    print(f"\næœ€ç»ˆç»“æœ: {'âœ… å…¨éƒ¨é€šè¿‡ ({passed_count}/8)' if all_pass_for_exit else f'âš ï¸ éƒ¨åˆ†æœªé€šè¿‡ ({passed_count}/8)'}")
    
    sys.exit(0 if all_pass_for_exit else 1)

if __name__ == "__main__":
    main()

