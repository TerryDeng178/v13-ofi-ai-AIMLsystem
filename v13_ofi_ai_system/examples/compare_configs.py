#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
compare_configs.py - CVDå‚æ•°ç½‘æ ¼æœç´¢ä¸å¯¹æ¯”åˆ†æ

åŠŸèƒ½ï¼š
- è‡ªåŠ¨ç”Ÿæˆå‚æ•°ç»„åˆçš„.envæ–‡ä»¶
- ä¸²è¡Œè¿è¡ŒCVDæµ‹è¯•
- æ±‡æ€»ç»“æœå¹¶ç”Ÿæˆæ’åè¡¨
- è¾“å‡ºå¯¹æ¯”å›¾è¡¨å’ŒæŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python compare_configs.py --symbol ETHUSDT --duration 1200 --grid "MAD_MULTIPLIER=[1.46,1.47,1.48];SCALE_FAST_WEIGHT=[0.30,0.32,0.35]"
"""

import argparse
import asyncio
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
import pandas as pd
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

def parse_grid_spec(grid_spec: str) -> Dict[str, List[float]]:
    """è§£æç½‘æ ¼è§„æ ¼å­—ç¬¦ä¸²"""
    params = {}
    for param_spec in grid_spec.split(';'):
        if '=' in param_spec:
            param_name, values_str = param_spec.split('=', 1)
            # è§£æ [1.46,1.47,1.48] æ ¼å¼
            values_str = values_str.strip('[]')
            values = [float(x.strip()) for x in values_str.split(',')]
            params[param_name] = values
    return params

def generate_config_combinations(params: Dict[str, List[float]]) -> List[Dict[str, float]]:
    """ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ"""
    import itertools
    
    param_names = list(params.keys())
    param_values = list(params.values())
    
    combinations = []
    for combo in itertools.product(*param_values):
        combination = dict(zip(param_names, combo))
        combinations.append(combination)
    
    return combinations

def create_env_file(config: Dict[str, float], output_dir: Path, index: int) -> Path:
    """åˆ›å»º.envé…ç½®æ–‡ä»¶"""
    env_file = output_dir / f"config_{index:02d}.env"
    
    # åŸºçº¿é…ç½®
    base_config = {
        "CVD_Z_MODE": "delta",
        "HALF_LIFE_TRADES": 300,
        "WINSOR_LIMIT": 8.0,
        "STALE_THRESHOLD_MS": 5000,
        "FREEZE_MIN": 80,
        "SCALE_MODE": "hybrid",
        "EWMA_FAST_HL": 80,
        "MAD_WINDOW_TRADES": 300,
        "MAD_SCALE_FACTOR": 1.4826,
        "SCALE_SLOW_WEIGHT": 0.65  # é»˜è®¤å€¼
    }
    
    # æ›´æ–°é…ç½®
    base_config.update(config)
    
    # è®¡ç®—SCALE_SLOW_WEIGHT
    if "SCALE_FAST_WEIGHT" in config:
        base_config["SCALE_SLOW_WEIGHT"] = 1.0 - config["SCALE_FAST_WEIGHT"]
    
    # å†™å…¥.envæ–‡ä»¶
    with open(env_file, 'w') as f:
        for key, value in base_config.items():
            f.write(f"{key}={value}\n")
    
    return env_file

def run_cvd_test(env_file: Path, symbol: str, duration: int, output_dir: Path) -> Dict[str, Any]:
    """è¿è¡ŒCVDæµ‹è¯•"""
    test_output_dir = output_dir / f"test_{env_file.stem}"
    test_output_dir.mkdir(exist_ok=True)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    with open(env_file, 'r') as f:
        for line in f:
            if '=' in line and not line.strip().startswith('#'):
                key, value = line.strip().split('=', 1)
                env[key] = value
    
    # è¿è¡ŒCVDæµ‹è¯•
    cmd = [
        sys.executable, "examples/run_realtime_cvd.py",
        "--symbol", symbol,
        "--duration", str(duration),
        "--output-dir", str(test_output_dir)
    ]
    
    start_time = time.time()
    try:
        result = subprocess.run(cmd, env=env, capture_output=True, text=True, timeout=duration+60)
        end_time = time.time()
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
        report_files = list(test_output_dir.glob("report_*.json"))
        if report_files:
            with open(report_files[0], 'r') as f:
                report = json.load(f)
        else:
            report = {"error": "No report file found"}
        
        return {
            "config_file": str(env_file),
            "test_dir": str(test_output_dir),
            "duration_actual": end_time - start_time,
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "report": report
        }
        
    except subprocess.TimeoutExpired:
        return {
            "config_file": str(env_file),
            "test_dir": str(test_output_dir),
            "error": "Test timeout",
            "return_code": -1
        }
    except Exception as e:
        return {
            "config_file": str(env_file),
            "test_dir": str(test_output_dir),
            "error": str(e),
            "return_code": -1
        }

def analyze_results(results: List[Dict[str, Any]]) -> pd.DataFrame:
    """åˆ†ææµ‹è¯•ç»“æœå¹¶ç”Ÿæˆæ’åè¡¨"""
    data = []
    
    for i, result in enumerate(results):
        if "error" in result:
            continue
            
        report = result.get("report", {})
        data_stats = report.get("data_stats", {})
        validation = report.get("validation", {})
        
        # æå–å…³é”®æŒ‡æ ‡
        row = {
            "config_id": i,
            "config_file": Path(result["config_file"]).stem,
            "total_records": data_stats.get("total_records", 0),
            "avg_rate_per_sec": data_stats.get("avg_rate_per_sec", 0),
            "duration_actual": result.get("duration_actual", 0),
            "return_code": result.get("return_code", -1),
            "validation_passed": sum(validation.values()) if validation else 0,
            "validation_total": len(validation) if validation else 0
        }
        
        # æ·»åŠ Z-scoreç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
        z_stats = data_stats.get("z_cvd_stats", {})
        if z_stats:
            row.update({
                "z_p50": z_stats.get("p50"),
                "z_p95": z_stats.get("p95"),
                "z_p99": z_stats.get("p99")
            })
        
        # æ·»åŠ å»¶è¿Ÿç»Ÿè®¡
        latency_stats = data_stats.get("latency_stats", {})
        if latency_stats:
            row.update({
                "latency_p50": latency_stats.get("p50"),
                "latency_p95": latency_stats.get("p95"),
                "latency_p99": latency_stats.get("p99")
            })
        
        data.append(row)
    
    df = pd.DataFrame(data)
    
    # è®¡ç®—æ’ååˆ†æ•°
    if not df.empty:
        # åŸºäºéªŒè¯é€šè¿‡ç‡å’Œæ•°æ®é‡è®¡ç®—åˆ†æ•°
        df["score"] = (
            df["validation_passed"] / df["validation_total"] * 0.4 +
            (df["total_records"] / df["total_records"].max()) * 0.3 +
            (1 - df["latency_p95"] / df["latency_p95"].max()) * 0.3
        )
        df = df.sort_values("score", ascending=False)
    
    return df

def generate_report(results: List[Dict[str, Any]], df: pd.DataFrame, output_dir: Path, report_file: Path):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report_content = f"""# CVDå‚æ•°æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ

- **æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æµ‹è¯•æ•°é‡**: {len(results)}
- **æˆåŠŸæ•°é‡**: {len([r for r in results if r.get('return_code') == 0])}
- **å¤±è´¥æ•°é‡**: {len([r for r in results if r.get('return_code') != 0])}

## å‚æ•°ç»„åˆæ’å

| æ’å | é…ç½®ID | éªŒè¯é€šè¿‡ç‡ | è®°å½•æ•° | å»¶è¿ŸP95 | åˆ†æ•° |
|------|--------|------------|--------|---------|------|
"""
    
    for i, (_, row) in enumerate(df.head(10).iterrows(), 1):
        report_content += f"| {i} | {row['config_id']} | {row['validation_passed']}/{row['validation_total']} | {row['total_records']} | {row.get('latency_p95', 'N/A'):.1f}ms | {row['score']:.3f} |\n"
    
    report_content += f"""

## è¯¦ç»†ç»“æœ

### Top 3 é…ç½®è¯¦æƒ…

"""
    
    for i, (_, row) in enumerate(df.head(3).iterrows(), 1):
        config_id = row['config_id']
        result = results[config_id] if config_id < len(results) else {}
        
        report_content += f"""
#### é…ç½® {i}: {row['config_file']}

- **éªŒè¯é€šè¿‡ç‡**: {row['validation_passed']}/{row['validation_total']}
- **æ€»è®°å½•æ•°**: {row['total_records']}
- **å¹³å‡é€Ÿç‡**: {row['avg_rate_per_sec']:.2f} æ¡/ç§’
- **å»¶è¿ŸP95**: {row.get('latency_p95', 'N/A'):.1f}ms
- **åˆ†æ•°**: {row['score']:.3f}

"""
    
    # ä¿å­˜æŠ¥å‘Š
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    parser = argparse.ArgumentParser(description="CVDå‚æ•°ç½‘æ ¼æœç´¢ä¸å¯¹æ¯”åˆ†æ")
    parser.add_argument("--symbol", default="ETHUSDT", help="äº¤æ˜“å¯¹")
    parser.add_argument("--duration", type=int, default=1200, help="æµ‹è¯•æ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--warmup-sec", type=int, default=300, help="é¢„çƒ­æ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--grid", required=True, help="å‚æ•°ç½‘æ ¼è§„æ ¼")
    parser.add_argument("--out", default="./data/cvd_grid_test", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--report", default="./docs/reports/PARAMETER_SENSITIVITY_ANALYSIS.md", help="æŠ¥å‘Šæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.out)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è§£æå‚æ•°ç½‘æ ¼
    params = parse_grid_spec(args.grid)
    print(f"å‚æ•°ç½‘æ ¼: {params}")
    
    # ç”Ÿæˆå‚æ•°ç»„åˆ
    combinations = generate_config_combinations(params)
    print(f"ç”Ÿæˆ {len(combinations)} ä¸ªå‚æ•°ç»„åˆ")
    
    # åˆ›å»ºé…ç½®ç›®å½•
    config_dir = output_dir / "configs"
    config_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆé…ç½®æ–‡ä»¶
    config_files = []
    for i, combo in enumerate(combinations):
        env_file = create_env_file(combo, config_dir, i)
        config_files.append(env_file)
        print(f"åˆ›å»ºé…ç½® {i+1}/{len(combinations)}: {env_file.name}")
    
    # è¿è¡Œæµ‹è¯•
    print(f"\nå¼€å§‹è¿è¡Œ {len(combinations)} ä¸ªæµ‹è¯•...")
    results = []
    
    for i, env_file in enumerate(config_files):
        print(f"è¿è¡Œæµ‹è¯• {i+1}/{len(config_files)}: {env_file.name}")
        result = run_cvd_test(env_file, args.symbol, args.duration, output_dir)
        results.append(result)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        results_file = output_dir / "grid_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
    
    # åˆ†æç»“æœ
    print("\nåˆ†æç»“æœ...")
    df = analyze_results(results)
    
    # ä¿å­˜æ’åè¡¨
    rank_file = output_dir / "grid_rank_table.csv"
    df.to_csv(rank_file, index=False)
    print(f"æ’åè¡¨å·²ä¿å­˜åˆ°: {rank_file}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report_file = Path(args.report)
    report_file.parent.mkdir(parents=True, exist_ok=True)
    generate_report(results, df, output_dir, report_file)
    
    print(f"\nâœ… ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"ğŸ“Š ç»“æœç›®å½•: {output_dir}")
    print(f"ğŸ“‹ æŠ¥å‘Šæ–‡ä»¶: {report_file}")
    print(f"ğŸ† Top 3 é…ç½®:")
    for i, (_, row) in enumerate(df.head(3).iterrows(), 1):
        print(f"  {i}. é…ç½®{row['config_id']}: åˆ†æ•°={row['score']:.3f}, éªŒè¯={row['validation_passed']}/{row['validation_total']}")

if __name__ == "__main__":
    main()
