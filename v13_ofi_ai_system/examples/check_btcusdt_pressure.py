#!/usr/bin/env python3
"""
BTCUSDTå‹åŠ›æµ‹è¯•ç›‘æ§è„šæœ¬
ç›‘æ§é«˜é¢‘äº¤æ˜“å¯¹ä¸‹çš„P0-Bä¿®å¤æ•ˆæœ
"""

import sys
import os
import time
import json
from pathlib import Path
import pandas as pd
import io

# Windows Unicodeæ”¯æŒ
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

def check_btcusdt_progress():
    """æ£€æŸ¥BTCUSDTå‹åŠ›æµ‹è¯•è¿›åº¦"""
    data_dir = Path("v13_ofi_ai_system/data/cvd_p0b_btcusdt_pressure")
    
    print("ğŸ” BTCUSDTå‹åŠ›æµ‹è¯•ç›‘æ§")
    print("=" * 50)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    parquet_files = list(data_dir.glob("*.parquet"))
    json_files = list(data_dir.glob("*.json"))
    
    if not parquet_files:
        print("â³ æµ‹è¯•è¿›è¡Œä¸­ï¼Œæ•°æ®æ–‡ä»¶å°šæœªç”Ÿæˆ...")
        return
    
    # è¯»å–æœ€æ–°æ•°æ®
    latest_parquet = max(parquet_files, key=lambda x: x.stat().st_mtime)
    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {latest_parquet.name}")
    
    try:
        df = pd.read_parquet(latest_parquet)
        print(f"ğŸ“ˆ å½“å‰æ•°æ®ç‚¹æ•°: {len(df):,}")
        
        if len(df) > 0:
            # æ—¶é—´è·¨åº¦
            time_span = (df['timestamp'].max() - df['timestamp'].min()) / 60
            print(f"â±ï¸  æ—¶é—´è·¨åº¦: {time_span:.1f} åˆ†é’Ÿ")
            
            # æ¶ˆæ¯é¢‘ç‡
            if time_span > 0:
                freq = len(df) / time_span
                print(f"ğŸ“Š æ¶ˆæ¯é¢‘ç‡: {freq:.1f} æ¡/åˆ†é’Ÿ")
            
            # å…³é”®æŒ‡æ ‡
            print("\nğŸ¯ å…³é”®æŒ‡æ ‡:")
            print(f"  - agg_dup_count: {df.get('agg_dup_count', [0]).iloc[-1] if 'agg_dup_count' in df.columns else 'N/A'}")
            print(f"  - agg_backward_count: {df.get('agg_backward_count', [0]).iloc[-1] if 'agg_backward_count' in df.columns else 'N/A'}")
            print(f"  - late_event_dropped: {df.get('late_event_dropped', [0]).iloc[-1] if 'late_event_dropped' in df.columns else 'N/A'}")
            
            # å»¶è¿Ÿç»Ÿè®¡
            if 'latency_ms' in df.columns:
                p95_latency = df['latency_ms'].quantile(0.95)
                print(f"  - å»¶è¿ŸP95: {p95_latency:.1f}ms")
            
            # æ°´ä½çº¿å¥åº·
            if 'buffer_size_p95' in df.columns:
                buffer_p95 = df['buffer_size_p95'].iloc[-1]
                buffer_max = df['buffer_size_max'].iloc[-1]
                print(f"  - æ°´ä½çº¿P95: {buffer_p95}")
                print(f"  - æ°´ä½çº¿Max: {buffer_max}")
            
            # CVDè¿ç»­æ€§æ£€æŸ¥ï¼ˆæŠ½æ ·ï¼‰
            if len(df) > 100:
                sample_df = df.sample(min(1000, len(df)))
                cvd_diffs = sample_df['cvd'].diff().dropna()
                continuity_errors = (abs(cvd_diffs) > 1e-6).sum()
                print(f"  - è¿ç»­æ€§é”™è¯¯(æŠ½æ ·): {continuity_errors}/1000")
        
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
    
    # æ£€æŸ¥JSONæŠ¥å‘Š
    if json_files:
        latest_json = max(json_files, key=lambda x: x.stat().st_mtime)
        try:
            with open(latest_json, 'r') as f:
                report = json.load(f)
            
            print(f"\nğŸ“‹ è¿è¡ŒæŠ¥å‘Š: {latest_json.name}")
            print(f"  - æ€»æ¶ˆæ¯æ•°: {report.get('total_messages', 'N/A')}")
            print(f"  - è§£æé”™è¯¯: {report.get('parse_errors', 'N/A')}")
            print(f"  - é‡è¿æ¬¡æ•°: {report.get('reconnect_count', 'N/A')}")
            print(f"  - é˜Ÿåˆ—ä¸¢å¼ƒç‡: {report.get('queue_dropped_rate', 'N/A')}")
            
        except Exception as e:
            print(f"âŒ è¯»å–æŠ¥å‘Šå¤±è´¥: {e}")

if __name__ == "__main__":
    check_btcusdt_progress()
