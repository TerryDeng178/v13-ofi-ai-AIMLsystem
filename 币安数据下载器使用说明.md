# å¸å®‰æ•°æ®ä¸‹è½½å™¨ä½¿ç”¨è¯´æ˜

## ğŸ“‹ æ¦‚è¿°

åŸºäºç°æœ‰æ°¸ç»­AIå¢å¼ºäº¤æ˜“ç³»ç»Ÿçš„æ•°æ®è·å–æ¨¡å—ï¼Œæœ¬æ–‡æ¡£æä¾›å®Œæ•´çš„å¸å®‰æ•°æ®ä¸‹è½½å™¨ä½¿ç”¨è¯´æ˜ï¼Œæ”¯æŒä¸‹è½½6ä¸ªæœˆå†å²Kçº¿æ•°æ®ã€èµ„é‡‘è´¹ç‡ã€æŒä»“é‡ç­‰å¤šç»´åº¦å¸‚åœºæ•°æ®ã€‚

**é€‚ç”¨åœºæ™¯**: é‡åŒ–äº¤æ˜“ã€ç­–ç•¥å›æµ‹ã€å¸‚åœºåˆ†æã€æ•°æ®ç ”ç©¶  
**æ•°æ®æº**: å¸å®‰æœŸè´§API (Binance Futures API)  
**æ”¯æŒå‘¨æœŸ**: 1åˆ†é’Ÿã€3åˆ†é’Ÿã€5åˆ†é’Ÿã€15åˆ†é’Ÿã€30åˆ†é’Ÿã€1å°æ—¶ã€4å°æ—¶ã€1å¤©  
**å†å²èŒƒå›´**: æœ€é•¿6ä¸ªæœˆå†å²æ•°æ®  

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install requests pandas numpy datetime pathlib

# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/cache
mkdir -p data/basis_history  
mkdir -p data/fr_oi_history
```

### 2. åŸºç¡€é…ç½®

```python
# å¸å®‰APIé…ç½®
BINANCE_FAPI_BASE = "https://fapi.binance.com"

# æ”¯æŒçš„æ—¶é—´å‘¨æœŸ
SUPPORTED_INTERVALS = ["1m", "3m", "5m", "15m", "30m", "1h", "4h", "1d"]

# æ”¯æŒçš„ä¸»è¦äº¤æ˜“å¯¹
SUPPORTED_SYMBOLS = ["BNBUSDT", "ETHUSDT", "BTCUSDT", "ADAUSDT", "SOLUSDT"]
```

---

## ğŸ“Š æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 1. Kçº¿æ•°æ®ä¸‹è½½å™¨

#### åŸºç¡€Kçº¿ä¸‹è½½
```python
def fetch_klines(symbol: str, interval: str, limit: int = 1000) -> pd.DataFrame:
    """
    è·å–Kçº¿æ•°æ®
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å· (å¦‚: "BNBUSDT")
        interval: Kçº¿å‘¨æœŸ (å¦‚: "1m", "5m", "1h")
        limit: è·å–æ ¹æ•° (æœ€å¤§1000)
        
    Returns:
        åŒ…å«Kçº¿æ•°æ®çš„DataFrame
    """
    url = f"{BINANCE_FAPI_BASE}/fapi/v1/klines"
    params = {
        "symbol": symbol,
        "interval": interval,
        "limit": min(limit, 1000)  # å¸å®‰APIé™åˆ¶
    }
    
    response = requests.get(url, params=params, timeout=10)
    response.raise_for_status()
    
    data = response.json()
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(data, columns=[
        'open_time', 'open', 'high', 'low', 'close', 'volume',
        'close_time', 'quote_asset_volume', 'number_of_trades',
        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
    ])
    
    # æ•°æ®ç±»å‹è½¬æ¢
    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
    
    numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                      'quote_asset_volume', 'taker_buy_base_asset_volume', 
                      'taker_buy_quote_asset_volume']
    for col in numeric_columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    df['number_of_trades'] = pd.to_numeric(df['number_of_trades'], errors='coerce')
    
    return df
```

#### æ‰¹é‡å†å²æ•°æ®ä¸‹è½½
```python
def download_historical_data(symbol: str, interval: str, months: int = 6):
    """
    ä¸‹è½½6ä¸ªæœˆå†å²æ•°æ®
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        interval: Kçº¿å‘¨æœŸ
        months: ä¸‹è½½æœˆæ•° (é»˜è®¤6ä¸ªæœˆ)
    """
    from datetime import datetime, timedelta
    import time
    
    # è®¡ç®—æ—¶é—´èŒƒå›´
    end_time = datetime.now()
    start_time = end_time - timedelta(days=months * 30)
    
    all_data = []
    current_time = start_time
    
    while current_time < end_time:
        # æ¯æ¬¡è·å–1000æ ¹Kçº¿
        df = fetch_klines(symbol, interval, 1000)
        
        if df.empty:
            break
            
        all_data.append(df)
        
        # æ›´æ–°æ—¶é—´åˆ°ä¸‹ä¸€æ‰¹
        current_time = df['close_time'].max()
        
        # é¿å…APIé™åˆ¶
        time.sleep(0.1)
        
        print(f"å·²ä¸‹è½½ {len(all_data)} æ‰¹æ•°æ®ï¼Œæœ€æ–°æ—¶é—´: {current_time}")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    final_df = pd.concat(all_data, ignore_index=True)
    final_df = final_df.drop_duplicates(subset=['open_time']).sort_values('open_time')
    
    # ä¿å­˜æ•°æ®
    cache_file = f"data/cache/{symbol}_{interval}.pkl"
    final_df.to_pickle(cache_file)
    
    print(f"âœ… æ•°æ®ä¸‹è½½å®Œæˆ: {len(final_df)} æ¡è®°å½•")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {cache_file}")
    
    return final_df
```

### 2. èµ„é‡‘è´¹ç‡ä¸‹è½½å™¨

```python
def fetch_funding_rate_history(symbol: str, limit: int = 1000):
    """
    è·å–èµ„é‡‘è´¹ç‡å†å²æ•°æ®
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        limit: è·å–æ¡æ•° (æœ€å¤§1000)
    """
    url = f"{BINANCE_FAPI_BASE}/fapi/v1/fundingRate"
    params = {
        "symbol": symbol,
        "limit": min(limit, 1000)
    }
    
    response = requests.get(url, params=params, timeout=10)
    response.raise_for_status()
    
    data = response.json()
    
    df = pd.DataFrame(data)
    df['fundingTime'] = pd.to_datetime(df['fundingTime'], unit='ms')
    df['fundingRate'] = pd.to_numeric(df['fundingRate'])
    df['markPrice'] = pd.to_numeric(df['markPrice'])
    
    return df

def download_funding_rate_6months(symbol: str):
    """
    ä¸‹è½½6ä¸ªæœˆèµ„é‡‘è´¹ç‡æ•°æ®
    """
    from datetime import datetime, timedelta
    
    all_data = []
    end_time = datetime.now()
    start_time = end_time - timedelta(days=180)  # 6ä¸ªæœˆ
    
    # åˆ†æ‰¹ä¸‹è½½
    current_time = start_time
    while current_time < end_time:
        df = fetch_funding_rate_history(symbol, 1000)
        if df.empty:
            break
            
        all_data.append(df)
        current_time = df['fundingTime'].max()
        time.sleep(0.1)
    
    # åˆå¹¶æ•°æ®
    final_df = pd.concat(all_data, ignore_index=True)
    final_df = final_df.drop_duplicates(subset=['fundingTime']).sort_values('fundingTime')
    
    # ä¿å­˜æ•°æ®
    cache_file = f"data/fr_oi_history/{symbol}_funding_rate_6months.csv"
    final_df.to_csv(cache_file, index=False)
    
    print(f"âœ… èµ„é‡‘è´¹ç‡æ•°æ®ä¸‹è½½å®Œæˆ: {len(final_df)} æ¡è®°å½•")
    return final_df
```

### 3. æŒä»“é‡æ•°æ®ä¸‹è½½å™¨

```python
def fetch_open_interest_history(symbol: str, period: str = "5m", limit: int = 1000):
    """
    è·å–æŒä»“é‡å†å²æ•°æ®
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        period: æ•°æ®å‘¨æœŸ (5m, 15m, 30m, 1h, 2h, 4h, 6h, 12h, 1d)
        limit: è·å–æ¡æ•°
    """
    url = f"{BINANCE_FAPI_BASE}/fapi/v1/openInterestHist"
    params = {
        "symbol": symbol,
        "period": period,
        "limit": min(limit, 1000)
    }
    
    response = requests.get(url, params=params, timeout=10)
    response.raise_for_status()
    
    data = response.json()
    
    df = pd.DataFrame(data)
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df['sumOpenInterest'] = pd.to_numeric(df['sumOpenInterest'])
    df['sumOpenInterestValue'] = pd.to_numeric(df['sumOpenInterestValue'])
    
    return df

def download_oi_6months(symbol: str):
    """
    ä¸‹è½½6ä¸ªæœˆæŒä»“é‡æ•°æ®
    """
    all_data = []
    end_time = datetime.now()
    start_time = end_time - timedelta(days=180)
    
    current_time = start_time
    while current_time < end_time:
        df = fetch_open_interest_history(symbol, "1h", 1000)
        if df.empty:
            break
            
        all_data.append(df)
        current_time = df['timestamp'].max()
        time.sleep(0.1)
    
    final_df = pd.concat(all_data, ignore_index=True)
    final_df = final_df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
    
    cache_file = f"data/fr_oi_history/{symbol}_open_interest_6months.csv"
    final_df.to_csv(cache_file, index=False)
    
    print(f"âœ… æŒä»“é‡æ•°æ®ä¸‹è½½å®Œæˆ: {len(final_df)} æ¡è®°å½•")
    return final_df
```

### 4. å¤šç©ºæ¯”æ•°æ®ä¸‹è½½å™¨

```python
def fetch_long_short_ratio(symbol: str, period: str = "5m", limit: int = 1000):
    """
    è·å–å¤šç©ºæ¯”å†å²æ•°æ®
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        period: æ•°æ®å‘¨æœŸ
        limit: è·å–æ¡æ•°
    """
    url = f"{BINANCE_FAPI_BASE}/fapi/v1/globalLongShortAccountRatio"
    params = {
        "symbol": symbol,
        "period": period,
        "limit": min(limit, 1000)
    }
    
    response = requests.get(url, params=params, timeout=10)
    response.raise_for_status()
    
    data = response.json()
    
    df = pd.DataFrame(data)
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df['longShortRatio'] = pd.to_numeric(df['longShortRatio'])
    df['longAccount'] = pd.to_numeric(df['longAccount'])
    df['shortAccount'] = pd.to_numeric(df['shortAccount'])
    
    return df
```

---

## ğŸ”§ å®Œæ•´ä¸‹è½½è„šæœ¬

### ä¸€é”®ä¸‹è½½6ä¸ªæœˆæ•°æ®

```python
#!/usr/bin/env python3
"""
å¸å®‰æ•°æ®ä¸‹è½½å™¨ - 6ä¸ªæœˆå†å²æ•°æ®æ‰¹é‡ä¸‹è½½
"""

import os
import time
import requests
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path

class BinanceDataDownloader:
    def __init__(self, base_url="https://fapi.binance.com"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def download_klines_6months(self, symbol: str, interval: str):
        """ä¸‹è½½6ä¸ªæœˆKçº¿æ•°æ®"""
        print(f"ğŸš€ å¼€å§‹ä¸‹è½½ {symbol} {interval} 6ä¸ªæœˆKçº¿æ•°æ®...")
        
        all_data = []
        end_time = datetime.now()
        start_time = end_time - timedelta(days=180)
        
        current_time = start_time
        batch_count = 0
        
        while current_time < end_time:
            try:
                # è·å–æ•°æ®
                df = self.fetch_klines(symbol, interval, 1000)
                
                if df.empty:
                    print("âš ï¸ æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œä¸‹è½½å®Œæˆ")
                    break
                
                all_data.append(df)
                batch_count += 1
                current_time = df['close_time'].max()
                
                print(f"ğŸ“Š æ‰¹æ¬¡ {batch_count}: {len(df)} æ¡æ•°æ®, æœ€æ–°æ—¶é—´: {current_time}")
                
                # APIé™åˆ¶ä¿æŠ¤
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
                time.sleep(1)
                continue
        
        if all_data:
            # åˆå¹¶æ•°æ®
            final_df = pd.concat(all_data, ignore_index=True)
            final_df = final_df.drop_duplicates(subset=['open_time']).sort_values('open_time')
            
            # ä¿å­˜æ•°æ®
            os.makedirs("data/cache", exist_ok=True)
            cache_file = f"data/cache/{symbol}_{interval}.pkl"
            final_df.to_pickle(cache_file)
            
            print(f"âœ… Kçº¿æ•°æ®ä¸‹è½½å®Œæˆ: {len(final_df)} æ¡è®°å½•")
            print(f"ğŸ“ ä¿å­˜ä½ç½®: {cache_file}")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {final_df['open_time'].min()} ~ {final_df['open_time'].max()}")
            
            return final_df
        else:
            print("âŒ æ²¡æœ‰ä¸‹è½½åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
    
    def download_funding_rate_6months(self, symbol: str):
        """ä¸‹è½½6ä¸ªæœˆèµ„é‡‘è´¹ç‡æ•°æ®"""
        print(f"ğŸ’° å¼€å§‹ä¸‹è½½ {symbol} 6ä¸ªæœˆèµ„é‡‘è´¹ç‡æ•°æ®...")
        
        all_data = []
        end_time = datetime.now()
        start_time = end_time - timedelta(days=180)
        
        current_time = start_time
        batch_count = 0
        
        while current_time < end_time:
            try:
                df = self.fetch_funding_rate_history(symbol, 1000)
                
                if df.empty:
                    break
                
                all_data.append(df)
                batch_count += 1
                current_time = df['fundingTime'].max()
                
                print(f"ğŸ’° æ‰¹æ¬¡ {batch_count}: {len(df)} æ¡æ•°æ®")
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ èµ„é‡‘è´¹ç‡ä¸‹è½½å¤±è´¥: {e}")
                time.sleep(1)
                continue
        
        if all_data:
            final_df = pd.concat(all_data, ignore_index=True)
            final_df = final_df.drop_duplicates(subset=['fundingTime']).sort_values('fundingTime')
            
            os.makedirs("data/fr_oi_history", exist_ok=True)
            cache_file = f"data/fr_oi_history/{symbol}_funding_rate_6months.csv"
            final_df.to_csv(cache_file, index=False)
            
            print(f"âœ… èµ„é‡‘è´¹ç‡æ•°æ®ä¸‹è½½å®Œæˆ: {len(final_df)} æ¡è®°å½•")
            return final_df
        
        return pd.DataFrame()
    
    def download_oi_6months(self, symbol: str):
        """ä¸‹è½½6ä¸ªæœˆæŒä»“é‡æ•°æ®"""
        print(f"ğŸ“Š å¼€å§‹ä¸‹è½½ {symbol} 6ä¸ªæœˆæŒä»“é‡æ•°æ®...")
        
        all_data = []
        end_time = datetime.now()
        start_time = end_time - timedelta(days=180)
        
        current_time = start_time
        batch_count = 0
        
        while current_time < end_time:
            try:
                df = self.fetch_open_interest_history(symbol, "1h", 1000)
                
                if df.empty:
                    break
                
                all_data.append(df)
                batch_count += 1
                current_time = df['timestamp'].max()
                
                print(f"ğŸ“Š æ‰¹æ¬¡ {batch_count}: {len(df)} æ¡æ•°æ®")
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ æŒä»“é‡ä¸‹è½½å¤±è´¥: {e}")
                time.sleep(1)
                continue
        
        if all_data:
            final_df = pd.concat(all_data, ignore_index=True)
            final_df = final_df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
            
            os.makedirs("data/fr_oi_history", exist_ok=True)
            cache_file = f"data/fr_oi_history/{symbol}_open_interest_6months.csv"
            final_df.to_csv(cache_file, index=False)
            
            print(f"âœ… æŒä»“é‡æ•°æ®ä¸‹è½½å®Œæˆ: {len(final_df)} æ¡è®°å½•")
            return final_df
        
        return pd.DataFrame()
    
    def fetch_klines(self, symbol: str, interval: str, limit: int):
        """è·å–Kçº¿æ•°æ®"""
        url = f"{self.base_url}/fapi/v1/klines"
        params = {"symbol": symbol, "interval": interval, "limit": limit}
        
        response = self.session.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        df = pd.DataFrame(data, columns=[
            'open_time', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_asset_volume', 'number_of_trades',
            'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
        ])
        
        # æ•°æ®ç±»å‹è½¬æ¢
        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
        df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
        
        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                          'quote_asset_volume', 'taker_buy_base_asset_volume', 
                          'taker_buy_quote_asset_volume']
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df['number_of_trades'] = pd.to_numeric(df['number_of_trades'], errors='coerce')
        
        return df
    
    def fetch_funding_rate_history(self, symbol: str, limit: int):
        """è·å–èµ„é‡‘è´¹ç‡å†å²æ•°æ®"""
        url = f"{self.base_url}/fapi/v1/fundingRate"
        params = {"symbol": symbol, "limit": limit}
        
        response = self.session.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        df = pd.DataFrame(data)
        df['fundingTime'] = pd.to_datetime(df['fundingTime'], unit='ms')
        df['fundingRate'] = pd.to_numeric(df['fundingRate'])
        df['markPrice'] = pd.to_numeric(df['markPrice'])
        
        return df
    
    def fetch_open_interest_history(self, symbol: str, period: str, limit: int):
        """è·å–æŒä»“é‡å†å²æ•°æ®"""
        url = f"{self.base_url}/fapi/v1/openInterestHist"
        params = {"symbol": symbol, "period": period, "limit": limit}
        
        response = self.session.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        df = pd.DataFrame(data)
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df['sumOpenInterest'] = pd.to_numeric(df['sumOpenInterest'])
        df['sumOpenInterestValue'] = pd.to_numeric(df['sumOpenInterestValue'])
        
        return df

def main():
    """ä¸»å‡½æ•° - æ‰¹é‡ä¸‹è½½æ•°æ®"""
    
    # é…ç½®å‚æ•°
    SYMBOLS = ["BNBUSDT", "ETHUSDT", "BTCUSDT"]
    INTERVALS = ["1m", "5m", "15m", "1h"]
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = BinanceDataDownloader()
    
    print("ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½å¸å®‰6ä¸ªæœˆå†å²æ•°æ®...")
    print(f"ğŸ“Š äº¤æ˜“å¯¹: {SYMBOLS}")
    print(f"â° æ—¶é—´å‘¨æœŸ: {INTERVALS}")
    print("=" * 50)
    
    # ä¸‹è½½Kçº¿æ•°æ®
    for symbol in SYMBOLS:
        for interval in INTERVALS:
            try:
                downloader.download_klines_6months(symbol, interval)
                print(f"âœ… {symbol} {interval} ä¸‹è½½å®Œæˆ")
            except Exception as e:
                print(f"âŒ {symbol} {interval} ä¸‹è½½å¤±è´¥: {e}")
            
            time.sleep(1)  # é¿å…APIé™åˆ¶
    
    # ä¸‹è½½èµ„é‡‘è´¹ç‡æ•°æ®
    for symbol in SYMBOLS:
        try:
            downloader.download_funding_rate_6months(symbol)
            print(f"âœ… {symbol} èµ„é‡‘è´¹ç‡ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"âŒ {symbol} èµ„é‡‘è´¹ç‡ä¸‹è½½å¤±è´¥: {e}")
        
        time.sleep(1)
    
    # ä¸‹è½½æŒä»“é‡æ•°æ®
    for symbol in SYMBOLS:
        try:
            downloader.download_oi_6months(symbol)
            print(f"âœ… {symbol} æŒä»“é‡ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"âŒ {symbol} æŒä»“é‡ä¸‹è½½å¤±è´¥: {e}")
        
        time.sleep(1)
    
    print("ğŸ‰ æ‰€æœ‰æ•°æ®ä¸‹è½½å®Œæˆï¼")
    print("ğŸ“ æ•°æ®ä¿å­˜ä½ç½®:")
    print("   - Kçº¿æ•°æ®: data/cache/")
    print("   - èµ„é‡‘è´¹ç‡: data/fr_oi_history/")
    print("   - æŒä»“é‡: data/fr_oi_history/")

if __name__ == "__main__":
    main()
```

---

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### 1. å•æ¬¡ä¸‹è½½ç¤ºä¾‹

```python
# åˆ›å»ºä¸‹è½½å™¨
downloader = BinanceDataDownloader()

# ä¸‹è½½BNBUSDT 1åˆ†é’ŸKçº¿æ•°æ® (6ä¸ªæœˆ)
df = downloader.download_klines_6months("BNBUSDT", "1m")

# ä¸‹è½½èµ„é‡‘è´¹ç‡æ•°æ®
fr_df = downloader.download_funding_rate_6months("BNBUSDT")

# ä¸‹è½½æŒä»“é‡æ•°æ®
oi_df = downloader.download_oi_6months("BNBUSDT")
```

### 2. æ‰¹é‡ä¸‹è½½ç¤ºä¾‹

```python
# æ‰¹é‡ä¸‹è½½å¤šä¸ªäº¤æ˜“å¯¹
symbols = ["BNBUSDT", "ETHUSDT", "BTCUSDT"]
intervals = ["1m", "5m", "15m", "1h"]

for symbol in symbols:
    for interval in intervals:
        downloader.download_klines_6months(symbol, interval)
        time.sleep(1)  # é¿å…APIé™åˆ¶
```

### 3. æ•°æ®éªŒè¯ç¤ºä¾‹

```python
# éªŒè¯ä¸‹è½½çš„æ•°æ®
def validate_data(df, data_type="Kçº¿"):
    print(f"ğŸ“Š {data_type}æ•°æ®éªŒè¯:")
    print(f"   æ•°æ®é‡: {len(df)} æ¡")
    print(f"   æ—¶é—´èŒƒå›´: {df.index.min()} ~ {df.index.max()}")
    print(f"   ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
    print(f"   æ•°æ®ç±»å‹: {df.dtypes.to_dict()}")

# éªŒè¯Kçº¿æ•°æ®
df = pd.read_pickle("data/cache/BNBUSDT_1m.pkl")
validate_data(df, "Kçº¿")
```

---

## âš™ï¸ é…ç½®å‚æ•°

### APIé™åˆ¶é…ç½®

```python
# APIè¯·æ±‚é™åˆ¶
API_CONFIG = {
    "max_requests_per_second": 10,      # æ¯ç§’æœ€å¤§è¯·æ±‚æ•°
    "request_delay": 0.1,               # è¯·æ±‚é—´éš”(ç§’)
    "timeout": 10,                      # è¯·æ±‚è¶…æ—¶(ç§’)
    "max_retries": 3,                   # æœ€å¤§é‡è¯•æ¬¡æ•°
    "backoff_factor": 1.0               # é‡è¯•é€€é¿å› å­
}
```

### æ•°æ®å­˜å‚¨é…ç½®

```python
# æ•°æ®å­˜å‚¨é…ç½®
STORAGE_CONFIG = {
    "cache_dir": "data/cache",          # Kçº¿æ•°æ®ç›®å½•
    "fr_oi_dir": "data/fr_oi_history",  # èµ„é‡‘è´¹ç‡/æŒä»“é‡ç›®å½•
    "basis_dir": "data/basis_history", # åŸºå·®æ•°æ®ç›®å½•
    "backup_dir": "data/backup"         # å¤‡ä»½ç›®å½•
}
```

### ä¸‹è½½èŒƒå›´é…ç½®

```python
# ä¸‹è½½èŒƒå›´é…ç½®
DOWNLOAD_CONFIG = {
    "months": 6,                        # ä¸‹è½½æœˆæ•°
    "symbols": ["BNBUSDT", "ETHUSDT", "BTCUSDT"],  # äº¤æ˜“å¯¹åˆ—è¡¨
    "intervals": ["1m", "5m", "15m", "1h"],        # æ—¶é—´å‘¨æœŸåˆ—è¡¨
    "auto_retry": True,                 # è‡ªåŠ¨é‡è¯•
    "save_format": "pkl"                # ä¿å­˜æ ¼å¼ (pkl/csv/json)
}
```

---

## ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥

### 1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥

```python
def check_data_integrity(df, expected_interval="1m"):
    """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
    
    # æ£€æŸ¥æ—¶é—´é—´éš”
    time_diff = df['open_time'].diff().dropna()
    expected_minutes = {"1m": 1, "5m": 5, "15m": 15, "1h": 60}
    expected_diff = expected_minutes.get(expected_interval, 1)
    
    irregular_intervals = time_diff[time_diff != pd.Timedelta(minutes=expected_diff)]
    
    print(f"ğŸ“Š æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
    print(f"   æ€»æ•°æ®é‡: {len(df)} æ¡")
    print(f"   æ—¶é—´èŒƒå›´: {df['open_time'].min()} ~ {df['open_time'].max()}")
    print(f"   å¼‚å¸¸é—´éš”: {len(irregular_intervals)} ä¸ª")
    print(f"   ç¼ºå¤±å€¼: {df.isnull().sum().sum()} ä¸ª")
    
    return len(irregular_intervals) == 0
```

### 2. æ•°æ®å‡†ç¡®æ€§æ£€æŸ¥

```python
def check_data_accuracy(df):
    """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§"""
    
    # æ£€æŸ¥ä»·æ ¼é€»è¾‘
    price_errors = df[(df['high'] < df['low']) | 
                     (df['high'] < df['open']) | 
                     (df['high'] < df['close']) |
                     (df['low'] > df['open']) | 
                     (df['low'] > df['close'])]
    
    # æ£€æŸ¥æˆäº¤é‡
    volume_errors = df[df['volume'] < 0]
    
    print(f"ğŸ” æ•°æ®å‡†ç¡®æ€§æ£€æŸ¥:")
    print(f"   ä»·æ ¼é€»è¾‘é”™è¯¯: {len(price_errors)} æ¡")
    print(f"   æˆäº¤é‡é”™è¯¯: {len(volume_errors)} æ¡")
    print(f"   æ•°æ®è´¨é‡: {'âœ… è‰¯å¥½' if len(price_errors) == 0 and len(volume_errors) == 0 else 'âŒ æœ‰é—®é¢˜'}")
    
    return len(price_errors) == 0 and len(volume_errors) == 0
```

---

## ğŸš¨ æ³¨æ„äº‹é¡¹

### 1. APIé™åˆ¶
- **è¯·æ±‚é¢‘ç‡**: å¸å®‰APIé™åˆ¶æ¯ç§’æœ€å¤š10ä¸ªè¯·æ±‚
- **æ•°æ®é‡é™åˆ¶**: å•æ¬¡è¯·æ±‚æœ€å¤š1000æ¡æ•°æ®
- **IPé™åˆ¶**: åŒä¸€IPæœ‰è¯·æ±‚é¢‘ç‡é™åˆ¶

### 2. ç½‘ç»œç¨³å®šæ€§
- **è¶…æ—¶è®¾ç½®**: å»ºè®®è®¾ç½®10ç§’è¶…æ—¶
- **é‡è¯•æœºåˆ¶**: ç½‘ç»œå¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•
- **æ–­ç‚¹ç»­ä¼ **: æ”¯æŒä¸­æ–­åç»§ç»­ä¸‹è½½

### 3. æ•°æ®å­˜å‚¨
- **ç£ç›˜ç©ºé—´**: 6ä¸ªæœˆæ•°æ®çº¦éœ€è¦1-2GBç©ºé—´
- **æ–‡ä»¶æ ¼å¼**: å»ºè®®ä½¿ç”¨Pickleæ ¼å¼ä¿å­˜
- **å¤‡ä»½ç­–ç•¥**: é‡è¦æ•°æ®éœ€è¦å®šæœŸå¤‡ä»½

### 4. æ³•å¾‹åˆè§„
- **ä½¿ç”¨æ¡æ¬¾**: éµå®ˆå¸å®‰APIä½¿ç”¨æ¡æ¬¾
- **æ•°æ®ç”¨é€”**: ä»…ç”¨äºç ”ç©¶å’Œå­¦ä¹ ç›®çš„
- **å•†ä¸šä½¿ç”¨**: å•†ä¸šä½¿ç”¨éœ€è¦è·å¾—æˆæƒ

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### å¸¸è§é—®é¢˜

**Q: ä¸‹è½½é€Ÿåº¦å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ**
A: å¯ä»¥è°ƒæ•´è¯·æ±‚é—´éš”ï¼Œä½†ä¸è¦ä½äº0.1ç§’ï¼Œé¿å…è§¦å‘APIé™åˆ¶ã€‚

**Q: æ•°æ®ä¸å®Œæ•´æ€ä¹ˆåŠï¼Ÿ**
A: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œä½¿ç”¨é‡è¯•æœºåˆ¶ï¼Œæˆ–è€…åˆ†æ®µä¸‹è½½ã€‚

**Q: å¦‚ä½•éªŒè¯æ•°æ®è´¨é‡ï¼Ÿ**
A: ä½¿ç”¨æä¾›çš„æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°ï¼Œæ£€æŸ¥æ—¶é—´é—´éš”ã€ä»·æ ¼é€»è¾‘ç­‰ã€‚

**Q: æ”¯æŒå“ªäº›äº¤æ˜“å¯¹ï¼Ÿ**
A: æ”¯æŒå¸å®‰æœŸè´§æ‰€æœ‰äº¤æ˜“å¯¹ï¼Œå»ºè®®ä½¿ç”¨ä¸»æµäº¤æ˜“å¯¹å¦‚BNBUSDTã€ETHUSDTã€BTCUSDTã€‚

### è”ç³»æ–¹å¼

- **GitHub**: é¡¹ç›®ä»“åº“åœ°å€
- **æ–‡æ¡£**: è¯¦ç»†ä½¿ç”¨æ–‡æ¡£
- **ç¤¾åŒº**: æŠ€æœ¯äº¤æµç¾¤

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-10-16)
- âœ… åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… æ”¯æŒ6ä¸ªæœˆå†å²æ•°æ®ä¸‹è½½
- âœ… æ”¯æŒKçº¿ã€èµ„é‡‘è´¹ç‡ã€æŒä»“é‡æ•°æ®
- âœ… å®Œæ•´çš„æ•°æ®è´¨é‡æ£€æŸ¥
- âœ… æ‰¹é‡ä¸‹è½½åŠŸèƒ½

---

**ğŸ“š æœ¬è¯´æ˜æ–‡æ¡£åŸºäºç°æœ‰æ°¸ç»­AIå¢å¼ºäº¤æ˜“ç³»ç»Ÿï¼Œæä¾›å®Œæ•´çš„å¸å®‰æ•°æ®ä¸‹è½½è§£å†³æ–¹æ¡ˆã€‚å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒé¡¹ç›®æ–‡æ¡£æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚**
