#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸å®‰6ä¸ªæœˆæ•°æ®ä¸‹è½½å™¨
ä¸“é—¨ç”¨äºä¸‹è½½6ä¸ªæœˆå®Œæ•´å†å²æ•°æ®ï¼Œä¼˜åŒ–APIé™åˆ¶å¤„ç†
"""

import requests
import pandas as pd
import numpy as np
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
import os
import json
from typing import Dict, List, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Binance6MonthsDownloader:
    """å¸å®‰6ä¸ªæœˆæ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.base_url = "https://fapi.binance.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # ä¿å®ˆçš„APIé™åˆ¶é…ç½®
        self.api_config = {
            'request_delay': 0.2,        # æ¯ä¸ªè¯·æ±‚é—´éš”0.2ç§’
            'batch_delay': 10,           # æ‰¹æ¬¡é—´å»¶è¿Ÿ10ç§’
            'max_retries': 5,            # æœ€å¤§é‡è¯•æ¬¡æ•°
            'timeout': 30,               # è¯·æ±‚è¶…æ—¶
            'batch_size': 7,             # æ¯æ‰¹ä¸‹è½½7å¤©æ•°æ®
            'max_requests_per_minute': 30  # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
        }
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        self.data_dir = Path("data/binance")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("å¸å®‰6ä¸ªæœˆæ•°æ®ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"APIé…ç½®: è¯·æ±‚å»¶è¿Ÿ{self.api_config['request_delay']}s, æ‰¹æ¬¡å»¶è¿Ÿ{self.api_config['batch_delay']}s")
    
    def download_6months_data(self, symbol: str = "ETHUSDT", interval: str = "1m") -> bool:
        """ä¸‹è½½6ä¸ªæœˆæ•°æ®"""
        logger.info("=" * 80)
        logger.info(f"å¼€å§‹ä¸‹è½½ {symbol} {interval} 6ä¸ªæœˆæ•°æ®")
        logger.info("=" * 80)
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        end_time = datetime.now()
        start_time = end_time - timedelta(days=180)  # 6ä¸ªæœˆ
        
        logger.info(f"æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
        
        all_data = []
        current_start = start_time
        batch_count = 0
        total_records = 0
        
        while current_start < end_time:
            try:
                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸæ—¶é—´
                current_end = min(current_start + timedelta(days=self.api_config['batch_size']), end_time)
                
                logger.info(f"ğŸ“Š æ‰¹æ¬¡ {batch_count + 1}: {current_start.strftime('%Y-%m-%d')} ~ {current_end.strftime('%Y-%m-%d')}")
                
                # ä¸‹è½½å½“å‰æ‰¹æ¬¡æ•°æ®
                batch_data = self._download_batch(symbol, interval, current_start, current_end)
                
                if batch_data is not None and not batch_data.empty:
                    all_data.append(batch_data)
                    total_records += len(batch_data)
                    logger.info(f"âœ… æ‰¹æ¬¡ {batch_count + 1} æˆåŠŸ: {len(batch_data)} æ¡è®°å½•")
                else:
                    logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_count + 1} æ•°æ®ä¸ºç©º")
                
                # æ›´æ–°ä¸‹ä¸€ä¸ªæ‰¹æ¬¡çš„å¼€å§‹æ—¶é—´
                current_start = current_end
                batch_count += 1
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if current_start < end_time:
                    logger.info(f"â³ ç­‰å¾… {self.api_config['batch_delay']} ç§’...")
                    time.sleep(self.api_config['batch_delay'])
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_count + 1} ä¸‹è½½å¤±è´¥: {e}")
                if "429" in str(e) or "Too Many Requests" in str(e):
                    logger.info(f"ğŸ”„ APIé™é€Ÿï¼Œç­‰å¾… {self.api_config['batch_delay'] * 2} ç§’åé‡è¯•...")
                    time.sleep(self.api_config['batch_delay'] * 2)
                else:
                    logger.error(f"âŒ éé™é€Ÿé”™è¯¯ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡: {e}")
                    current_start += timedelta(days=self.api_config['batch_size'])
                    batch_count += 1
                    continue
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            logger.info("ğŸ”„ åˆå¹¶æ•°æ®...")
            final_df = pd.concat(all_data, ignore_index=True)
            
            # å»é‡å’Œæ’åº
            final_df = final_df.drop_duplicates(subset=['timestamp']).sort_values('timestamp').reset_index(drop=True)
            
            # ä¿å­˜æ•°æ®
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{symbol}_{interval}_6months_{timestamp}.csv"
            filepath = self.data_dir / filename
            
            final_df.to_csv(filepath, index=False)
            
            logger.info("=" * 80)
            logger.info("ğŸ‰ 6ä¸ªæœˆæ•°æ®ä¸‹è½½å®Œæˆï¼")
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {len(final_df)}")
            logger.info(f"ğŸ“ ä¿å­˜ä½ç½®: {filepath}")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {final_df['timestamp'].min()} ~ {final_df['timestamp'].max()}")
            logger.info(f"ğŸ’° ä»·æ ¼èŒƒå›´: {final_df['close'].min():.2f} ~ {final_df['close'].max():.2f}")
            
            # æ•°æ®è´¨é‡åˆ†æ
            self._analyze_data_quality(final_df)
            
            return True
        else:
            logger.error("âŒ æ²¡æœ‰ä¸‹è½½åˆ°ä»»ä½•æ•°æ®")
            return False
    
    def _download_batch(self, symbol: str, interval: str, start_time: datetime, end_time: datetime) -> Optional[pd.DataFrame]:
        """ä¸‹è½½å•ä¸ªæ‰¹æ¬¡æ•°æ®"""
        retry_count = 0
        
        while retry_count < self.api_config['max_retries']:
            try:
                # è®¡ç®—æ—¶é—´æˆ³
                start_timestamp = int(start_time.timestamp() * 1000)
                end_timestamp = int(end_time.timestamp() * 1000)
                
                # æ„å»ºè¯·æ±‚URL
                url = f"{self.base_url}/fapi/v1/klines"
                params = {
                    'symbol': symbol,
                    'interval': interval,
                    'startTime': start_timestamp,
                    'endTime': end_timestamp,
                    'limit': 1000
                }
                
                # å‘é€è¯·æ±‚
                response = self.session.get(url, params=params, timeout=self.api_config['timeout'])
                
                if response.status_code == 200:
                    data = response.json()
                    
                    if not data:
                        logger.warning("APIè¿”å›ç©ºæ•°æ®")
                        return pd.DataFrame()
                    
                    # è½¬æ¢ä¸ºDataFrame
                    df = pd.DataFrame(data, columns=[
                        'timestamp', 'open', 'high', 'low', 'close', 'volume',
                        'close_time', 'quote_asset_volume', 'number_of_trades',
                        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
                    ])
                    
                    # æ•°æ®ç±»å‹è½¬æ¢
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
                    
                    numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                                      'quote_asset_volume', 'taker_buy_base_asset_volume', 
                                      'taker_buy_quote_asset_volume']
                    for col in numeric_columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    df['number_of_trades'] = pd.to_numeric(df['number_of_trades'], errors='coerce')
                    
                    # è¯·æ±‚é—´å»¶è¿Ÿ
                    time.sleep(self.api_config['request_delay'])
                    
                    return df
                    
                elif response.status_code == 429:
                    logger.warning(f"APIé™é€Ÿ (429)ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.api_config['batch_delay'])
                    retry_count += 1
                    continue
                    
                elif response.status_code == 418:
                    logger.error(f"APIè¢«é˜»æ­¢ (418)ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´...")
                    time.sleep(self.api_config['batch_delay'] * 3)
                    retry_count += 1
                    continue
                    
                else:
                    logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                    retry_count += 1
                    time.sleep(self.api_config['batch_delay'])
                    continue
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
                retry_count += 1
                time.sleep(self.api_config['batch_delay'])
                continue
                
            except Exception as e:
                logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
                retry_count += 1
                time.sleep(self.api_config['batch_delay'])
                continue
        
        logger.error(f"æ‰¹æ¬¡ä¸‹è½½å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.api_config['max_retries']}")
        return None
    
    def _analyze_data_quality(self, df: pd.DataFrame):
        """åˆ†ææ•°æ®è´¨é‡"""
        logger.info("ğŸ“Š æ•°æ®è´¨é‡åˆ†æ:")
        
        # åŸºç¡€ç»Ÿè®¡
        logger.info(f"   æ€»è®°å½•æ•°: {len(df)}")
        logger.info(f"   æ—¶é—´è·¨åº¦: {(df['timestamp'].max() - df['timestamp'].min()).days} å¤©")
        logger.info(f"   ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
        logger.info(f"   é‡å¤å€¼: {df.duplicated().sum()}")
        
        # ä»·æ ¼ç»Ÿè®¡
        logger.info(f"   å¼€ç›˜ä»·èŒƒå›´: {df['open'].min():.2f} ~ {df['open'].max():.2f}")
        logger.info(f"   æ”¶ç›˜ä»·èŒƒå›´: {df['close'].min():.2f} ~ {df['close'].max():.2f}")
        logger.info(f"   æˆäº¤é‡èŒƒå›´: {df['volume'].min():.2f} ~ {df['volume'].max():.2f}")
        
        # æ—¶é—´é—´éš”æ£€æŸ¥
        time_diffs = df['timestamp'].diff().dropna()
        expected_interval = pd.Timedelta(minutes=1)  # 1åˆ†é’ŸKçº¿
        irregular_intervals = time_diffs[time_diffs != expected_interval]
        
        if len(irregular_intervals) > 0:
            logger.warning(f"   å¼‚å¸¸æ—¶é—´é—´éš”: {len(irregular_intervals)} ä¸ª")
        else:
            logger.info("   âœ… æ—¶é—´é—´éš”æ­£å¸¸")
        
        # ä»·æ ¼é€»è¾‘æ£€æŸ¥
        price_errors = df[(df['high'] < df['low']) | 
                         (df['high'] < df['open']) | 
                         (df['high'] < df['close']) |
                         (df['low'] > df['open']) | 
                         (df['low'] > df['close'])]
        
        if len(price_errors) > 0:
            logger.warning(f"   ä»·æ ¼é€»è¾‘é”™è¯¯: {len(price_errors)} æ¡")
        else:
            logger.info("   âœ… ä»·æ ¼é€»è¾‘æ­£å¸¸")
    
    def download_multiple_symbols(self, symbols: List[str], interval: str = "1m"):
        """ä¸‹è½½å¤šä¸ªäº¤æ˜“å¯¹çš„æ•°æ®"""
        logger.info(f"å¼€å§‹ä¸‹è½½å¤šä¸ªäº¤æ˜“å¯¹çš„6ä¸ªæœˆæ•°æ®: {symbols}")
        
        results = {}
        for symbol in symbols:
            logger.info(f"\n{'='*50}")
            logger.info(f"ä¸‹è½½ {symbol} æ•°æ®")
            logger.info(f"{'='*50}")
            
            success = self.download_6months_data(symbol, interval)
            results[symbol] = success
            
            if success:
                logger.info(f"âœ… {symbol} ä¸‹è½½æˆåŠŸ")
            else:
                logger.error(f"âŒ {symbol} ä¸‹è½½å¤±è´¥")
            
            # äº¤æ˜“å¯¹é—´å»¶è¿Ÿ
            if symbol != symbols[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ªäº¤æ˜“å¯¹
                logger.info("â³ äº¤æ˜“å¯¹é—´å»¶è¿Ÿ...")
                time.sleep(self.api_config['batch_delay'])
        
        # è¾“å‡ºæ€»ç»“
        logger.info("\n" + "="*80)
        logger.info("å¤šäº¤æ˜“å¯¹ä¸‹è½½æ€»ç»“")
        logger.info("="*80)
        
        for symbol, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            logger.info(f"{symbol}: {status}")
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("å¸å®‰6ä¸ªæœˆæ•°æ®ä¸‹è½½å™¨")
    logger.info("=" * 80)
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = Binance6MonthsDownloader()
    
    # ä¸‹è½½ETHUSDTæ•°æ®
    success = downloader.download_6months_data("ETHUSDT", "1m")
    
    if success:
        logger.info("ğŸ‰ 6ä¸ªæœˆæ•°æ®ä¸‹è½½å®Œæˆï¼")
        
        # å¯é€‰ï¼šä¸‹è½½å…¶ä»–äº¤æ˜“å¯¹
        other_symbols = ["BTCUSDT", "BNBUSDT"]
        logger.info(f"æ˜¯å¦ä¸‹è½½å…¶ä»–äº¤æ˜“å¯¹æ•°æ®: {other_symbols}? (y/n)")
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç”¨æˆ·è¾“å…¥é€»è¾‘
        
    else:
        logger.error("âŒ 6ä¸ªæœˆæ•°æ®ä¸‹è½½å¤±è´¥")


if __name__ == "__main__":
    main()
