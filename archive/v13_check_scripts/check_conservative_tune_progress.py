#!/usr/bin/env python3
"""
ä¿å®ˆå¾®è°ƒè¯•éªŒçŸ©é˜µç›‘æ§è„šæœ¬
ç›‘æ§Step Aå’ŒStep Bçš„æµ‹è¯•è¿›åº¦ï¼Œé‡ç‚¹å…³æ³¨"åˆ†æ¯"å’ŒZ-scoreè´¨é‡
"""

import os
import sys
import time
import json
import io
import numpy as np
from pathlib import Path

# Windows Unicodeå…¼å®¹æ€§
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

def check_conservative_tune_progress(test_group="A1"):
    """æ£€æŸ¥ä¿å®ˆå¾®è°ƒæµ‹è¯•è¿›åº¦"""
    print(f"ğŸ” ä¿å®ˆå¾®è°ƒè¯•éªŒçŸ©é˜µç›‘æ§ - {test_group}")
    print("=" * 60)
    
    # æ ¹æ®æµ‹è¯•ç»„ç¡®å®šè¾“å‡ºç›®å½•
    if test_group == "A1":
        output_dir = Path("../data/cvd_p1_1_rollback")
        config_name = "å›æ»šåŸºçº¿ï¼ˆ300/8/5000/50ï¼‰"
    elif test_group == "A2":
        output_dir = Path("../data/cvd_step_a_tune")
        config_name = "åŠè¡°æœŸ280ï¼ˆå…¶ä½™åŒA1ï¼‰"
    elif test_group == "B1":
        output_dir = Path("../data/cvd_step_b_tune")
        config_name = "A2 + ç¨³å¥å°ºåº¦ï¼ˆåŒEWMAæˆ–MADåœ°æ¿ï¼‰"
    else:
        print(f"âŒ æœªçŸ¥æµ‹è¯•ç»„: {test_group}")
        return
    
    print(f"ğŸ“‹ æµ‹è¯•é…ç½®: {config_name}")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    if not output_dir.exists():
        print("âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œæµ‹è¯•å¯èƒ½å°šæœªå¼€å§‹")
        return
    
    # æŸ¥æ‰¾æœ€æ–°çš„parquetæ–‡ä»¶
    parquet_files = list(output_dir.glob("*.parquet"))
    if not parquet_files:
        print("â³ ç­‰å¾…æ•°æ®æ–‡ä»¶ç”Ÿæˆ...")
        return
    
    latest_file = max(parquet_files, key=lambda x: x.stat().st_mtime)
    print(f"ğŸ“ æœ€æ–°æ•°æ®æ–‡ä»¶: {latest_file.name}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´
    file_size = latest_file.stat().st_size
    mod_time = latest_file.stat().st_mtime
    current_time = time.time()
    age_seconds = current_time - mod_time
    
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:,} bytes")
    print(f"â° æœ€åæ›´æ–°: {age_seconds:.1f} ç§’å‰")
    
    # å°è¯•è¯»å–parquetæ–‡ä»¶è·å–è®°å½•æ•°
    try:
        import pandas as pd
        df = pd.read_parquet(latest_file)
        record_count = len(df)
        print(f"ğŸ“ˆ å½“å‰è®°å½•æ•°: {record_count:,}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Z-scoreæ•°æ®
        if 'z_cvd' in df.columns:
            z_data = df['z_cvd'].dropna()
            if len(z_data) > 0:
                median_z = abs(z_data).median()
                p_z_gt_2 = (abs(z_data) > 2).mean() * 100
                p_z_gt_3 = (abs(z_data) > 3).mean() * 100
                
                print(f"ğŸ¯ Z-scoreè´¨é‡:")
                print(f"   median(|Z|): {median_z:.4f}")
                print(f"   P(|Z|>2): {p_z_gt_2:.2f}%")
                print(f"   P(|Z|>3): {p_z_gt_3:.2f}%")
                
                # ç›®æ ‡æ£€æŸ¥
                print(f"ğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
                print(f"   median(|Z|) â‰¤ 1.0: {'âœ…' if median_z <= 1.0 else 'âŒ'}")
                print(f"   P(|Z|>2) â‰¤ 8%: {'âœ…' if p_z_gt_2 <= 8.0 else 'âŒ'}")
                print(f"   P(|Z|>3) â‰¤ 3%: {'âœ…' if p_z_gt_3 <= 3.0 else 'âŒ'}")
                
                # æ–°å¢ç›‘æ§ï¼šç©ºçª—åå‰3ç¬”Zåˆ†å¸ƒ
                if 'meta' in df.columns:
                    try:
                        # è§£æmetaå­—æ®µè·å–warmupä¿¡æ¯
                        warmup_mask = df['meta'].apply(lambda x: x.get('warmup', False) if isinstance(x, dict) else False)
                        if warmup_mask.any():
                            warmup_data = z_data[warmup_mask]
                            if len(warmup_data) > 0:
                                warmup_median = abs(warmup_data).median()
                                warmup_p_gt_2 = (abs(warmup_data) > 2).mean() * 100
                                print(f"ğŸ” ç©ºçª—åå‰3ç¬”Zåˆ†å¸ƒ:")
                                print(f"   median(|Z|): {warmup_median:.4f}")
                                print(f"   P(|Z|>2): {warmup_p_gt_2:.2f}%")
                    except Exception as e:
                        print(f"âš ï¸ è§£æwarmupæ•°æ®æ—¶å‡ºé”™: {e}")
        
        # æ£€æŸ¥æµ‹è¯•æ—¶é•¿
        if 'timestamp' in df.columns:
            time_span = (df['timestamp'].max() - df['timestamp'].min()) / 1000
            print(f"â±ï¸ æµ‹è¯•æ—¶é•¿: {time_span:.1f} ç§’ ({time_span/60:.1f} åˆ†é’Ÿ)")
            
            if time_span >= 1200:  # 20åˆ†é’Ÿ
                print("âœ… æµ‹è¯•æ—¶é•¿è¾¾æ ‡ (â‰¥20åˆ†é’Ÿ)")
            else:
                remaining = 1200 - time_span
                print(f"â³ è¿˜éœ€ {remaining:.1f} ç§’ ({remaining/60:.1f} åˆ†é’Ÿ)")
        
        # æ–°å¢ç›‘æ§ï¼šscale_tï¼ˆåˆ†æ¯ï¼‰ç»Ÿè®¡
        if 'meta' in df.columns:
            try:
                # å°è¯•æå–scaleä¿¡æ¯
                scale_values = []
                for meta in df['meta']:
                    if isinstance(meta, dict) and 'scale' in meta:
                        scale_values.append(meta['scale'])
                
                if scale_values:
                    scale_array = np.array(scale_values)
                    scale_p5 = np.percentile(scale_array, 5)
                    scale_p50 = np.percentile(scale_array, 50)
                    scale_p95 = np.percentile(scale_array, 95)
                    
                    print(f"ğŸ“Š Scaleï¼ˆåˆ†æ¯ï¼‰ç»Ÿè®¡:")
                    print(f"   P5: {scale_p5:.6f}")
                    print(f"   P50: {scale_p50:.6f}")
                    print(f"   P95: {scale_p95:.6f}")
            except Exception as e:
                print(f"âš ï¸ è§£æscaleæ•°æ®æ—¶å‡ºé”™: {e}")
        
    except Exception as e:
        print(f"âš ï¸ è¯»å–æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æŠ¥å‘Šæ–‡ä»¶
    report_files = list(output_dir.glob("*.json"))
    if report_files:
        latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
        print(f"ğŸ“‹ æœ€æ–°æŠ¥å‘Š: {latest_report.name}")
        
        try:
            with open(latest_report, 'r', encoding='utf-8') as f:
                report_data = json.load(f)
            
            if 'final_metrics' in report_data:
                metrics = report_data['final_metrics']
                print(f"ğŸ“Š è¿è¡ŒæŒ‡æ ‡:")
                print(f"   æ€»æ¶ˆæ¯æ•°: {metrics.get('total_messages', 'N/A'):,}")
                print(f"   è§£æé”™è¯¯: {metrics.get('parse_errors', 'N/A')}")
                print(f"   é˜Ÿåˆ—ä¸¢å¼ƒ: {metrics.get('queue_dropped', 'N/A')}")
                print(f"   é‡è¿æ¬¡æ•°: {metrics.get('reconnect_count', 'N/A')}")
                
                if 'agg_dup_count' in metrics:
                    print(f"   é‡å¤ID: {metrics['agg_dup_count']}")
                    print(f"   å€’åºID: {metrics.get('agg_backward_count', 'N/A')}")
                    print(f"   å»¶è¿Ÿäº‹ä»¶: {metrics.get('late_event_dropped', 'N/A')}")
                    print(f"   ç¼“å†²P95: {metrics.get('buffer_size_p95', 'N/A')}")
                    print(f"   ç¼“å†²æœ€å¤§: {metrics.get('buffer_size_max', 'N/A')}")
                
                # æ–°å¢ç›‘æ§ï¼šå¸¸è§„å¥åº·é¡¹
                print(f"ğŸ” å¸¸è§„å¥åº·é¡¹:")
                print(f"   Zå†»ç»“æ¬¡æ•°: {metrics.get('z_freeze_count', 'N/A')}")
                print(f"   10ç§’ä»¥ä¸Šç¼ºå£: {metrics.get('gaps_over_10s', 'N/A')}")
                print(f"   P99åˆ°è¾¾é—´éš”: {metrics.get('p99_interarrival', 'N/A')}ms")
        
        except Exception as e:
            print(f"âš ï¸ è¯»å–æŠ¥å‘Šæ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    print("=" * 60)

if __name__ == "__main__":
    import sys
    test_group = sys.argv[1] if len(sys.argv) > 1 else "A1"
    check_conservative_tune_progress(test_group)
